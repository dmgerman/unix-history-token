begin_unit|revision:0.9.5;language:C;cregit-version:0.0.1
begin_comment
comment|/*  * Copyright 2011-2015 Samy Al Bahra.  * All rights reserved.  *  * Redistribution and use in source and binary forms, with or without  * modification, are permitted provided that the following conditions  * are met:  * 1. Redistributions of source code must retain the above copyright  *    notice, this list of conditions and the following disclaimer.  * 2. Redistributions in binary form must reproduce the above copyright  *    notice, this list of conditions and the following disclaimer in the  *    documentation and/or other materials provided with the distribution.  *  * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND  * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE  * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE  * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS  * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)  * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT  * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF  * SUCH DAMAGE.  */
end_comment

begin_comment
comment|/*  * The implementation here is inspired from the work described in:  *   Fraser, K. 2004. Practical Lock-Freedom. PhD Thesis, University  *   of Cambridge Computing Laboratory.  */
end_comment

begin_include
include|#
directive|include
file|<ck_backoff.h>
end_include

begin_include
include|#
directive|include
file|<ck_cc.h>
end_include

begin_include
include|#
directive|include
file|<ck_epoch.h>
end_include

begin_include
include|#
directive|include
file|<ck_pr.h>
end_include

begin_include
include|#
directive|include
file|<ck_stack.h>
end_include

begin_include
include|#
directive|include
file|<ck_stdbool.h>
end_include

begin_include
include|#
directive|include
file|<ck_string.h>
end_include

begin_comment
comment|/*  * Only three distinct values are used for reclamation, but reclamation occurs  * at e+2 rather than e+1. Any thread in a "critical section" would have  * acquired some snapshot (e) of the global epoch value (e_g) and set an active  * flag. Any hazardous references will only occur after a full memory barrier.  * For example, assume an initial e_g value of 1, e value of 0 and active value  * of 0.  *  * ck_epoch_begin(...)  *   e = e_g  *   active = 1  *   memory_barrier();  *  * Any serialized reads may observe e = 0 or e = 1 with active = 0, or e = 0 or  * e = 1 with active = 1. The e_g value can only go from 1 to 2 if every thread  * has already observed the value of "1" (or the value we are incrementing  * from). This guarantees us that for any given value e_g, any threads with-in  * critical sections (referred to as "active" threads from here on) would have  * an e value of e_g-1 or e_g. This also means that hazardous references may be  * shared in both e_g-1 and e_g even if they are logically deleted in e_g.  *  * For example, assume all threads have an e value of e_g. Another thread may  * increment to e_g to e_g+1. Older threads may have a reference to an object  * which is only deleted in e_g+1. It could be that reader threads are  * executing some hash table look-ups, while some other writer thread (which  * causes epoch counter tick) actually deletes the same items that reader  * threads are looking up (this writer thread having an e value of e_g+1).  * This is possible if the writer thread re-observes the epoch after the  * counter tick.  *  * Psuedo-code for writer:  *   ck_epoch_begin()  *   ht_delete(x)  *   ck_epoch_end()  *   ck_epoch_begin()  *   ht_delete(x)  *   ck_epoch_end()  *  * Psuedo-code for reader:  *   for (;;) {  *      x = ht_lookup(x)  *      ck_pr_inc(&x->value);  *   }  *  * Of course, it is also possible for references logically deleted at e_g-1 to  * still be accessed at e_g as threads are "active" at the same time  * (real-world time) mutating shared objects.  *  * Now, if the epoch counter is ticked to e_g+1, then no new hazardous  * references could exist to objects logically deleted at e_g-1. The reason for  * this is that at e_g+1, all epoch read-side critical sections started at  * e_g-1 must have been completed. If any epoch read-side critical sections at  * e_g-1 were still active, then we would never increment to e_g+1 (active != 0  * ^ e != e_g).  Additionally, e_g may still have hazardous references to  * objects logically deleted at e_g-1 which means objects logically deleted at  * e_g-1 cannot be deleted at e_g+1 unless all threads have observed e_g+1  * (since it is valid for active threads to be at e_g and threads at e_g still  * require safe memory accesses).  *  * However, at e_g+2, all active threads must be either at e_g+1 or e_g+2.  * Though e_g+2 may share hazardous references with e_g+1, and e_g+1 shares  * hazardous references to e_g, no active threads are at e_g or e_g-1. This  * means no hazardous references could exist to objects deleted at e_g-1 (at  * e_g+2).  *  * To summarize these important points,  *   1) Active threads will always have a value of e_g or e_g-1.  *   2) Items that are logically deleted e_g or e_g-1 cannot be physically  *      deleted.  *   3) Objects logically deleted at e_g-1 can be physically destroyed at e_g+2  *      or at e_g+1 if no threads are at e_g.  *  * Last but not least, if we are at e_g+2, then no active thread is at e_g  * which means it is safe to apply modulo-3 arithmetic to e_g value in order to  * re-use e_g to represent the e_g+3 state. This means it is sufficient to  * represent e_g using only the values 0, 1 or 2. Every time a thread re-visits  * a e_g (which can be determined with a non-empty deferral list) it can assume  * objects in the e_g deferral list involved at least three e_g transitions and  * are thus, safe, for physical deletion.  *  * Blocking semantics for epoch reclamation have additional restrictions.  * Though we only require three deferral lists, reasonable blocking semantics  * must be able to more gracefully handle bursty write work-loads which could  * easily cause e_g wrap-around if modulo-3 arithmetic is used. This allows for  * easy-to-trigger live-lock situations. The work-around to this is to not  * apply modulo arithmetic to e_g but only to deferral list indexing.  */
end_comment

begin_define
define|#
directive|define
name|CK_EPOCH_GRACE
value|3U
end_define

begin_enum
enum|enum
block|{
name|CK_EPOCH_STATE_USED
init|=
literal|0
block|,
name|CK_EPOCH_STATE_FREE
init|=
literal|1
block|}
enum|;
end_enum

begin_macro
name|CK_STACK_CONTAINER
argument_list|(
argument|struct ck_epoch_record
argument_list|,
argument|record_next
argument_list|,
argument|ck_epoch_record_container
argument_list|)
end_macro

begin_macro
name|CK_STACK_CONTAINER
argument_list|(
argument|struct ck_epoch_entry
argument_list|,
argument|stack_entry
argument_list|,
argument|ck_epoch_entry_container
argument_list|)
end_macro

begin_define
define|#
directive|define
name|CK_EPOCH_SENSE_MASK
value|(CK_EPOCH_SENSE - 1)
end_define

begin_function
name|void
name|_ck_epoch_delref
parameter_list|(
name|struct
name|ck_epoch_record
modifier|*
name|record
parameter_list|,
name|struct
name|ck_epoch_section
modifier|*
name|section
parameter_list|)
block|{
name|struct
name|ck_epoch_ref
modifier|*
name|current
decl_stmt|,
modifier|*
name|other
decl_stmt|;
name|unsigned
name|int
name|i
init|=
name|section
operator|->
name|bucket
decl_stmt|;
name|current
operator|=
operator|&
name|record
operator|->
name|local
operator|.
name|bucket
index|[
name|i
index|]
expr_stmt|;
name|current
operator|->
name|count
operator|--
expr_stmt|;
if|if
condition|(
name|current
operator|->
name|count
operator|>
literal|0
condition|)
return|return;
comment|/* 	 * If the current bucket no longer has any references, then 	 * determine whether we have already transitioned into a newer 	 * epoch. If so, then make sure to update our shared snapshot 	 * to allow for forward progress. 	 * 	 * If no other active bucket exists, then the record will go 	 * inactive in order to allow for forward progress. 	 */
name|other
operator|=
operator|&
name|record
operator|->
name|local
operator|.
name|bucket
index|[
operator|(
name|i
operator|+
literal|1
operator|)
operator|&
name|CK_EPOCH_SENSE_MASK
index|]
expr_stmt|;
if|if
condition|(
name|other
operator|->
name|count
operator|>
literal|0
operator|&&
operator|(
call|(
name|int
call|)
argument_list|(
name|current
operator|->
name|epoch
operator|-
name|other
operator|->
name|epoch
argument_list|)
operator|<
literal|0
operator|)
condition|)
block|{
comment|/* 		 * The other epoch value is actually the newest, 		 * transition to it. 		 */
name|ck_pr_store_uint
argument_list|(
operator|&
name|record
operator|->
name|epoch
argument_list|,
name|other
operator|->
name|epoch
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
end_function

begin_function
name|void
name|_ck_epoch_addref
parameter_list|(
name|struct
name|ck_epoch_record
modifier|*
name|record
parameter_list|,
name|struct
name|ck_epoch_section
modifier|*
name|section
parameter_list|)
block|{
name|struct
name|ck_epoch
modifier|*
name|global
init|=
name|record
operator|->
name|global
decl_stmt|;
name|struct
name|ck_epoch_ref
modifier|*
name|ref
decl_stmt|;
name|unsigned
name|int
name|epoch
decl_stmt|,
name|i
decl_stmt|;
name|epoch
operator|=
name|ck_pr_load_uint
argument_list|(
operator|&
name|global
operator|->
name|epoch
argument_list|)
expr_stmt|;
name|i
operator|=
name|epoch
operator|&
name|CK_EPOCH_SENSE_MASK
expr_stmt|;
name|ref
operator|=
operator|&
name|record
operator|->
name|local
operator|.
name|bucket
index|[
name|i
index|]
expr_stmt|;
if|if
condition|(
name|ref
operator|->
name|count
operator|++
operator|==
literal|0
condition|)
block|{
ifndef|#
directive|ifndef
name|CK_MD_TSO
name|struct
name|ck_epoch_ref
modifier|*
name|previous
decl_stmt|;
comment|/* 		 * The system has already ticked. If another non-zero bucket 		 * exists, make sure to order our observations with respect 		 * to it. Otherwise, it is possible to acquire a reference 		 * from the previous epoch generation. 		 * 		 * On TSO architectures, the monoticity of the global counter 		 * and load-{store, load} ordering are sufficient to guarantee 		 * this ordering. 		 */
name|previous
operator|=
operator|&
name|record
operator|->
name|local
operator|.
name|bucket
index|[
operator|(
name|i
operator|+
literal|1
operator|)
operator|&
name|CK_EPOCH_SENSE_MASK
index|]
expr_stmt|;
if|if
condition|(
name|previous
operator|->
name|count
operator|>
literal|0
condition|)
name|ck_pr_fence_acqrel
argument_list|()
expr_stmt|;
endif|#
directive|endif
comment|/* !CK_MD_TSO */
comment|/* 		 * If this is this is a new reference into the current 		 * bucket then cache the associated epoch value. 		 */
name|ref
operator|->
name|epoch
operator|=
name|epoch
expr_stmt|;
block|}
name|section
operator|->
name|bucket
operator|=
name|i
expr_stmt|;
return|return;
block|}
end_function

begin_function
name|void
name|ck_epoch_init
parameter_list|(
name|struct
name|ck_epoch
modifier|*
name|global
parameter_list|)
block|{
name|ck_stack_init
argument_list|(
operator|&
name|global
operator|->
name|records
argument_list|)
expr_stmt|;
name|global
operator|->
name|epoch
operator|=
literal|1
expr_stmt|;
name|global
operator|->
name|n_free
operator|=
literal|0
expr_stmt|;
name|ck_pr_fence_store
argument_list|()
expr_stmt|;
return|return;
block|}
end_function

begin_function
name|struct
name|ck_epoch_record
modifier|*
name|ck_epoch_recycle
parameter_list|(
name|struct
name|ck_epoch
modifier|*
name|global
parameter_list|)
block|{
name|struct
name|ck_epoch_record
modifier|*
name|record
decl_stmt|;
name|ck_stack_entry_t
modifier|*
name|cursor
decl_stmt|;
name|unsigned
name|int
name|state
decl_stmt|;
if|if
condition|(
name|ck_pr_load_uint
argument_list|(
operator|&
name|global
operator|->
name|n_free
argument_list|)
operator|==
literal|0
condition|)
return|return
name|NULL
return|;
name|CK_STACK_FOREACH
argument_list|(
argument|&global->records
argument_list|,
argument|cursor
argument_list|)
block|{
name|record
operator|=
name|ck_epoch_record_container
argument_list|(
name|cursor
argument_list|)
expr_stmt|;
if|if
condition|(
name|ck_pr_load_uint
argument_list|(
operator|&
name|record
operator|->
name|state
argument_list|)
operator|==
name|CK_EPOCH_STATE_FREE
condition|)
block|{
comment|/* Serialize with respect to deferral list clean-up. */
name|ck_pr_fence_load
argument_list|()
expr_stmt|;
name|state
operator|=
name|ck_pr_fas_uint
argument_list|(
operator|&
name|record
operator|->
name|state
argument_list|,
name|CK_EPOCH_STATE_USED
argument_list|)
expr_stmt|;
if|if
condition|(
name|state
operator|==
name|CK_EPOCH_STATE_FREE
condition|)
block|{
name|ck_pr_dec_uint
argument_list|(
operator|&
name|global
operator|->
name|n_free
argument_list|)
expr_stmt|;
return|return
name|record
return|;
block|}
block|}
block|}
return|return
name|NULL
return|;
block|}
end_function

begin_function
name|void
name|ck_epoch_register
parameter_list|(
name|struct
name|ck_epoch
modifier|*
name|global
parameter_list|,
name|struct
name|ck_epoch_record
modifier|*
name|record
parameter_list|)
block|{
name|size_t
name|i
decl_stmt|;
name|record
operator|->
name|global
operator|=
name|global
expr_stmt|;
name|record
operator|->
name|state
operator|=
name|CK_EPOCH_STATE_USED
expr_stmt|;
name|record
operator|->
name|active
operator|=
literal|0
expr_stmt|;
name|record
operator|->
name|epoch
operator|=
literal|0
expr_stmt|;
name|record
operator|->
name|n_dispatch
operator|=
literal|0
expr_stmt|;
name|record
operator|->
name|n_peak
operator|=
literal|0
expr_stmt|;
name|record
operator|->
name|n_pending
operator|=
literal|0
expr_stmt|;
name|memset
argument_list|(
operator|&
name|record
operator|->
name|local
argument_list|,
literal|0
argument_list|,
sizeof|sizeof
name|record
operator|->
name|local
argument_list|)
expr_stmt|;
for|for
control|(
name|i
operator|=
literal|0
init|;
name|i
operator|<
name|CK_EPOCH_LENGTH
condition|;
name|i
operator|++
control|)
name|ck_stack_init
argument_list|(
operator|&
name|record
operator|->
name|pending
index|[
name|i
index|]
argument_list|)
expr_stmt|;
name|ck_pr_fence_store
argument_list|()
expr_stmt|;
name|ck_stack_push_upmc
argument_list|(
operator|&
name|global
operator|->
name|records
argument_list|,
operator|&
name|record
operator|->
name|record_next
argument_list|)
expr_stmt|;
return|return;
block|}
end_function

begin_function
name|void
name|ck_epoch_unregister
parameter_list|(
name|struct
name|ck_epoch_record
modifier|*
name|record
parameter_list|)
block|{
name|struct
name|ck_epoch
modifier|*
name|global
init|=
name|record
operator|->
name|global
decl_stmt|;
name|size_t
name|i
decl_stmt|;
name|record
operator|->
name|active
operator|=
literal|0
expr_stmt|;
name|record
operator|->
name|epoch
operator|=
literal|0
expr_stmt|;
name|record
operator|->
name|n_dispatch
operator|=
literal|0
expr_stmt|;
name|record
operator|->
name|n_peak
operator|=
literal|0
expr_stmt|;
name|record
operator|->
name|n_pending
operator|=
literal|0
expr_stmt|;
name|memset
argument_list|(
operator|&
name|record
operator|->
name|local
argument_list|,
literal|0
argument_list|,
sizeof|sizeof
name|record
operator|->
name|local
argument_list|)
expr_stmt|;
for|for
control|(
name|i
operator|=
literal|0
init|;
name|i
operator|<
name|CK_EPOCH_LENGTH
condition|;
name|i
operator|++
control|)
name|ck_stack_init
argument_list|(
operator|&
name|record
operator|->
name|pending
index|[
name|i
index|]
argument_list|)
expr_stmt|;
name|ck_pr_fence_store
argument_list|()
expr_stmt|;
name|ck_pr_store_uint
argument_list|(
operator|&
name|record
operator|->
name|state
argument_list|,
name|CK_EPOCH_STATE_FREE
argument_list|)
expr_stmt|;
name|ck_pr_inc_uint
argument_list|(
operator|&
name|global
operator|->
name|n_free
argument_list|)
expr_stmt|;
return|return;
block|}
end_function

begin_function
specifier|static
name|struct
name|ck_epoch_record
modifier|*
name|ck_epoch_scan
parameter_list|(
name|struct
name|ck_epoch
modifier|*
name|global
parameter_list|,
name|struct
name|ck_epoch_record
modifier|*
name|cr
parameter_list|,
name|unsigned
name|int
name|epoch
parameter_list|,
name|bool
modifier|*
name|af
parameter_list|)
block|{
name|ck_stack_entry_t
modifier|*
name|cursor
decl_stmt|;
if|if
condition|(
name|cr
operator|==
name|NULL
condition|)
block|{
name|cursor
operator|=
name|CK_STACK_FIRST
argument_list|(
operator|&
name|global
operator|->
name|records
argument_list|)
expr_stmt|;
operator|*
name|af
operator|=
name|false
expr_stmt|;
block|}
else|else
block|{
name|cursor
operator|=
operator|&
name|cr
operator|->
name|record_next
expr_stmt|;
operator|*
name|af
operator|=
name|true
expr_stmt|;
block|}
while|while
condition|(
name|cursor
operator|!=
name|NULL
condition|)
block|{
name|unsigned
name|int
name|state
decl_stmt|,
name|active
decl_stmt|;
name|cr
operator|=
name|ck_epoch_record_container
argument_list|(
name|cursor
argument_list|)
expr_stmt|;
name|state
operator|=
name|ck_pr_load_uint
argument_list|(
operator|&
name|cr
operator|->
name|state
argument_list|)
expr_stmt|;
if|if
condition|(
name|state
operator|&
name|CK_EPOCH_STATE_FREE
condition|)
block|{
name|cursor
operator|=
name|CK_STACK_NEXT
argument_list|(
name|cursor
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|active
operator|=
name|ck_pr_load_uint
argument_list|(
operator|&
name|cr
operator|->
name|active
argument_list|)
expr_stmt|;
operator|*
name|af
operator||=
name|active
expr_stmt|;
if|if
condition|(
name|active
operator|!=
literal|0
operator|&&
name|ck_pr_load_uint
argument_list|(
operator|&
name|cr
operator|->
name|epoch
argument_list|)
operator|!=
name|epoch
condition|)
return|return
name|cr
return|;
name|cursor
operator|=
name|CK_STACK_NEXT
argument_list|(
name|cursor
argument_list|)
expr_stmt|;
block|}
return|return
name|NULL
return|;
block|}
end_function

begin_function
specifier|static
name|void
name|ck_epoch_dispatch
parameter_list|(
name|struct
name|ck_epoch_record
modifier|*
name|record
parameter_list|,
name|unsigned
name|int
name|e
parameter_list|)
block|{
name|unsigned
name|int
name|epoch
init|=
name|e
operator|&
operator|(
name|CK_EPOCH_LENGTH
operator|-
literal|1
operator|)
decl_stmt|;
name|ck_stack_entry_t
modifier|*
name|head
decl_stmt|,
modifier|*
name|next
decl_stmt|,
modifier|*
name|cursor
decl_stmt|;
name|unsigned
name|int
name|i
init|=
literal|0
decl_stmt|;
name|head
operator|=
name|CK_STACK_FIRST
argument_list|(
operator|&
name|record
operator|->
name|pending
index|[
name|epoch
index|]
argument_list|)
expr_stmt|;
name|ck_stack_init
argument_list|(
operator|&
name|record
operator|->
name|pending
index|[
name|epoch
index|]
argument_list|)
expr_stmt|;
for|for
control|(
name|cursor
operator|=
name|head
init|;
name|cursor
operator|!=
name|NULL
condition|;
name|cursor
operator|=
name|next
control|)
block|{
name|struct
name|ck_epoch_entry
modifier|*
name|entry
init|=
name|ck_epoch_entry_container
argument_list|(
name|cursor
argument_list|)
decl_stmt|;
name|next
operator|=
name|CK_STACK_NEXT
argument_list|(
name|cursor
argument_list|)
expr_stmt|;
name|entry
operator|->
name|function
argument_list|(
name|entry
argument_list|)
expr_stmt|;
name|i
operator|++
expr_stmt|;
block|}
if|if
condition|(
name|record
operator|->
name|n_pending
operator|>
name|record
operator|->
name|n_peak
condition|)
name|record
operator|->
name|n_peak
operator|=
name|record
operator|->
name|n_pending
expr_stmt|;
name|record
operator|->
name|n_dispatch
operator|+=
name|i
expr_stmt|;
name|record
operator|->
name|n_pending
operator|-=
name|i
expr_stmt|;
return|return;
block|}
end_function

begin_comment
comment|/*  * Reclaim all objects associated with a record.  */
end_comment

begin_function
name|void
name|ck_epoch_reclaim
parameter_list|(
name|struct
name|ck_epoch_record
modifier|*
name|record
parameter_list|)
block|{
name|unsigned
name|int
name|epoch
decl_stmt|;
for|for
control|(
name|epoch
operator|=
literal|0
init|;
name|epoch
operator|<
name|CK_EPOCH_LENGTH
condition|;
name|epoch
operator|++
control|)
name|ck_epoch_dispatch
argument_list|(
name|record
argument_list|,
name|epoch
argument_list|)
expr_stmt|;
return|return;
block|}
end_function

begin_comment
comment|/*  * This function must not be called with-in read section.  */
end_comment

begin_function
name|void
name|ck_epoch_synchronize
parameter_list|(
name|struct
name|ck_epoch_record
modifier|*
name|record
parameter_list|)
block|{
name|struct
name|ck_epoch
modifier|*
name|global
init|=
name|record
operator|->
name|global
decl_stmt|;
name|struct
name|ck_epoch_record
modifier|*
name|cr
decl_stmt|;
name|unsigned
name|int
name|delta
decl_stmt|,
name|epoch
decl_stmt|,
name|goal
decl_stmt|,
name|i
decl_stmt|;
name|bool
name|active
decl_stmt|;
name|ck_pr_fence_memory
argument_list|()
expr_stmt|;
comment|/* 	 * The observation of the global epoch must be ordered with respect to 	 * all prior operations. The re-ordering of loads is permitted given 	 * monoticity of global epoch counter. 	 * 	 * If UINT_MAX concurrent mutations were to occur then it is possible 	 * to encounter an ABA-issue. If this is a concern, consider tuning 	 * write-side concurrency. 	 */
name|delta
operator|=
name|epoch
operator|=
name|ck_pr_load_uint
argument_list|(
operator|&
name|global
operator|->
name|epoch
argument_list|)
expr_stmt|;
name|goal
operator|=
name|epoch
operator|+
name|CK_EPOCH_GRACE
expr_stmt|;
for|for
control|(
name|i
operator|=
literal|0
operator|,
name|cr
operator|=
name|NULL
init|;
name|i
operator|<
name|CK_EPOCH_GRACE
operator|-
literal|1
condition|;
name|cr
operator|=
name|NULL
operator|,
name|i
operator|++
control|)
block|{
name|bool
name|r
decl_stmt|;
comment|/* 		 * Determine whether all threads have observed the current 		 * epoch with respect to the updates on invocation. 		 */
while|while
condition|(
name|cr
operator|=
name|ck_epoch_scan
argument_list|(
name|global
argument_list|,
name|cr
argument_list|,
name|delta
argument_list|,
operator|&
name|active
argument_list|)
operator|,
name|cr
operator|!=
name|NULL
condition|)
block|{
name|unsigned
name|int
name|e_d
decl_stmt|;
name|ck_pr_stall
argument_list|()
expr_stmt|;
comment|/* 			 * Another writer may have already observed a grace 			 * period. 			 */
name|e_d
operator|=
name|ck_pr_load_uint
argument_list|(
operator|&
name|global
operator|->
name|epoch
argument_list|)
expr_stmt|;
if|if
condition|(
name|e_d
operator|!=
name|delta
condition|)
block|{
name|delta
operator|=
name|e_d
expr_stmt|;
goto|goto
name|reload
goto|;
block|}
block|}
comment|/* 		 * If we have observed all threads as inactive, then we assume 		 * we are at a grace period. 		 */
if|if
condition|(
name|active
operator|==
name|false
condition|)
break|break;
comment|/* 		 * Increment current epoch. CAS semantics are used to eliminate 		 * increment operations for synchronization that occurs for the 		 * same global epoch value snapshot. 		 * 		 * If we can guarantee there will only be one active barrier or 		 * epoch tick at a given time, then it is sufficient to use an 		 * increment operation. In a multi-barrier workload, however, 		 * it is possible to overflow the epoch value if we apply 		 * modulo-3 arithmetic. 		 */
name|r
operator|=
name|ck_pr_cas_uint_value
argument_list|(
operator|&
name|global
operator|->
name|epoch
argument_list|,
name|delta
argument_list|,
name|delta
operator|+
literal|1
argument_list|,
operator|&
name|delta
argument_list|)
expr_stmt|;
comment|/* Order subsequent thread active checks. */
name|ck_pr_fence_atomic_load
argument_list|()
expr_stmt|;
comment|/* 		 * If CAS has succeeded, then set delta to latest snapshot. 		 * Otherwise, we have just acquired latest snapshot. 		 */
name|delta
operator|=
name|delta
operator|+
name|r
expr_stmt|;
continue|continue;
name|reload
label|:
if|if
condition|(
operator|(
name|goal
operator|>
name|epoch
operator|)
operator|&
operator|(
name|delta
operator|>=
name|goal
operator|)
condition|)
block|{
comment|/* 			 * Right now, epoch overflow is handled as an edge 			 * case. If we have already observed an epoch 			 * generation, then we can be sure no hazardous 			 * references exist to objects from this generation. We 			 * can actually avoid an addtional scan step at this 			 * point. 			 */
break|break;
block|}
block|}
comment|/* 	 * A majority of use-cases will not require full barrier semantics. 	 * However, if non-temporal instructions are used, full barrier 	 * semantics are necessary. 	 */
name|ck_pr_fence_memory
argument_list|()
expr_stmt|;
name|record
operator|->
name|epoch
operator|=
name|delta
expr_stmt|;
return|return;
block|}
end_function

begin_function
name|void
name|ck_epoch_barrier
parameter_list|(
name|struct
name|ck_epoch_record
modifier|*
name|record
parameter_list|)
block|{
name|ck_epoch_synchronize
argument_list|(
name|record
argument_list|)
expr_stmt|;
name|ck_epoch_reclaim
argument_list|(
name|record
argument_list|)
expr_stmt|;
return|return;
block|}
end_function

begin_comment
comment|/*  * It may be worth it to actually apply these deferral semantics to an epoch  * that was observed at ck_epoch_call time. The problem is that the latter  * would require a full fence.  *  * ck_epoch_call will dispatch to the latest epoch snapshot that was observed.  * There are cases where it will fail to reclaim as early as it could. If this  * becomes a problem, we could actually use a heap for epoch buckets but that  * is far from ideal too.  */
end_comment

begin_function
name|bool
name|ck_epoch_poll
parameter_list|(
name|struct
name|ck_epoch_record
modifier|*
name|record
parameter_list|)
block|{
name|bool
name|active
decl_stmt|;
name|unsigned
name|int
name|epoch
decl_stmt|;
name|unsigned
name|int
name|snapshot
decl_stmt|;
name|struct
name|ck_epoch_record
modifier|*
name|cr
init|=
name|NULL
decl_stmt|;
name|struct
name|ck_epoch
modifier|*
name|global
init|=
name|record
operator|->
name|global
decl_stmt|;
name|epoch
operator|=
name|ck_pr_load_uint
argument_list|(
operator|&
name|global
operator|->
name|epoch
argument_list|)
expr_stmt|;
comment|/* Serialize epoch snapshots with respect to global epoch. */
name|ck_pr_fence_memory
argument_list|()
expr_stmt|;
name|cr
operator|=
name|ck_epoch_scan
argument_list|(
name|global
argument_list|,
name|cr
argument_list|,
name|epoch
argument_list|,
operator|&
name|active
argument_list|)
expr_stmt|;
if|if
condition|(
name|cr
operator|!=
name|NULL
condition|)
block|{
name|record
operator|->
name|epoch
operator|=
name|epoch
expr_stmt|;
return|return
name|false
return|;
block|}
comment|/* We are at a grace period if all threads are inactive. */
if|if
condition|(
name|active
operator|==
name|false
condition|)
block|{
name|record
operator|->
name|epoch
operator|=
name|epoch
expr_stmt|;
for|for
control|(
name|epoch
operator|=
literal|0
init|;
name|epoch
operator|<
name|CK_EPOCH_LENGTH
condition|;
name|epoch
operator|++
control|)
name|ck_epoch_dispatch
argument_list|(
name|record
argument_list|,
name|epoch
argument_list|)
expr_stmt|;
return|return
name|true
return|;
block|}
comment|/* If an active thread exists, rely on epoch observation. */
if|if
condition|(
name|ck_pr_cas_uint_value
argument_list|(
operator|&
name|global
operator|->
name|epoch
argument_list|,
name|epoch
argument_list|,
name|epoch
operator|+
literal|1
argument_list|,
operator|&
name|snapshot
argument_list|)
operator|==
name|false
condition|)
block|{
name|record
operator|->
name|epoch
operator|=
name|snapshot
expr_stmt|;
block|}
else|else
block|{
name|record
operator|->
name|epoch
operator|=
name|epoch
operator|+
literal|1
expr_stmt|;
block|}
name|ck_epoch_dispatch
argument_list|(
name|record
argument_list|,
name|epoch
operator|+
literal|1
argument_list|)
expr_stmt|;
return|return
name|true
return|;
block|}
end_function

end_unit

