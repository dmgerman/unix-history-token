begin_unit|revision:0.9.5;language:C;cregit-version:0.0.1
begin_comment
comment|/*-  * Copyright (c) 1991 Regents of the University of California.  * All rights reserved.  * Copyright (c) 1994 John S. Dyson  * All rights reserved.  * Copyright (c) 1994 David Greenman  * All rights reserved.  * Copyright (c) 2005 Yahoo! Technologies Norway AS  * All rights reserved.  *  * This code is derived from software contributed to Berkeley by  * The Mach Operating System project at Carnegie-Mellon University.  *  * Redistribution and use in source and binary forms, with or without  * modification, are permitted provided that the following conditions  * are met:  * 1. Redistributions of source code must retain the above copyright  *    notice, this list of conditions and the following disclaimer.  * 2. Redistributions in binary form must reproduce the above copyright  *    notice, this list of conditions and the following disclaimer in the  *    documentation and/or other materials provided with the distribution.  * 3. All advertising materials mentioning features or use of this software  *    must display the following acknowledgement:  *	This product includes software developed by the University of  *	California, Berkeley and its contributors.  * 4. Neither the name of the University nor the names of its contributors  *    may be used to endorse or promote products derived from this software  *    without specific prior written permission.  *  * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND  * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE  * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE  * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS  * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)  * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT  * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF  * SUCH DAMAGE.  *  *	from: @(#)vm_pageout.c	7.4 (Berkeley) 5/7/91  *  *  * Copyright (c) 1987, 1990 Carnegie-Mellon University.  * All rights reserved.  *  * Authors: Avadis Tevanian, Jr., Michael Wayne Young  *  * Permission to use, copy, modify and distribute this software and  * its documentation is hereby granted, provided that both the copyright  * notice and this permission notice appear in all copies of the  * software, derivative works or modified versions, and any portions  * thereof, and that both notices appear in supporting documentation.  *  * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"  * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND  * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.  *  * Carnegie Mellon requests users of this software to return to  *  *  Software Distribution Coordinator  or  Software.Distribution@CS.CMU.EDU  *  School of Computer Science  *  Carnegie Mellon University  *  Pittsburgh PA 15213-3890  *  * any improvements or extensions that they make and grant Carnegie the  * rights to redistribute these changes.  */
end_comment

begin_comment
comment|/*  *	The proverbial page-out daemon.  */
end_comment

begin_include
include|#
directive|include
file|<sys/cdefs.h>
end_include

begin_expr_stmt
name|__FBSDID
argument_list|(
literal|"$FreeBSD$"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_include
include|#
directive|include
file|"opt_vm.h"
end_include

begin_include
include|#
directive|include
file|"opt_kdtrace.h"
end_include

begin_include
include|#
directive|include
file|<sys/param.h>
end_include

begin_include
include|#
directive|include
file|<sys/systm.h>
end_include

begin_include
include|#
directive|include
file|<sys/kernel.h>
end_include

begin_include
include|#
directive|include
file|<sys/eventhandler.h>
end_include

begin_include
include|#
directive|include
file|<sys/lock.h>
end_include

begin_include
include|#
directive|include
file|<sys/mutex.h>
end_include

begin_include
include|#
directive|include
file|<sys/proc.h>
end_include

begin_include
include|#
directive|include
file|<sys/kthread.h>
end_include

begin_include
include|#
directive|include
file|<sys/ktr.h>
end_include

begin_include
include|#
directive|include
file|<sys/mount.h>
end_include

begin_include
include|#
directive|include
file|<sys/racct.h>
end_include

begin_include
include|#
directive|include
file|<sys/resourcevar.h>
end_include

begin_include
include|#
directive|include
file|<sys/sched.h>
end_include

begin_include
include|#
directive|include
file|<sys/sdt.h>
end_include

begin_include
include|#
directive|include
file|<sys/signalvar.h>
end_include

begin_include
include|#
directive|include
file|<sys/smp.h>
end_include

begin_include
include|#
directive|include
file|<sys/time.h>
end_include

begin_include
include|#
directive|include
file|<sys/vnode.h>
end_include

begin_include
include|#
directive|include
file|<sys/vmmeter.h>
end_include

begin_include
include|#
directive|include
file|<sys/rwlock.h>
end_include

begin_include
include|#
directive|include
file|<sys/sx.h>
end_include

begin_include
include|#
directive|include
file|<sys/sysctl.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_param.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_object.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_page.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_map.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_pageout.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_pager.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_phys.h>
end_include

begin_include
include|#
directive|include
file|<vm/swap_pager.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_extern.h>
end_include

begin_include
include|#
directive|include
file|<vm/uma.h>
end_include

begin_comment
comment|/*  * System initialization  */
end_comment

begin_comment
comment|/* the kernel process "vm_pageout"*/
end_comment

begin_function_decl
specifier|static
name|void
name|vm_pageout
parameter_list|(
name|void
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|void
name|vm_pageout_init
parameter_list|(
name|void
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|int
name|vm_pageout_clean
parameter_list|(
name|vm_page_t
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|void
name|vm_pageout_scan
parameter_list|(
name|struct
name|vm_domain
modifier|*
name|vmd
parameter_list|,
name|int
name|pass
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|void
name|vm_pageout_mightbe_oom
parameter_list|(
name|struct
name|vm_domain
modifier|*
name|vmd
parameter_list|,
name|int
name|page_shortage
parameter_list|,
name|int
name|starting_page_shortage
parameter_list|)
function_decl|;
end_function_decl

begin_expr_stmt
name|SYSINIT
argument_list|(
name|pagedaemon_init
argument_list|,
name|SI_SUB_KTHREAD_PAGE
argument_list|,
name|SI_ORDER_FIRST
argument_list|,
name|vm_pageout_init
argument_list|,
name|NULL
argument_list|)
expr_stmt|;
end_expr_stmt

begin_decl_stmt
name|struct
name|proc
modifier|*
name|pageproc
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|struct
name|kproc_desc
name|page_kp
init|=
block|{
literal|"pagedaemon"
block|,
name|vm_pageout
block|,
operator|&
name|pageproc
block|}
decl_stmt|;
end_decl_stmt

begin_expr_stmt
name|SYSINIT
argument_list|(
name|pagedaemon
argument_list|,
name|SI_SUB_KTHREAD_PAGE
argument_list|,
name|SI_ORDER_SECOND
argument_list|,
name|kproc_start
argument_list|,
operator|&
name|page_kp
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SDT_PROVIDER_DEFINE
argument_list|(
name|vm
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SDT_PROBE_DEFINE
argument_list|(
name|vm
argument_list|, , ,
name|vm__lowmem_cache
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SDT_PROBE_DEFINE
argument_list|(
name|vm
argument_list|, , ,
name|vm__lowmem_scan
argument_list|)
expr_stmt|;
end_expr_stmt

begin_if
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_comment
comment|/* the kernel process "vm_daemon"*/
end_comment

begin_function_decl
specifier|static
name|void
name|vm_daemon
parameter_list|(
name|void
parameter_list|)
function_decl|;
end_function_decl

begin_decl_stmt
specifier|static
name|struct
name|proc
modifier|*
name|vmproc
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|struct
name|kproc_desc
name|vm_kp
init|=
block|{
literal|"vmdaemon"
block|,
name|vm_daemon
block|,
operator|&
name|vmproc
block|}
decl_stmt|;
end_decl_stmt

begin_expr_stmt
name|SYSINIT
argument_list|(
name|vmdaemon
argument_list|,
name|SI_SUB_KTHREAD_VM
argument_list|,
name|SI_ORDER_FIRST
argument_list|,
name|kproc_start
argument_list|,
operator|&
name|vm_kp
argument_list|)
expr_stmt|;
end_expr_stmt

begin_endif
endif|#
directive|endif
end_endif

begin_decl_stmt
name|int
name|vm_pages_needed
decl_stmt|;
end_decl_stmt

begin_comment
comment|/* Event on which pageout daemon sleeps */
end_comment

begin_decl_stmt
name|int
name|vm_pageout_deficit
decl_stmt|;
end_decl_stmt

begin_comment
comment|/* Estimated number of pages deficit */
end_comment

begin_decl_stmt
name|int
name|vm_pageout_pages_needed
decl_stmt|;
end_decl_stmt

begin_comment
comment|/* flag saying that the pageout daemon needs pages */
end_comment

begin_decl_stmt
name|int
name|vm_pageout_wakeup_thresh
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|int
name|vm_pageout_oom_seq
init|=
literal|12
decl_stmt|;
end_decl_stmt

begin_if
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_decl_stmt
specifier|static
name|int
name|vm_pageout_req_swapout
decl_stmt|;
end_decl_stmt

begin_comment
comment|/* XXX */
end_comment

begin_decl_stmt
specifier|static
name|int
name|vm_daemon_needed
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|struct
name|mtx
name|vm_daemon_mtx
decl_stmt|;
end_decl_stmt

begin_comment
comment|/* Allow for use by vm_pageout before vm_daemon is initialized. */
end_comment

begin_expr_stmt
name|MTX_SYSINIT
argument_list|(
name|vm_daemon
argument_list|,
operator|&
name|vm_daemon_mtx
argument_list|,
literal|"vm daemon"
argument_list|,
name|MTX_DEF
argument_list|)
expr_stmt|;
end_expr_stmt

begin_endif
endif|#
directive|endif
end_endif

begin_decl_stmt
specifier|static
name|int
name|vm_max_launder
init|=
literal|32
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|int
name|vm_pageout_update_period
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|int
name|defer_swap_pageouts
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|int
name|disable_swap_pageouts
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|int
name|lowmem_period
init|=
literal|10
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|time_t
name|lowmem_uptime
decl_stmt|;
end_decl_stmt

begin_if
if|#
directive|if
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_decl_stmt
specifier|static
name|int
name|vm_swap_enabled
init|=
literal|0
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|int
name|vm_swap_idle_enabled
init|=
literal|0
decl_stmt|;
end_decl_stmt

begin_else
else|#
directive|else
end_else

begin_decl_stmt
specifier|static
name|int
name|vm_swap_enabled
init|=
literal|1
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|int
name|vm_swap_idle_enabled
init|=
literal|0
decl_stmt|;
end_decl_stmt

begin_endif
endif|#
directive|endif
end_endif

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|pageout_wakeup_thresh
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_pageout_wakeup_thresh
argument_list|,
literal|0
argument_list|,
literal|"free page threshold for waking up the pageout daemon"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|max_launder
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_max_launder
argument_list|,
literal|0
argument_list|,
literal|"Limit dirty flushes in pageout"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|pageout_update_period
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_pageout_update_period
argument_list|,
literal|0
argument_list|,
literal|"Maximum active LRU update period"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|lowmem_period
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|lowmem_period
argument_list|,
literal|0
argument_list|,
literal|"Low memory callback period"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_if
if|#
directive|if
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|VM_SWAPPING_ENABLED
argument_list|,
name|swap_enabled
argument_list|,
name|CTLFLAG_RD
argument_list|,
operator|&
name|vm_swap_enabled
argument_list|,
literal|0
argument_list|,
literal|"Enable entire process swapout"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|swap_idle_enabled
argument_list|,
name|CTLFLAG_RD
argument_list|,
operator|&
name|vm_swap_idle_enabled
argument_list|,
literal|0
argument_list|,
literal|"Allow swapout on idle criteria"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_else
else|#
directive|else
end_else

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|VM_SWAPPING_ENABLED
argument_list|,
name|swap_enabled
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_swap_enabled
argument_list|,
literal|0
argument_list|,
literal|"Enable entire process swapout"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|swap_idle_enabled
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_swap_idle_enabled
argument_list|,
literal|0
argument_list|,
literal|"Allow swapout on idle criteria"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_endif
endif|#
directive|endif
end_endif

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|defer_swapspace_pageouts
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|defer_swap_pageouts
argument_list|,
literal|0
argument_list|,
literal|"Give preference to dirty pages in mem"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|disable_swapspace_pageouts
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|disable_swap_pageouts
argument_list|,
literal|0
argument_list|,
literal|"Disallow swapout of dirty pages"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_decl_stmt
specifier|static
name|int
name|pageout_lock_miss
decl_stmt|;
end_decl_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|pageout_lock_miss
argument_list|,
name|CTLFLAG_RD
argument_list|,
operator|&
name|pageout_lock_miss
argument_list|,
literal|0
argument_list|,
literal|"vget() lock misses during pageout"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|pageout_oom_seq
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_pageout_oom_seq
argument_list|,
literal|0
argument_list|,
literal|"back-to-back calls to oom detector to start OOM"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_define
define|#
directive|define
name|VM_PAGEOUT_PAGE_COUNT
value|16
end_define

begin_decl_stmt
name|int
name|vm_pageout_page_count
init|=
name|VM_PAGEOUT_PAGE_COUNT
decl_stmt|;
end_decl_stmt

begin_decl_stmt
name|int
name|vm_page_max_wired
decl_stmt|;
end_decl_stmt

begin_comment
comment|/* XXX max # of wired pages system-wide */
end_comment

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|max_wired
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_page_max_wired
argument_list|,
literal|0
argument_list|,
literal|"System-wide limit to wired page count"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_function_decl
specifier|static
name|boolean_t
name|vm_pageout_fallback_object_lock
parameter_list|(
name|vm_page_t
parameter_list|,
name|vm_page_t
modifier|*
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|boolean_t
name|vm_pageout_launder
parameter_list|(
name|struct
name|vm_pagequeue
modifier|*
name|pq
parameter_list|,
name|int
parameter_list|,
name|vm_paddr_t
parameter_list|,
name|vm_paddr_t
parameter_list|)
function_decl|;
end_function_decl

begin_if
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_function_decl
specifier|static
name|void
name|vm_pageout_map_deactivate_pages
parameter_list|(
name|vm_map_t
parameter_list|,
name|long
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|void
name|vm_pageout_object_deactivate_pages
parameter_list|(
name|pmap_t
parameter_list|,
name|vm_object_t
parameter_list|,
name|long
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|void
name|vm_req_vmdaemon
parameter_list|(
name|int
name|req
parameter_list|)
function_decl|;
end_function_decl

begin_endif
endif|#
directive|endif
end_endif

begin_function_decl
specifier|static
name|boolean_t
name|vm_pageout_page_lock
parameter_list|(
name|vm_page_t
parameter_list|,
name|vm_page_t
modifier|*
parameter_list|)
function_decl|;
end_function_decl

begin_comment
comment|/*  * Initialize a dummy page for marking the caller's place in the specified  * paging queue.  In principle, this function only needs to set the flag  * PG_MARKER.  Nonetheless, it write busies and initializes the hold count  * to one as safety precautions.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout_init_marker
parameter_list|(
name|vm_page_t
name|marker
parameter_list|,
name|u_short
name|queue
parameter_list|)
block|{
name|bzero
argument_list|(
name|marker
argument_list|,
sizeof|sizeof
argument_list|(
operator|*
name|marker
argument_list|)
argument_list|)
expr_stmt|;
name|marker
operator|->
name|flags
operator|=
name|PG_MARKER
expr_stmt|;
name|marker
operator|->
name|busy_lock
operator|=
name|VPB_SINGLE_EXCLUSIVER
expr_stmt|;
name|marker
operator|->
name|queue
operator|=
name|queue
expr_stmt|;
name|marker
operator|->
name|hold_count
operator|=
literal|1
expr_stmt|;
block|}
end_function

begin_comment
comment|/*  * vm_pageout_fallback_object_lock:  *   * Lock vm object currently associated with `m'. VM_OBJECT_TRYWLOCK is  * known to have failed and page queue must be either PQ_ACTIVE or  * PQ_INACTIVE.  To avoid lock order violation, unlock the page queue  * while locking the vm object.  Use marker page to detect page queue  * changes and maintain notion of next page on page queue.  Return  * TRUE if no changes were detected, FALSE otherwise.  vm object is  * locked on return.  *   * This function depends on both the lock portion of struct vm_object  * and normal struct vm_page being type stable.  */
end_comment

begin_function
specifier|static
name|boolean_t
name|vm_pageout_fallback_object_lock
parameter_list|(
name|vm_page_t
name|m
parameter_list|,
name|vm_page_t
modifier|*
name|next
parameter_list|)
block|{
name|struct
name|vm_page
name|marker
decl_stmt|;
name|struct
name|vm_pagequeue
modifier|*
name|pq
decl_stmt|;
name|boolean_t
name|unchanged
decl_stmt|;
name|u_short
name|queue
decl_stmt|;
name|vm_object_t
name|object
decl_stmt|;
name|queue
operator|=
name|m
operator|->
name|queue
expr_stmt|;
name|vm_pageout_init_marker
argument_list|(
operator|&
name|marker
argument_list|,
name|queue
argument_list|)
expr_stmt|;
name|pq
operator|=
name|vm_page_pagequeue
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|object
operator|=
name|m
operator|->
name|object
expr_stmt|;
name|TAILQ_INSERT_AFTER
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
name|m
argument_list|,
operator|&
name|marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|VM_OBJECT_WLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
comment|/* 	 * The page's object might have changed, and/or the page might 	 * have moved from its original position in the queue.  If the 	 * page's object has changed, then the caller should abandon 	 * processing the page because the wrong object lock was 	 * acquired.  Use the marker's plinks.q, not the page's, to 	 * determine if the page has been moved.  The state of the 	 * page's plinks.q can be indeterminate; whereas, the marker's 	 * plinks.q must be valid. 	 */
operator|*
name|next
operator|=
name|TAILQ_NEXT
argument_list|(
operator|&
name|marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|unchanged
operator|=
name|m
operator|->
name|object
operator|==
name|object
operator|&&
name|m
operator|==
name|TAILQ_PREV
argument_list|(
operator|&
name|marker
argument_list|,
name|pglist
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
operator|!
name|unchanged
operator|||
name|m
operator|->
name|queue
operator|==
name|queue
argument_list|,
operator|(
literal|"page %p queue %d %d"
operator|,
name|m
operator|,
name|queue
operator|,
name|m
operator|->
name|queue
operator|)
argument_list|)
expr_stmt|;
name|TAILQ_REMOVE
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
operator|&
name|marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
return|return
operator|(
name|unchanged
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  * Lock the page while holding the page queue lock.  Use marker page  * to detect page queue changes and maintain notion of next page on  * page queue.  Return TRUE if no changes were detected, FALSE  * otherwise.  The page is locked on return. The page queue lock might  * be dropped and reacquired.  *  * This function depends on normal struct vm_page being type stable.  */
end_comment

begin_function
specifier|static
name|boolean_t
name|vm_pageout_page_lock
parameter_list|(
name|vm_page_t
name|m
parameter_list|,
name|vm_page_t
modifier|*
name|next
parameter_list|)
block|{
name|struct
name|vm_page
name|marker
decl_stmt|;
name|struct
name|vm_pagequeue
modifier|*
name|pq
decl_stmt|;
name|boolean_t
name|unchanged
decl_stmt|;
name|u_short
name|queue
decl_stmt|;
name|vm_page_lock_assert
argument_list|(
name|m
argument_list|,
name|MA_NOTOWNED
argument_list|)
expr_stmt|;
if|if
condition|(
name|vm_page_trylock
argument_list|(
name|m
argument_list|)
condition|)
return|return
operator|(
name|TRUE
operator|)
return|;
name|queue
operator|=
name|m
operator|->
name|queue
expr_stmt|;
name|vm_pageout_init_marker
argument_list|(
operator|&
name|marker
argument_list|,
name|queue
argument_list|)
expr_stmt|;
name|pq
operator|=
name|vm_page_pagequeue
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|TAILQ_INSERT_AFTER
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
name|m
argument_list|,
operator|&
name|marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
comment|/* Page queue might have changed. */
operator|*
name|next
operator|=
name|TAILQ_NEXT
argument_list|(
operator|&
name|marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|unchanged
operator|=
name|m
operator|==
name|TAILQ_PREV
argument_list|(
operator|&
name|marker
argument_list|,
name|pglist
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
operator|!
name|unchanged
operator|||
name|m
operator|->
name|queue
operator|==
name|queue
argument_list|,
operator|(
literal|"page %p queue %d %d"
operator|,
name|m
operator|,
name|queue
operator|,
name|m
operator|->
name|queue
operator|)
argument_list|)
expr_stmt|;
name|TAILQ_REMOVE
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
operator|&
name|marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
return|return
operator|(
name|unchanged
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  * vm_pageout_clean:  *  * Clean the page and remove it from the laundry.  *   * We set the busy bit to cause potential page faults on this page to  * block.  Note the careful timing, however, the busy bit isn't set till  * late and we cannot do anything that will mess with the page.  */
end_comment

begin_function
specifier|static
name|int
name|vm_pageout_clean
parameter_list|(
name|vm_page_t
name|m
parameter_list|)
block|{
name|vm_object_t
name|object
decl_stmt|;
name|vm_page_t
name|mc
index|[
literal|2
operator|*
name|vm_pageout_page_count
index|]
decl_stmt|,
name|pb
decl_stmt|,
name|ps
decl_stmt|;
name|int
name|pageout_count
decl_stmt|;
name|int
name|ib
decl_stmt|,
name|is
decl_stmt|,
name|page_base
decl_stmt|;
name|vm_pindex_t
name|pindex
init|=
name|m
operator|->
name|pindex
decl_stmt|;
name|vm_page_lock_assert
argument_list|(
name|m
argument_list|,
name|MA_OWNED
argument_list|)
expr_stmt|;
name|object
operator|=
name|m
operator|->
name|object
expr_stmt|;
name|VM_OBJECT_ASSERT_WLOCKED
argument_list|(
name|object
argument_list|)
expr_stmt|;
comment|/* 	 * It doesn't cost us anything to pageout OBJT_DEFAULT or OBJT_SWAP 	 * with the new swapper, but we could have serious problems paging 	 * out other object types if there is insufficient memory.   	 * 	 * Unfortunately, checking free memory here is far too late, so the 	 * check has been moved up a procedural level. 	 */
comment|/* 	 * Can't clean the page if it's busy or held. 	 */
name|vm_page_assert_unbusied
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
name|m
operator|->
name|hold_count
operator|==
literal|0
argument_list|,
operator|(
literal|"vm_pageout_clean: page %p is held"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|mc
index|[
name|vm_pageout_page_count
index|]
operator|=
name|pb
operator|=
name|ps
operator|=
name|m
expr_stmt|;
name|pageout_count
operator|=
literal|1
expr_stmt|;
name|page_base
operator|=
name|vm_pageout_page_count
expr_stmt|;
name|ib
operator|=
literal|1
expr_stmt|;
name|is
operator|=
literal|1
expr_stmt|;
comment|/* 	 * Scan object for clusterable pages. 	 * 	 * We can cluster ONLY if: ->> the page is NOT 	 * clean, wired, busy, held, or mapped into a 	 * buffer, and one of the following: 	 * 1) The page is inactive, or a seldom used 	 *    active page. 	 * -or- 	 * 2) we force the issue. 	 * 	 * During heavy mmap/modification loads the pageout 	 * daemon can really fragment the underlying file 	 * due to flushing pages out of order and not trying 	 * align the clusters (which leave sporatic out-of-order 	 * holes).  To solve this problem we do the reverse scan 	 * first and attempt to align our cluster, then do a  	 * forward scan if room remains. 	 */
name|more
label|:
while|while
condition|(
name|ib
operator|&&
name|pageout_count
operator|<
name|vm_pageout_page_count
condition|)
block|{
name|vm_page_t
name|p
decl_stmt|;
if|if
condition|(
name|ib
operator|>
name|pindex
condition|)
block|{
name|ib
operator|=
literal|0
expr_stmt|;
break|break;
block|}
if|if
condition|(
operator|(
name|p
operator|=
name|vm_page_prev
argument_list|(
name|pb
argument_list|)
operator|)
operator|==
name|NULL
operator|||
name|vm_page_busied
argument_list|(
name|p
argument_list|)
condition|)
block|{
name|ib
operator|=
literal|0
expr_stmt|;
break|break;
block|}
name|vm_page_test_dirty
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|dirty
operator|==
literal|0
condition|)
block|{
name|ib
operator|=
literal|0
expr_stmt|;
break|break;
block|}
name|vm_page_lock
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|queue
operator|!=
name|PQ_INACTIVE
operator|||
name|p
operator|->
name|hold_count
operator|!=
literal|0
condition|)
block|{
comment|/* may be undergoing I/O */
name|vm_page_unlock
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|ib
operator|=
literal|0
expr_stmt|;
break|break;
block|}
name|vm_page_unlock
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|mc
index|[
operator|--
name|page_base
index|]
operator|=
name|pb
operator|=
name|p
expr_stmt|;
operator|++
name|pageout_count
expr_stmt|;
operator|++
name|ib
expr_stmt|;
comment|/* 		 * alignment boundry, stop here and switch directions.  Do 		 * not clear ib. 		 */
if|if
condition|(
operator|(
name|pindex
operator|-
operator|(
name|ib
operator|-
literal|1
operator|)
operator|)
operator|%
name|vm_pageout_page_count
operator|==
literal|0
condition|)
break|break;
block|}
while|while
condition|(
name|pageout_count
operator|<
name|vm_pageout_page_count
operator|&&
name|pindex
operator|+
name|is
operator|<
name|object
operator|->
name|size
condition|)
block|{
name|vm_page_t
name|p
decl_stmt|;
if|if
condition|(
operator|(
name|p
operator|=
name|vm_page_next
argument_list|(
name|ps
argument_list|)
operator|)
operator|==
name|NULL
operator|||
name|vm_page_busied
argument_list|(
name|p
argument_list|)
condition|)
break|break;
name|vm_page_test_dirty
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|dirty
operator|==
literal|0
condition|)
break|break;
name|vm_page_lock
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|queue
operator|!=
name|PQ_INACTIVE
operator|||
name|p
operator|->
name|hold_count
operator|!=
literal|0
condition|)
block|{
comment|/* may be undergoing I/O */
name|vm_page_unlock
argument_list|(
name|p
argument_list|)
expr_stmt|;
break|break;
block|}
name|vm_page_unlock
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|mc
index|[
name|page_base
operator|+
name|pageout_count
index|]
operator|=
name|ps
operator|=
name|p
expr_stmt|;
operator|++
name|pageout_count
expr_stmt|;
operator|++
name|is
expr_stmt|;
block|}
comment|/* 	 * If we exhausted our forward scan, continue with the reverse scan 	 * when possible, even past a page boundry.  This catches boundry 	 * conditions. 	 */
if|if
condition|(
name|ib
operator|&&
name|pageout_count
operator|<
name|vm_pageout_page_count
condition|)
goto|goto
name|more
goto|;
comment|/* 	 * we allow reads during pageouts... 	 */
return|return
operator|(
name|vm_pageout_flush
argument_list|(
operator|&
name|mc
index|[
name|page_base
index|]
argument_list|,
name|pageout_count
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|,
name|NULL
argument_list|,
name|NULL
argument_list|)
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  * vm_pageout_flush() - launder the given pages  *  *	The given pages are laundered.  Note that we setup for the start of  *	I/O ( i.e. busy the page ), mark it read-only, and bump the object  *	reference count all in here rather then in the parent.  If we want  *	the parent to do more sophisticated things we may have to change  *	the ordering.  *  *	Returned runlen is the count of pages between mreq and first  *	page after mreq with status VM_PAGER_AGAIN.  *	*eio is set to TRUE if pager returned VM_PAGER_ERROR or VM_PAGER_FAIL  *	for any page in runlen set.  */
end_comment

begin_function
name|int
name|vm_pageout_flush
parameter_list|(
name|vm_page_t
modifier|*
name|mc
parameter_list|,
name|int
name|count
parameter_list|,
name|int
name|flags
parameter_list|,
name|int
name|mreq
parameter_list|,
name|int
modifier|*
name|prunlen
parameter_list|,
name|boolean_t
modifier|*
name|eio
parameter_list|)
block|{
name|vm_object_t
name|object
init|=
name|mc
index|[
literal|0
index|]
operator|->
name|object
decl_stmt|;
name|int
name|pageout_status
index|[
name|count
index|]
decl_stmt|;
name|int
name|numpagedout
init|=
literal|0
decl_stmt|;
name|int
name|i
decl_stmt|,
name|runlen
decl_stmt|;
name|VM_OBJECT_ASSERT_WLOCKED
argument_list|(
name|object
argument_list|)
expr_stmt|;
comment|/* 	 * Initiate I/O.  Bump the vm_page_t->busy counter and 	 * mark the pages read-only. 	 * 	 * We do not have to fixup the clean/dirty bits here... we can 	 * allow the pager to do it after the I/O completes. 	 * 	 * NOTE! mc[i]->dirty may be partial or fragmented due to an 	 * edge case with file fragments. 	 */
for|for
control|(
name|i
operator|=
literal|0
init|;
name|i
operator|<
name|count
condition|;
name|i
operator|++
control|)
block|{
name|KASSERT
argument_list|(
name|mc
index|[
name|i
index|]
operator|->
name|valid
operator|==
name|VM_PAGE_BITS_ALL
argument_list|,
operator|(
literal|"vm_pageout_flush: partially invalid page %p index %d/%d"
operator|,
name|mc
index|[
name|i
index|]
operator|,
name|i
operator|,
name|count
operator|)
argument_list|)
expr_stmt|;
name|vm_page_sbusy
argument_list|(
name|mc
index|[
name|i
index|]
argument_list|)
expr_stmt|;
name|pmap_remove_write
argument_list|(
name|mc
index|[
name|i
index|]
argument_list|)
expr_stmt|;
block|}
name|vm_object_pip_add
argument_list|(
name|object
argument_list|,
name|count
argument_list|)
expr_stmt|;
name|vm_pager_put_pages
argument_list|(
name|object
argument_list|,
name|mc
argument_list|,
name|count
argument_list|,
name|flags
argument_list|,
name|pageout_status
argument_list|)
expr_stmt|;
name|runlen
operator|=
name|count
operator|-
name|mreq
expr_stmt|;
if|if
condition|(
name|eio
operator|!=
name|NULL
condition|)
operator|*
name|eio
operator|=
name|FALSE
expr_stmt|;
for|for
control|(
name|i
operator|=
literal|0
init|;
name|i
operator|<
name|count
condition|;
name|i
operator|++
control|)
block|{
name|vm_page_t
name|mt
init|=
name|mc
index|[
name|i
index|]
decl_stmt|;
name|KASSERT
argument_list|(
name|pageout_status
index|[
name|i
index|]
operator|==
name|VM_PAGER_PEND
operator|||
operator|!
name|pmap_page_is_write_mapped
argument_list|(
name|mt
argument_list|)
argument_list|,
operator|(
literal|"vm_pageout_flush: page %p is not write protected"
operator|,
name|mt
operator|)
argument_list|)
expr_stmt|;
switch|switch
condition|(
name|pageout_status
index|[
name|i
index|]
condition|)
block|{
case|case
name|VM_PAGER_OK
case|:
case|case
name|VM_PAGER_PEND
case|:
name|numpagedout
operator|++
expr_stmt|;
break|break;
case|case
name|VM_PAGER_BAD
case|:
comment|/* 			 * Page outside of range of object. Right now we 			 * essentially lose the changes by pretending it 			 * worked. 			 */
name|vm_page_undirty
argument_list|(
name|mt
argument_list|)
expr_stmt|;
break|break;
case|case
name|VM_PAGER_ERROR
case|:
case|case
name|VM_PAGER_FAIL
case|:
comment|/* 			 * If page couldn't be paged out, then reactivate the 			 * page so it doesn't clog the inactive list.  (We 			 * will try paging out it again later). 			 */
name|vm_page_lock
argument_list|(
name|mt
argument_list|)
expr_stmt|;
name|vm_page_activate
argument_list|(
name|mt
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|mt
argument_list|)
expr_stmt|;
if|if
condition|(
name|eio
operator|!=
name|NULL
operator|&&
name|i
operator|>=
name|mreq
operator|&&
name|i
operator|-
name|mreq
operator|<
name|runlen
condition|)
operator|*
name|eio
operator|=
name|TRUE
expr_stmt|;
break|break;
case|case
name|VM_PAGER_AGAIN
case|:
if|if
condition|(
name|i
operator|>=
name|mreq
operator|&&
name|i
operator|-
name|mreq
operator|<
name|runlen
condition|)
name|runlen
operator|=
name|i
operator|-
name|mreq
expr_stmt|;
break|break;
block|}
comment|/* 		 * If the operation is still going, leave the page busy to 		 * block all other accesses. Also, leave the paging in 		 * progress indicator set so that we don't attempt an object 		 * collapse. 		 */
if|if
condition|(
name|pageout_status
index|[
name|i
index|]
operator|!=
name|VM_PAGER_PEND
condition|)
block|{
name|vm_object_pip_wakeup
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vm_page_sunbusy
argument_list|(
name|mt
argument_list|)
expr_stmt|;
if|if
condition|(
name|vm_page_count_severe
argument_list|()
condition|)
block|{
name|vm_page_lock
argument_list|(
name|mt
argument_list|)
expr_stmt|;
name|vm_page_try_to_cache
argument_list|(
name|mt
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|mt
argument_list|)
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|prunlen
operator|!=
name|NULL
condition|)
operator|*
name|prunlen
operator|=
name|runlen
expr_stmt|;
return|return
operator|(
name|numpagedout
operator|)
return|;
block|}
end_function

begin_function
specifier|static
name|boolean_t
name|vm_pageout_launder
parameter_list|(
name|struct
name|vm_pagequeue
modifier|*
name|pq
parameter_list|,
name|int
name|tries
parameter_list|,
name|vm_paddr_t
name|low
parameter_list|,
name|vm_paddr_t
name|high
parameter_list|)
block|{
name|struct
name|mount
modifier|*
name|mp
decl_stmt|;
name|struct
name|vnode
modifier|*
name|vp
decl_stmt|;
name|vm_object_t
name|object
decl_stmt|;
name|vm_paddr_t
name|pa
decl_stmt|;
name|vm_page_t
name|m
decl_stmt|,
name|m_tmp
decl_stmt|,
name|next
decl_stmt|;
name|int
name|lockmode
decl_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|TAILQ_FOREACH_SAFE
argument_list|(
argument|m
argument_list|,
argument|&pq->pq_pl
argument_list|,
argument|plinks.q
argument_list|,
argument|next
argument_list|)
block|{
if|if
condition|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_MARKER
operator|)
operator|!=
literal|0
condition|)
continue|continue;
name|pa
operator|=
name|VM_PAGE_TO_PHYS
argument_list|(
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|pa
operator|<
name|low
operator|||
name|pa
operator|+
name|PAGE_SIZE
operator|>
name|high
condition|)
continue|continue;
if|if
condition|(
operator|!
name|vm_pageout_page_lock
argument_list|(
name|m
argument_list|,
operator|&
name|next
argument_list|)
operator|||
name|m
operator|->
name|hold_count
operator|!=
literal|0
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|object
operator|=
name|m
operator|->
name|object
expr_stmt|;
if|if
condition|(
operator|(
operator|!
name|VM_OBJECT_TRYWLOCK
argument_list|(
name|object
argument_list|)
operator|&&
operator|(
operator|!
name|vm_pageout_fallback_object_lock
argument_list|(
name|m
argument_list|,
operator|&
name|next
argument_list|)
operator|||
name|m
operator|->
name|hold_count
operator|!=
literal|0
operator|)
operator|)
operator|||
name|vm_page_busied
argument_list|(
name|m
argument_list|)
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|vm_page_test_dirty
argument_list|(
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|m
operator|->
name|dirty
operator|==
literal|0
operator|&&
name|object
operator|->
name|ref_count
operator|!=
literal|0
condition|)
name|pmap_remove_all
argument_list|(
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|m
operator|->
name|dirty
operator|!=
literal|0
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|tries
operator|==
literal|0
operator|||
operator|(
name|object
operator|->
name|flags
operator|&
name|OBJ_DEAD
operator|)
operator|!=
literal|0
condition|)
block|{
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
name|object
operator|->
name|type
operator|==
name|OBJT_VNODE
condition|)
block|{
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|vp
operator|=
name|object
operator|->
name|handle
expr_stmt|;
name|vm_object_reference_locked
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
operator|(
name|void
operator|)
name|vn_start_write
argument_list|(
name|vp
argument_list|,
operator|&
name|mp
argument_list|,
name|V_WAIT
argument_list|)
expr_stmt|;
name|lockmode
operator|=
name|MNT_SHARED_WRITES
argument_list|(
name|vp
operator|->
name|v_mount
argument_list|)
condition|?
name|LK_SHARED
else|:
name|LK_EXCLUSIVE
expr_stmt|;
name|vn_lock
argument_list|(
name|vp
argument_list|,
name|lockmode
operator||
name|LK_RETRY
argument_list|)
expr_stmt|;
name|VM_OBJECT_WLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vm_object_page_clean
argument_list|(
name|object
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|,
name|OBJPC_SYNC
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|VOP_UNLOCK
argument_list|(
name|vp
argument_list|,
literal|0
argument_list|)
expr_stmt|;
name|vm_object_deallocate
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vn_finished_write
argument_list|(
name|mp
argument_list|)
expr_stmt|;
return|return
operator|(
name|TRUE
operator|)
return|;
block|}
elseif|else
if|if
condition|(
name|object
operator|->
name|type
operator|==
name|OBJT_SWAP
operator|||
name|object
operator|->
name|type
operator|==
name|OBJT_DEFAULT
condition|)
block|{
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|m_tmp
operator|=
name|m
expr_stmt|;
name|vm_pageout_flush
argument_list|(
operator|&
name|m_tmp
argument_list|,
literal|1
argument_list|,
name|VM_PAGER_PUT_SYNC
argument_list|,
literal|0
argument_list|,
name|NULL
argument_list|,
name|NULL
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
return|return
operator|(
name|TRUE
operator|)
return|;
block|}
block|}
else|else
block|{
comment|/* 			 * Dequeue here to prevent lock recursion in 			 * vm_page_cache(). 			 */
name|vm_page_dequeue_locked
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_page_cache
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
block|}
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
return|return
operator|(
name|FALSE
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  * Increase the number of cached pages.  The specified value, "tries",  * determines which categories of pages are cached:  *  *  0: All clean, inactive pages within the specified physical address range  *     are cached.  Will not sleep.  *  1: The vm_lowmem handlers are called.  All inactive pages within  *     the specified physical address range are cached.  May sleep.  *  2: The vm_lowmem handlers are called.  All inactive and active pages  *     within the specified physical address range are cached.  May sleep.  */
end_comment

begin_function
name|void
name|vm_pageout_grow_cache
parameter_list|(
name|int
name|tries
parameter_list|,
name|vm_paddr_t
name|low
parameter_list|,
name|vm_paddr_t
name|high
parameter_list|)
block|{
name|int
name|actl
decl_stmt|,
name|actmax
decl_stmt|,
name|inactl
decl_stmt|,
name|inactmax
decl_stmt|,
name|dom
decl_stmt|,
name|initial_dom
decl_stmt|;
specifier|static
name|int
name|start_dom
init|=
literal|0
decl_stmt|;
if|if
condition|(
name|tries
operator|>
literal|0
condition|)
block|{
comment|/* 		 * Decrease registered cache sizes.  The vm_lowmem handlers 		 * may acquire locks and/or sleep, so they can only be invoked 		 * when "tries" is greater than zero. 		 */
name|SDT_PROBE0
argument_list|(
name|vm
argument_list|, , ,
name|vm__lowmem_cache
argument_list|)
expr_stmt|;
name|EVENTHANDLER_INVOKE
argument_list|(
name|vm_lowmem
argument_list|,
literal|0
argument_list|)
expr_stmt|;
comment|/* 		 * We do this explicitly after the caches have been drained 		 * above. 		 */
name|uma_reclaim
argument_list|()
expr_stmt|;
block|}
comment|/* 	 * Make the next scan start on the next domain. 	 */
name|initial_dom
operator|=
name|atomic_fetchadd_int
argument_list|(
operator|&
name|start_dom
argument_list|,
literal|1
argument_list|)
operator|%
name|vm_ndomains
expr_stmt|;
name|inactl
operator|=
literal|0
expr_stmt|;
name|inactmax
operator|=
name|cnt
operator|.
name|v_inactive_count
expr_stmt|;
name|actl
operator|=
literal|0
expr_stmt|;
name|actmax
operator|=
name|tries
operator|<
literal|2
condition|?
literal|0
else|:
name|cnt
operator|.
name|v_active_count
expr_stmt|;
name|dom
operator|=
name|initial_dom
expr_stmt|;
comment|/* 	 * Scan domains in round-robin order, first inactive queues, 	 * then active.  Since domain usually owns large physically 	 * contiguous chunk of memory, it makes sense to completely 	 * exhaust one domain before switching to next, while growing 	 * the pool of contiguous physical pages. 	 * 	 * Do not even start launder a domain which cannot contain 	 * the specified address range, as indicated by segments 	 * constituting the domain. 	 */
name|again_inact
label|:
if|if
condition|(
name|inactl
operator|<
name|inactmax
condition|)
block|{
if|if
condition|(
name|vm_phys_domain_intersects
argument_list|(
name|vm_dom
index|[
name|dom
index|]
operator|.
name|vmd_segs
argument_list|,
name|low
argument_list|,
name|high
argument_list|)
operator|&&
name|vm_pageout_launder
argument_list|(
operator|&
name|vm_dom
index|[
name|dom
index|]
operator|.
name|vmd_pagequeues
index|[
name|PQ_INACTIVE
index|]
argument_list|,
name|tries
argument_list|,
name|low
argument_list|,
name|high
argument_list|)
condition|)
block|{
name|inactl
operator|++
expr_stmt|;
goto|goto
name|again_inact
goto|;
block|}
if|if
condition|(
operator|++
name|dom
operator|==
name|vm_ndomains
condition|)
name|dom
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|dom
operator|!=
name|initial_dom
condition|)
goto|goto
name|again_inact
goto|;
block|}
name|again_act
label|:
if|if
condition|(
name|actl
operator|<
name|actmax
condition|)
block|{
if|if
condition|(
name|vm_phys_domain_intersects
argument_list|(
name|vm_dom
index|[
name|dom
index|]
operator|.
name|vmd_segs
argument_list|,
name|low
argument_list|,
name|high
argument_list|)
operator|&&
name|vm_pageout_launder
argument_list|(
operator|&
name|vm_dom
index|[
name|dom
index|]
operator|.
name|vmd_pagequeues
index|[
name|PQ_ACTIVE
index|]
argument_list|,
name|tries
argument_list|,
name|low
argument_list|,
name|high
argument_list|)
condition|)
block|{
name|actl
operator|++
expr_stmt|;
goto|goto
name|again_act
goto|;
block|}
if|if
condition|(
operator|++
name|dom
operator|==
name|vm_ndomains
condition|)
name|dom
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|dom
operator|!=
name|initial_dom
condition|)
goto|goto
name|again_act
goto|;
block|}
block|}
end_function

begin_if
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_comment
comment|/*  *	vm_pageout_object_deactivate_pages  *  *	Deactivate enough pages to satisfy the inactive target  *	requirements.  *  *	The object and map must be locked.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout_object_deactivate_pages
parameter_list|(
name|pmap_t
name|pmap
parameter_list|,
name|vm_object_t
name|first_object
parameter_list|,
name|long
name|desired
parameter_list|)
block|{
name|vm_object_t
name|backing_object
decl_stmt|,
name|object
decl_stmt|;
name|vm_page_t
name|p
decl_stmt|;
name|int
name|act_delta
decl_stmt|,
name|remove_mode
decl_stmt|;
name|VM_OBJECT_ASSERT_LOCKED
argument_list|(
name|first_object
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|first_object
operator|->
name|flags
operator|&
name|OBJ_FICTITIOUS
operator|)
operator|!=
literal|0
condition|)
return|return;
for|for
control|(
name|object
operator|=
name|first_object
init|;
condition|;
name|object
operator|=
name|backing_object
control|)
block|{
if|if
condition|(
name|pmap_resident_count
argument_list|(
name|pmap
argument_list|)
operator|<=
name|desired
condition|)
goto|goto
name|unlock_return
goto|;
name|VM_OBJECT_ASSERT_LOCKED
argument_list|(
name|object
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|object
operator|->
name|flags
operator|&
name|OBJ_UNMANAGED
operator|)
operator|!=
literal|0
operator|||
name|object
operator|->
name|paging_in_progress
operator|!=
literal|0
condition|)
goto|goto
name|unlock_return
goto|;
name|remove_mode
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|object
operator|->
name|shadow_count
operator|>
literal|1
condition|)
name|remove_mode
operator|=
literal|1
expr_stmt|;
comment|/* 		 * Scan the object's entire memory queue. 		 */
name|TAILQ_FOREACH
argument_list|(
argument|p
argument_list|,
argument|&object->memq
argument_list|,
argument|listq
argument_list|)
block|{
if|if
condition|(
name|pmap_resident_count
argument_list|(
name|pmap
argument_list|)
operator|<=
name|desired
condition|)
goto|goto
name|unlock_return
goto|;
if|if
condition|(
name|vm_page_busied
argument_list|(
name|p
argument_list|)
condition|)
continue|continue;
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_pdpages
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|wire_count
operator|!=
literal|0
operator|||
name|p
operator|->
name|hold_count
operator|!=
literal|0
operator|||
operator|!
name|pmap_page_exists_quick
argument_list|(
name|pmap
argument_list|,
name|p
argument_list|)
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|act_delta
operator|=
name|pmap_ts_referenced
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|p
operator|->
name|aflags
operator|&
name|PGA_REFERENCED
operator|)
operator|!=
literal|0
condition|)
block|{
if|if
condition|(
name|act_delta
operator|==
literal|0
condition|)
name|act_delta
operator|=
literal|1
expr_stmt|;
name|vm_page_aflag_clear
argument_list|(
name|p
argument_list|,
name|PGA_REFERENCED
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|p
operator|->
name|queue
operator|!=
name|PQ_ACTIVE
operator|&&
name|act_delta
operator|!=
literal|0
condition|)
block|{
name|vm_page_activate
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|p
operator|->
name|act_count
operator|+=
name|act_delta
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|p
operator|->
name|queue
operator|==
name|PQ_ACTIVE
condition|)
block|{
if|if
condition|(
name|act_delta
operator|==
literal|0
condition|)
block|{
name|p
operator|->
name|act_count
operator|-=
name|min
argument_list|(
name|p
operator|->
name|act_count
argument_list|,
name|ACT_DECLINE
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|remove_mode
operator|&&
name|p
operator|->
name|act_count
operator|==
literal|0
condition|)
block|{
name|pmap_remove_all
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|vm_page_deactivate
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
else|else
name|vm_page_requeue
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|vm_page_activate
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|act_count
operator|<
name|ACT_MAX
operator|-
name|ACT_ADVANCE
condition|)
name|p
operator|->
name|act_count
operator|+=
name|ACT_ADVANCE
expr_stmt|;
name|vm_page_requeue
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|p
operator|->
name|queue
operator|==
name|PQ_INACTIVE
condition|)
name|pmap_remove_all
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|(
name|backing_object
operator|=
name|object
operator|->
name|backing_object
operator|)
operator|==
name|NULL
condition|)
goto|goto
name|unlock_return
goto|;
name|VM_OBJECT_RLOCK
argument_list|(
name|backing_object
argument_list|)
expr_stmt|;
if|if
condition|(
name|object
operator|!=
name|first_object
condition|)
name|VM_OBJECT_RUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
block|}
name|unlock_return
label|:
if|if
condition|(
name|object
operator|!=
name|first_object
condition|)
name|VM_OBJECT_RUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|/*  * deactivate some number of pages in a map, try to do it fairly, but  * that is really hard to do.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout_map_deactivate_pages
parameter_list|(
name|map
parameter_list|,
name|desired
parameter_list|)
name|vm_map_t
name|map
decl_stmt|;
name|long
name|desired
decl_stmt|;
block|{
name|vm_map_entry_t
name|tmpe
decl_stmt|;
name|vm_object_t
name|obj
decl_stmt|,
name|bigobj
decl_stmt|;
name|int
name|nothingwired
decl_stmt|;
if|if
condition|(
operator|!
name|vm_map_trylock
argument_list|(
name|map
argument_list|)
condition|)
return|return;
name|bigobj
operator|=
name|NULL
expr_stmt|;
name|nothingwired
operator|=
name|TRUE
expr_stmt|;
comment|/* 	 * first, search out the biggest object, and try to free pages from 	 * that. 	 */
name|tmpe
operator|=
name|map
operator|->
name|header
operator|.
name|next
expr_stmt|;
while|while
condition|(
name|tmpe
operator|!=
operator|&
name|map
operator|->
name|header
condition|)
block|{
if|if
condition|(
operator|(
name|tmpe
operator|->
name|eflags
operator|&
name|MAP_ENTRY_IS_SUB_MAP
operator|)
operator|==
literal|0
condition|)
block|{
name|obj
operator|=
name|tmpe
operator|->
name|object
operator|.
name|vm_object
expr_stmt|;
if|if
condition|(
name|obj
operator|!=
name|NULL
operator|&&
name|VM_OBJECT_TRYRLOCK
argument_list|(
name|obj
argument_list|)
condition|)
block|{
if|if
condition|(
name|obj
operator|->
name|shadow_count
operator|<=
literal|1
operator|&&
operator|(
name|bigobj
operator|==
name|NULL
operator|||
name|bigobj
operator|->
name|resident_page_count
operator|<
name|obj
operator|->
name|resident_page_count
operator|)
condition|)
block|{
if|if
condition|(
name|bigobj
operator|!=
name|NULL
condition|)
name|VM_OBJECT_RUNLOCK
argument_list|(
name|bigobj
argument_list|)
expr_stmt|;
name|bigobj
operator|=
name|obj
expr_stmt|;
block|}
else|else
name|VM_OBJECT_RUNLOCK
argument_list|(
name|obj
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|tmpe
operator|->
name|wired_count
operator|>
literal|0
condition|)
name|nothingwired
operator|=
name|FALSE
expr_stmt|;
name|tmpe
operator|=
name|tmpe
operator|->
name|next
expr_stmt|;
block|}
if|if
condition|(
name|bigobj
operator|!=
name|NULL
condition|)
block|{
name|vm_pageout_object_deactivate_pages
argument_list|(
name|map
operator|->
name|pmap
argument_list|,
name|bigobj
argument_list|,
name|desired
argument_list|)
expr_stmt|;
name|VM_OBJECT_RUNLOCK
argument_list|(
name|bigobj
argument_list|)
expr_stmt|;
block|}
comment|/* 	 * Next, hunt around for other pages to deactivate.  We actually 	 * do this search sort of wrong -- .text first is not the best idea. 	 */
name|tmpe
operator|=
name|map
operator|->
name|header
operator|.
name|next
expr_stmt|;
while|while
condition|(
name|tmpe
operator|!=
operator|&
name|map
operator|->
name|header
condition|)
block|{
if|if
condition|(
name|pmap_resident_count
argument_list|(
name|vm_map_pmap
argument_list|(
name|map
argument_list|)
argument_list|)
operator|<=
name|desired
condition|)
break|break;
if|if
condition|(
operator|(
name|tmpe
operator|->
name|eflags
operator|&
name|MAP_ENTRY_IS_SUB_MAP
operator|)
operator|==
literal|0
condition|)
block|{
name|obj
operator|=
name|tmpe
operator|->
name|object
operator|.
name|vm_object
expr_stmt|;
if|if
condition|(
name|obj
operator|!=
name|NULL
condition|)
block|{
name|VM_OBJECT_RLOCK
argument_list|(
name|obj
argument_list|)
expr_stmt|;
name|vm_pageout_object_deactivate_pages
argument_list|(
name|map
operator|->
name|pmap
argument_list|,
name|obj
argument_list|,
name|desired
argument_list|)
expr_stmt|;
name|VM_OBJECT_RUNLOCK
argument_list|(
name|obj
argument_list|)
expr_stmt|;
block|}
block|}
name|tmpe
operator|=
name|tmpe
operator|->
name|next
expr_stmt|;
block|}
ifdef|#
directive|ifdef
name|__ia64__
comment|/* 	 * Remove all non-wired, managed mappings if a process is swapped out. 	 * This will free page table pages. 	 */
if|if
condition|(
name|desired
operator|==
literal|0
condition|)
name|pmap_remove_pages
argument_list|(
name|map
operator|->
name|pmap
argument_list|)
expr_stmt|;
else|#
directive|else
comment|/* 	 * Remove all mappings if a process is swapped out, this will free page 	 * table pages. 	 */
if|if
condition|(
name|desired
operator|==
literal|0
operator|&&
name|nothingwired
condition|)
block|{
name|pmap_remove
argument_list|(
name|vm_map_pmap
argument_list|(
name|map
argument_list|)
argument_list|,
name|vm_map_min
argument_list|(
name|map
argument_list|)
argument_list|,
name|vm_map_max
argument_list|(
name|map
argument_list|)
argument_list|)
expr_stmt|;
block|}
endif|#
directive|endif
name|vm_map_unlock
argument_list|(
name|map
argument_list|)
expr_stmt|;
block|}
end_function

begin_endif
endif|#
directive|endif
end_endif

begin_comment
comment|/* !defined(NO_SWAPPING) */
end_comment

begin_comment
comment|/*  *	vm_pageout_scan does the dirty work for the pageout daemon.  *  *	pass 0 - Update active LRU/deactivate pages  *	pass 1 - Move inactive to cache or free  *	pass 2 - Launder dirty pages  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout_scan
parameter_list|(
name|struct
name|vm_domain
modifier|*
name|vmd
parameter_list|,
name|int
name|pass
parameter_list|)
block|{
name|vm_page_t
name|m
decl_stmt|,
name|next
decl_stmt|;
name|struct
name|vm_pagequeue
modifier|*
name|pq
decl_stmt|;
name|vm_object_t
name|object
decl_stmt|;
name|long
name|min_scan
decl_stmt|;
name|int
name|act_delta
decl_stmt|,
name|addl_page_shortage
decl_stmt|,
name|deficit
decl_stmt|,
name|maxscan
decl_stmt|,
name|page_shortage
decl_stmt|;
name|int
name|vnodes_skipped
init|=
literal|0
decl_stmt|;
name|int
name|maxlaunder
decl_stmt|,
name|scan_tick
decl_stmt|,
name|scanned
decl_stmt|,
name|starting_page_shortage
decl_stmt|;
name|int
name|lockmode
decl_stmt|;
name|boolean_t
name|queue_locked
decl_stmt|;
comment|/* 	 * If we need to reclaim memory ask kernel caches to return 	 * some.  We rate limit to avoid thrashing. 	 */
if|if
condition|(
name|vmd
operator|==
operator|&
name|vm_dom
index|[
literal|0
index|]
operator|&&
name|pass
operator|>
literal|0
operator|&&
operator|(
name|time_uptime
operator|-
name|lowmem_uptime
operator|)
operator|>=
name|lowmem_period
condition|)
block|{
comment|/* 		 * Decrease registered cache sizes. 		 */
name|SDT_PROBE0
argument_list|(
name|vm
argument_list|, , ,
name|vm__lowmem_scan
argument_list|)
expr_stmt|;
name|EVENTHANDLER_INVOKE
argument_list|(
name|vm_lowmem
argument_list|,
name|VM_LOW_PAGES
argument_list|)
expr_stmt|;
comment|/* 		 * We do this explicitly after the caches have been 		 * drained above. 		 */
name|uma_reclaim
argument_list|()
expr_stmt|;
name|lowmem_uptime
operator|=
name|time_uptime
expr_stmt|;
block|}
comment|/* 	 * The addl_page_shortage is the number of temporarily 	 * stuck pages in the inactive queue.  In other words, the 	 * number of pages from the inactive count that should be 	 * discounted in setting the target for the active queue scan. 	 */
name|addl_page_shortage
operator|=
literal|0
expr_stmt|;
comment|/* 	 * Calculate the number of pages we want to either free or move 	 * to the cache. 	 */
if|if
condition|(
name|pass
operator|>
literal|0
condition|)
block|{
name|deficit
operator|=
name|atomic_readandclear_int
argument_list|(
operator|&
name|vm_pageout_deficit
argument_list|)
expr_stmt|;
name|page_shortage
operator|=
name|vm_paging_target
argument_list|()
operator|+
name|deficit
expr_stmt|;
block|}
else|else
name|page_shortage
operator|=
name|deficit
operator|=
literal|0
expr_stmt|;
name|starting_page_shortage
operator|=
name|page_shortage
expr_stmt|;
comment|/* 	 * maxlaunder limits the number of dirty pages we flush per scan. 	 * For most systems a smaller value (16 or 32) is more robust under 	 * extreme memory and disk pressure because any unnecessary writes 	 * to disk can result in extreme performance degredation.  However, 	 * systems with excessive dirty pages (especially when MAP_NOSYNC is 	 * used) will die horribly with limited laundering.  If the pageout 	 * daemon cannot clean enough pages in the first pass, we let it go 	 * all out in succeeding passes. 	 */
if|if
condition|(
operator|(
name|maxlaunder
operator|=
name|vm_max_launder
operator|)
operator|<=
literal|1
condition|)
name|maxlaunder
operator|=
literal|1
expr_stmt|;
if|if
condition|(
name|pass
operator|>
literal|1
condition|)
name|maxlaunder
operator|=
literal|10000
expr_stmt|;
comment|/* 	 * Start scanning the inactive queue for pages we can move to the 	 * cache or free.  The scan will stop when the target is reached or 	 * we have scanned the entire inactive queue.  Note that m->act_count 	 * is not used to form decisions for the inactive queue, only for the 	 * active queue. 	 */
name|pq
operator|=
operator|&
name|vmd
operator|->
name|vmd_pagequeues
index|[
name|PQ_INACTIVE
index|]
expr_stmt|;
name|maxscan
operator|=
name|pq
operator|->
name|pq_cnt
expr_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|TRUE
expr_stmt|;
for|for
control|(
name|m
operator|=
name|TAILQ_FIRST
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|)
init|;
name|m
operator|!=
name|NULL
operator|&&
name|maxscan
operator|--
operator|>
literal|0
operator|&&
name|page_shortage
operator|>
literal|0
condition|;
name|m
operator|=
name|next
control|)
block|{
name|vm_pagequeue_assert_locked
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
name|queue_locked
argument_list|,
operator|(
literal|"unlocked inactive queue"
operator|)
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
name|m
operator|->
name|queue
operator|==
name|PQ_INACTIVE
argument_list|,
operator|(
literal|"Inactive queue %p"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_pdpages
argument_list|)
expr_stmt|;
name|next
operator|=
name|TAILQ_NEXT
argument_list|(
name|m
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
comment|/* 		 * skip marker pages 		 */
if|if
condition|(
name|m
operator|->
name|flags
operator|&
name|PG_MARKER
condition|)
continue|continue;
name|KASSERT
argument_list|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_FICTITIOUS
operator|)
operator|==
literal|0
argument_list|,
operator|(
literal|"Fictitious page %p cannot be in inactive queue"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
operator|(
name|m
operator|->
name|oflags
operator|&
name|VPO_UNMANAGED
operator|)
operator|==
literal|0
argument_list|,
operator|(
literal|"Unmanaged page %p cannot be in inactive queue"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
comment|/* 		 * The page or object lock acquisitions fail if the 		 * page was removed from the queue or moved to a 		 * different position within the queue.  In either 		 * case, addl_page_shortage should not be incremented. 		 */
if|if
condition|(
operator|!
name|vm_pageout_page_lock
argument_list|(
name|m
argument_list|,
operator|&
name|next
argument_list|)
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|object
operator|=
name|m
operator|->
name|object
expr_stmt|;
if|if
condition|(
operator|!
name|VM_OBJECT_TRYWLOCK
argument_list|(
name|object
argument_list|)
operator|&&
operator|!
name|vm_pageout_fallback_object_lock
argument_list|(
name|m
argument_list|,
operator|&
name|next
argument_list|)
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|/* 		 * Don't mess with busy pages, keep them at at the 		 * front of the queue, most likely they are being 		 * paged out.  Increment addl_page_shortage for busy 		 * pages, because they may leave the inactive queue 		 * shortly after page scan is finished. 		 */
if|if
condition|(
name|vm_page_busied
argument_list|(
name|m
argument_list|)
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|addl_page_shortage
operator|++
expr_stmt|;
continue|continue;
block|}
comment|/* 		 * We unlock the inactive page queue, invalidating the 		 * 'next' pointer.  Use our marker to remember our 		 * place. 		 */
name|TAILQ_INSERT_AFTER
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
name|m
argument_list|,
operator|&
name|vmd
operator|->
name|vmd_marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|FALSE
expr_stmt|;
comment|/* 		 * We bump the activation count if the page has been 		 * referenced while in the inactive queue.  This makes 		 * it less likely that the page will be added back to the 		 * inactive queue prematurely again.  Here we check the  		 * page tables (or emulated bits, if any), given the upper  		 * level VM system not knowing anything about existing  		 * references. 		 */
name|act_delta
operator|=
literal|0
expr_stmt|;
if|if
condition|(
operator|(
name|m
operator|->
name|aflags
operator|&
name|PGA_REFERENCED
operator|)
operator|!=
literal|0
condition|)
block|{
name|vm_page_aflag_clear
argument_list|(
name|m
argument_list|,
name|PGA_REFERENCED
argument_list|)
expr_stmt|;
name|act_delta
operator|=
literal|1
expr_stmt|;
block|}
if|if
condition|(
name|object
operator|->
name|ref_count
operator|!=
literal|0
condition|)
block|{
name|act_delta
operator|+=
name|pmap_ts_referenced
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|KASSERT
argument_list|(
operator|!
name|pmap_page_is_mapped
argument_list|(
name|m
argument_list|)
argument_list|,
operator|(
literal|"vm_pageout_scan: page %p is mapped"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
block|}
comment|/* 		 * If the upper level VM system knows about any page  		 * references, we reactivate the page or requeue it. 		 */
if|if
condition|(
name|act_delta
operator|!=
literal|0
condition|)
block|{
if|if
condition|(
name|object
operator|->
name|ref_count
condition|)
block|{
name|vm_page_activate
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|m
operator|->
name|act_count
operator|+=
name|act_delta
operator|+
name|ACT_ADVANCE
expr_stmt|;
block|}
else|else
block|{
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|TRUE
expr_stmt|;
name|vm_page_requeue_locked
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
goto|goto
name|relock_queue
goto|;
block|}
if|if
condition|(
name|m
operator|->
name|hold_count
operator|!=
literal|0
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
comment|/* 			 * Held pages are essentially stuck in the 			 * queue.  So, they ought to be discounted 			 * from the inactive count.  See the 			 * calculation of the page_shortage for the 			 * loop over the active queue below. 			 */
name|addl_page_shortage
operator|++
expr_stmt|;
goto|goto
name|relock_queue
goto|;
block|}
comment|/* 		 * If the page appears to be clean at the machine-independent 		 * layer, then remove all of its mappings from the pmap in 		 * anticipation of placing it onto the cache queue.  If, 		 * however, any of the page's mappings allow write access, 		 * then the page may still be modified until the last of those 		 * mappings are removed. 		 */
if|if
condition|(
name|object
operator|->
name|ref_count
operator|!=
literal|0
condition|)
block|{
name|vm_page_test_dirty
argument_list|(
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|m
operator|->
name|dirty
operator|==
literal|0
condition|)
name|pmap_remove_all
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|m
operator|->
name|valid
operator|==
literal|0
condition|)
block|{
comment|/* 			 * Invalid pages can be easily freed 			 */
name|vm_page_free
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_dfree
argument_list|)
expr_stmt|;
operator|--
name|page_shortage
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|m
operator|->
name|dirty
operator|==
literal|0
condition|)
block|{
comment|/* 			 * Clean pages can be placed onto the cache queue. 			 * This effectively frees them. 			 */
name|vm_page_cache
argument_list|(
name|m
argument_list|)
expr_stmt|;
operator|--
name|page_shortage
expr_stmt|;
block|}
elseif|else
if|if
condition|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_WINATCFLS
operator|)
operator|==
literal|0
operator|&&
name|pass
operator|<
literal|2
condition|)
block|{
comment|/* 			 * Dirty pages need to be paged out, but flushing 			 * a page is extremely expensive verses freeing 			 * a clean page.  Rather then artificially limiting 			 * the number of pages we can flush, we instead give 			 * dirty pages extra priority on the inactive queue 			 * by forcing them to be cycled through the queue 			 * twice before being flushed, after which the 			 * (now clean) page will cycle through once more 			 * before being freed.  This significantly extends 			 * the thrash point for a heavily loaded machine. 			 */
name|m
operator|->
name|flags
operator||=
name|PG_WINATCFLS
expr_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|TRUE
expr_stmt|;
name|vm_page_requeue_locked
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|maxlaunder
operator|>
literal|0
condition|)
block|{
comment|/* 			 * We always want to try to flush some dirty pages if 			 * we encounter them, to keep the system stable. 			 * Normally this number is small, but under extreme 			 * pressure where there are insufficient clean pages 			 * on the inactive queue, we may have to go all out. 			 */
name|int
name|swap_pageouts_ok
decl_stmt|;
name|struct
name|vnode
modifier|*
name|vp
init|=
name|NULL
decl_stmt|;
name|struct
name|mount
modifier|*
name|mp
init|=
name|NULL
decl_stmt|;
if|if
condition|(
operator|(
name|object
operator|->
name|type
operator|!=
name|OBJT_SWAP
operator|)
operator|&&
operator|(
name|object
operator|->
name|type
operator|!=
name|OBJT_DEFAULT
operator|)
condition|)
block|{
name|swap_pageouts_ok
operator|=
literal|1
expr_stmt|;
block|}
else|else
block|{
name|swap_pageouts_ok
operator|=
operator|!
operator|(
name|defer_swap_pageouts
operator|||
name|disable_swap_pageouts
operator|)
expr_stmt|;
name|swap_pageouts_ok
operator||=
operator|(
operator|!
name|disable_swap_pageouts
operator|&&
name|defer_swap_pageouts
operator|&&
name|vm_page_count_min
argument_list|()
operator|)
expr_stmt|;
block|}
comment|/* 			 * We don't bother paging objects that are "dead".   			 * Those objects are in a "rundown" state. 			 */
if|if
condition|(
operator|!
name|swap_pageouts_ok
operator|||
operator|(
name|object
operator|->
name|flags
operator|&
name|OBJ_DEAD
operator|)
condition|)
block|{
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|TRUE
expr_stmt|;
name|vm_page_requeue_locked
argument_list|(
name|m
argument_list|)
expr_stmt|;
goto|goto
name|relock_queue
goto|;
block|}
comment|/* 			 * The object is already known NOT to be dead.   It 			 * is possible for the vget() to block the whole 			 * pageout daemon, but the new low-memory handling 			 * code should prevent it. 			 * 			 * The previous code skipped locked vnodes and, worse, 			 * reordered pages in the queue.  This results in 			 * completely non-deterministic operation and, on a 			 * busy system, can lead to extremely non-optimal 			 * pageouts.  For example, it can cause clean pages 			 * to be freed and dirty pages to be moved to the end 			 * of the queue.  Since dirty pages are also moved to 			 * the end of the queue once-cleaned, this gives 			 * way too large a weighting to defering the freeing 			 * of dirty pages. 			 * 			 * We can't wait forever for the vnode lock, we might 			 * deadlock due to a vn_read() getting stuck in 			 * vm_wait while holding this vnode.  We skip the  			 * vnode if we can't get it in a reasonable amount 			 * of time. 			 */
if|if
condition|(
name|object
operator|->
name|type
operator|==
name|OBJT_VNODE
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vp
operator|=
name|object
operator|->
name|handle
expr_stmt|;
if|if
condition|(
name|vp
operator|->
name|v_type
operator|==
name|VREG
operator|&&
name|vn_start_write
argument_list|(
name|vp
argument_list|,
operator|&
name|mp
argument_list|,
name|V_NOWAIT
argument_list|)
operator|!=
literal|0
condition|)
block|{
name|mp
operator|=
name|NULL
expr_stmt|;
operator|++
name|pageout_lock_miss
expr_stmt|;
if|if
condition|(
name|object
operator|->
name|flags
operator|&
name|OBJ_MIGHTBEDIRTY
condition|)
name|vnodes_skipped
operator|++
expr_stmt|;
goto|goto
name|unlock_and_continue
goto|;
block|}
name|KASSERT
argument_list|(
name|mp
operator|!=
name|NULL
argument_list|,
operator|(
literal|"vp %p with NULL v_mount"
operator|,
name|vp
operator|)
argument_list|)
expr_stmt|;
name|vm_object_reference_locked
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|lockmode
operator|=
name|MNT_SHARED_WRITES
argument_list|(
name|vp
operator|->
name|v_mount
argument_list|)
condition|?
name|LK_SHARED
else|:
name|LK_EXCLUSIVE
expr_stmt|;
if|if
condition|(
name|vget
argument_list|(
name|vp
argument_list|,
name|lockmode
operator||
name|LK_TIMELOCK
argument_list|,
name|curthread
argument_list|)
condition|)
block|{
name|VM_OBJECT_WLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
operator|++
name|pageout_lock_miss
expr_stmt|;
if|if
condition|(
name|object
operator|->
name|flags
operator|&
name|OBJ_MIGHTBEDIRTY
condition|)
name|vnodes_skipped
operator|++
expr_stmt|;
name|vp
operator|=
name|NULL
expr_stmt|;
goto|goto
name|unlock_and_continue
goto|;
block|}
name|VM_OBJECT_WLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|TRUE
expr_stmt|;
comment|/* 				 * The page might have been moved to another 				 * queue during potential blocking in vget() 				 * above.  The page might have been freed and 				 * reused for another vnode. 				 */
if|if
condition|(
name|m
operator|->
name|queue
operator|!=
name|PQ_INACTIVE
operator|||
name|m
operator|->
name|object
operator|!=
name|object
operator|||
name|TAILQ_NEXT
argument_list|(
name|m
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
operator|!=
operator|&
name|vmd
operator|->
name|vmd_marker
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|object
operator|->
name|flags
operator|&
name|OBJ_MIGHTBEDIRTY
condition|)
name|vnodes_skipped
operator|++
expr_stmt|;
goto|goto
name|unlock_and_continue
goto|;
block|}
comment|/* 				 * The page may have been busied during the 				 * blocking in vget().  We don't move the 				 * page back onto the end of the queue so that 				 * statistics are more correct if we don't. 				 */
if|if
condition|(
name|vm_page_busied
argument_list|(
name|m
argument_list|)
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|addl_page_shortage
operator|++
expr_stmt|;
goto|goto
name|unlock_and_continue
goto|;
block|}
comment|/* 				 * If the page has become held it might 				 * be undergoing I/O, so skip it 				 */
if|if
condition|(
name|m
operator|->
name|hold_count
operator|!=
literal|0
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|addl_page_shortage
operator|++
expr_stmt|;
if|if
condition|(
name|object
operator|->
name|flags
operator|&
name|OBJ_MIGHTBEDIRTY
condition|)
name|vnodes_skipped
operator|++
expr_stmt|;
goto|goto
name|unlock_and_continue
goto|;
block|}
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|FALSE
expr_stmt|;
block|}
comment|/* 			 * If a page is dirty, then it is either being washed 			 * (but not yet cleaned) or it is still in the 			 * laundry.  If it is still in the laundry, then we 			 * start the cleaning operation.  			 * 			 * decrement page_shortage on success to account for 			 * the (future) cleaned page.  Otherwise we could wind 			 * up laundering or cleaning too many pages. 			 */
if|if
condition|(
name|vm_pageout_clean
argument_list|(
name|m
argument_list|)
operator|!=
literal|0
condition|)
block|{
operator|--
name|page_shortage
expr_stmt|;
operator|--
name|maxlaunder
expr_stmt|;
block|}
name|unlock_and_continue
label|:
name|vm_page_lock_assert
argument_list|(
name|m
argument_list|,
name|MA_NOTOWNED
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
if|if
condition|(
name|mp
operator|!=
name|NULL
condition|)
block|{
if|if
condition|(
name|queue_locked
condition|)
block|{
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|FALSE
expr_stmt|;
block|}
if|if
condition|(
name|vp
operator|!=
name|NULL
condition|)
name|vput
argument_list|(
name|vp
argument_list|)
expr_stmt|;
name|vm_object_deallocate
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vn_finished_write
argument_list|(
name|mp
argument_list|)
expr_stmt|;
block|}
name|vm_page_lock_assert
argument_list|(
name|m
argument_list|,
name|MA_NOTOWNED
argument_list|)
expr_stmt|;
goto|goto
name|relock_queue
goto|;
block|}
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|relock_queue
label|:
if|if
condition|(
operator|!
name|queue_locked
condition|)
block|{
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|TRUE
expr_stmt|;
block|}
name|next
operator|=
name|TAILQ_NEXT
argument_list|(
operator|&
name|vmd
operator|->
name|vmd_marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|TAILQ_REMOVE
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
operator|&
name|vmd
operator|->
name|vmd_marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
block|}
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
comment|/* 	 * Wakeup the swapout daemon if we didn't cache or free the targeted 	 * number of pages.  	 */
if|if
condition|(
name|vm_swap_enabled
operator|&&
name|page_shortage
operator|>
literal|0
condition|)
name|vm_req_vmdaemon
argument_list|(
name|VM_SWAP_NORMAL
argument_list|)
expr_stmt|;
endif|#
directive|endif
comment|/* 	 * Wakeup the sync daemon if we skipped a vnode in a writeable object 	 * and we didn't cache or free enough pages. 	 */
if|if
condition|(
name|vnodes_skipped
operator|>
literal|0
operator|&&
name|page_shortage
operator|>
name|cnt
operator|.
name|v_free_target
operator|-
name|cnt
operator|.
name|v_free_min
condition|)
operator|(
name|void
operator|)
name|speedup_syncer
argument_list|()
expr_stmt|;
comment|/* 	 * If the inactive queue scan fails repeatedly to meet its 	 * target, kill the largest process. 	 */
name|vm_pageout_mightbe_oom
argument_list|(
name|vmd
argument_list|,
name|page_shortage
argument_list|,
name|starting_page_shortage
argument_list|)
expr_stmt|;
comment|/* 	 * Compute the number of pages we want to try to move from the 	 * active queue to the inactive queue. 	 */
name|page_shortage
operator|=
name|cnt
operator|.
name|v_inactive_target
operator|-
name|cnt
operator|.
name|v_inactive_count
operator|+
name|vm_paging_target
argument_list|()
operator|+
name|deficit
operator|+
name|addl_page_shortage
expr_stmt|;
name|pq
operator|=
operator|&
name|vmd
operator|->
name|vmd_pagequeues
index|[
name|PQ_ACTIVE
index|]
expr_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|maxscan
operator|=
name|pq
operator|->
name|pq_cnt
expr_stmt|;
comment|/* 	 * If we're just idle polling attempt to visit every 	 * active page within 'update_period' seconds. 	 */
name|scan_tick
operator|=
name|ticks
expr_stmt|;
if|if
condition|(
name|vm_pageout_update_period
operator|!=
literal|0
condition|)
block|{
name|min_scan
operator|=
name|pq
operator|->
name|pq_cnt
expr_stmt|;
name|min_scan
operator|*=
name|scan_tick
operator|-
name|vmd
operator|->
name|vmd_last_active_scan
expr_stmt|;
name|min_scan
operator|/=
name|hz
operator|*
name|vm_pageout_update_period
expr_stmt|;
block|}
else|else
name|min_scan
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|min_scan
operator|>
literal|0
operator|||
operator|(
name|page_shortage
operator|>
literal|0
operator|&&
name|maxscan
operator|>
literal|0
operator|)
condition|)
name|vmd
operator|->
name|vmd_last_active_scan
operator|=
name|scan_tick
expr_stmt|;
comment|/* 	 * Scan the active queue for pages that can be deactivated.  Update 	 * the per-page activity counter and use it to identify deactivation 	 * candidates.  Held pages may be deactivated. 	 */
for|for
control|(
name|m
operator|=
name|TAILQ_FIRST
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|)
operator|,
name|scanned
operator|=
literal|0
init|;
name|m
operator|!=
name|NULL
operator|&&
operator|(
name|scanned
operator|<
name|min_scan
operator|||
operator|(
name|page_shortage
operator|>
literal|0
operator|&&
name|scanned
operator|<
name|maxscan
operator|)
operator|)
condition|;
name|m
operator|=
name|next
operator|,
name|scanned
operator|++
control|)
block|{
name|KASSERT
argument_list|(
name|m
operator|->
name|queue
operator|==
name|PQ_ACTIVE
argument_list|,
operator|(
literal|"vm_pageout_scan: page %p isn't active"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
name|next
operator|=
name|TAILQ_NEXT
argument_list|(
name|m
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_MARKER
operator|)
operator|!=
literal|0
condition|)
continue|continue;
name|KASSERT
argument_list|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_FICTITIOUS
operator|)
operator|==
literal|0
argument_list|,
operator|(
literal|"Fictitious page %p cannot be in active queue"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
operator|(
name|m
operator|->
name|oflags
operator|&
name|VPO_UNMANAGED
operator|)
operator|==
literal|0
argument_list|,
operator|(
literal|"Unmanaged page %p cannot be in active queue"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|vm_pageout_page_lock
argument_list|(
name|m
argument_list|,
operator|&
name|next
argument_list|)
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|/* 		 * The count for page daemon pages is updated after checking 		 * the page for eligibility. 		 */
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_pdpages
argument_list|)
expr_stmt|;
comment|/* 		 * Check to see "how much" the page has been used. 		 */
name|act_delta
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|m
operator|->
name|aflags
operator|&
name|PGA_REFERENCED
condition|)
block|{
name|vm_page_aflag_clear
argument_list|(
name|m
argument_list|,
name|PGA_REFERENCED
argument_list|)
expr_stmt|;
name|act_delta
operator|+=
literal|1
expr_stmt|;
block|}
comment|/* 		 * Perform an unsynchronized object ref count check.  While 		 * the page lock ensures that the page is not reallocated to 		 * another object, in particular, one with unmanaged mappings 		 * that cannot support pmap_ts_referenced(), two races are, 		 * nonetheless, possible: 		 * 1) The count was transitioning to zero, but we saw a non- 		 *    zero value.  pmap_ts_referenced() will return zero 		 *    because the page is not mapped. 		 * 2) The count was transitioning to one, but we saw zero.  		 *    This race delays the detection of a new reference.  At 		 *    worst, we will deactivate and reactivate the page. 		 */
if|if
condition|(
name|m
operator|->
name|object
operator|->
name|ref_count
operator|!=
literal|0
condition|)
name|act_delta
operator|+=
name|pmap_ts_referenced
argument_list|(
name|m
argument_list|)
expr_stmt|;
comment|/* 		 * Advance or decay the act_count based on recent usage. 		 */
if|if
condition|(
name|act_delta
condition|)
block|{
name|m
operator|->
name|act_count
operator|+=
name|ACT_ADVANCE
operator|+
name|act_delta
expr_stmt|;
if|if
condition|(
name|m
operator|->
name|act_count
operator|>
name|ACT_MAX
condition|)
name|m
operator|->
name|act_count
operator|=
name|ACT_MAX
expr_stmt|;
block|}
else|else
block|{
name|m
operator|->
name|act_count
operator|-=
name|min
argument_list|(
name|m
operator|->
name|act_count
argument_list|,
name|ACT_DECLINE
argument_list|)
expr_stmt|;
name|act_delta
operator|=
name|m
operator|->
name|act_count
expr_stmt|;
block|}
comment|/* 		 * Move this page to the tail of the active or inactive 		 * queue depending on usage. 		 */
if|if
condition|(
name|act_delta
operator|==
literal|0
condition|)
block|{
comment|/* Dequeue to avoid later lock recursion. */
name|vm_page_dequeue_locked
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_page_deactivate
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|page_shortage
operator|--
expr_stmt|;
block|}
else|else
name|vm_page_requeue_locked
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
comment|/* 	 * Idle process swapout -- run once per second. 	 */
if|if
condition|(
name|vm_swap_idle_enabled
condition|)
block|{
specifier|static
name|long
name|lsec
decl_stmt|;
if|if
condition|(
name|time_second
operator|!=
name|lsec
condition|)
block|{
name|vm_req_vmdaemon
argument_list|(
name|VM_SWAP_IDLE
argument_list|)
expr_stmt|;
name|lsec
operator|=
name|time_second
expr_stmt|;
block|}
block|}
endif|#
directive|endif
block|}
end_function

begin_decl_stmt
specifier|static
name|int
name|vm_pageout_oom_vote
decl_stmt|;
end_decl_stmt

begin_comment
comment|/*  * The pagedaemon threads randlomly select one to perform the  * OOM.  Trying to kill processes before all pagedaemons  * failed to reach free target is premature.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout_mightbe_oom
parameter_list|(
name|struct
name|vm_domain
modifier|*
name|vmd
parameter_list|,
name|int
name|page_shortage
parameter_list|,
name|int
name|starting_page_shortage
parameter_list|)
block|{
name|int
name|old_vote
decl_stmt|;
if|if
condition|(
name|starting_page_shortage
operator|<=
literal|0
operator|||
name|starting_page_shortage
operator|!=
name|page_shortage
condition|)
name|vmd
operator|->
name|vmd_oom_seq
operator|=
literal|0
expr_stmt|;
else|else
name|vmd
operator|->
name|vmd_oom_seq
operator|++
expr_stmt|;
if|if
condition|(
name|vmd
operator|->
name|vmd_oom_seq
operator|<
name|vm_pageout_oom_seq
condition|)
block|{
if|if
condition|(
name|vmd
operator|->
name|vmd_oom
condition|)
block|{
name|vmd
operator|->
name|vmd_oom
operator|=
name|FALSE
expr_stmt|;
name|atomic_subtract_int
argument_list|(
operator|&
name|vm_pageout_oom_vote
argument_list|,
literal|1
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
comment|/* 	 * Do not follow the call sequence until OOM condition is 	 * cleared. 	 */
name|vmd
operator|->
name|vmd_oom_seq
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|vmd
operator|->
name|vmd_oom
condition|)
return|return;
name|vmd
operator|->
name|vmd_oom
operator|=
name|TRUE
expr_stmt|;
name|old_vote
operator|=
name|atomic_fetchadd_int
argument_list|(
operator|&
name|vm_pageout_oom_vote
argument_list|,
literal|1
argument_list|)
expr_stmt|;
if|if
condition|(
name|old_vote
operator|!=
name|vm_ndomains
operator|-
literal|1
condition|)
return|return;
comment|/* 	 * The current pagedaemon thread is the last in the quorum to 	 * start OOM.  Initiate the selection and signaling of the 	 * victim. 	 */
name|vm_pageout_oom
argument_list|(
name|VM_OOM_MEM
argument_list|)
expr_stmt|;
comment|/* 	 * After one round of OOM terror, recall our vote.  On the 	 * next pass, current pagedaemon would vote again if the low 	 * memory condition is still there, due to vmd_oom being 	 * false. 	 */
name|vmd
operator|->
name|vmd_oom
operator|=
name|FALSE
expr_stmt|;
name|atomic_subtract_int
argument_list|(
operator|&
name|vm_pageout_oom_vote
argument_list|,
literal|1
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|/*  * The OOM killer is the page daemon's action of last resort when  * memory allocation requests have been stalled for a prolonged period  * of time because it cannot reclaim memory.  This function computes  * the approximate number of physical pages that could be reclaimed if  * the specified address space is destroyed.  *  * Private, anonymous memory owned by the address space is the  * principal resource that we expect to recover after an OOM kill.  * Since the physical pages mapped by the address space's COW entries  * are typically shared pages, they are unlikely to be released and so  * they are not counted.  *  * To get to the point where the page daemon runs the OOM killer, its  * efforts to write-back vnode-backed pages may have stalled.  This  * could be caused by a memory allocation deadlock in the write path  * that might be resolved by an OOM kill.  Therefore, physical pages  * belonging to vnode-backed objects are counted, because they might  * be freed without being written out first if the address space holds  * the last reference to an unlinked vnode.  *  * Similarly, physical pages belonging to OBJT_PHYS objects are  * counted because the address space might hold the last reference to  * the object.  */
end_comment

begin_function
specifier|static
name|long
name|vm_pageout_oom_pagecount
parameter_list|(
name|struct
name|vmspace
modifier|*
name|vmspace
parameter_list|)
block|{
name|vm_map_t
name|map
decl_stmt|;
name|vm_map_entry_t
name|entry
decl_stmt|;
name|vm_object_t
name|obj
decl_stmt|;
name|long
name|res
decl_stmt|;
name|map
operator|=
operator|&
name|vmspace
operator|->
name|vm_map
expr_stmt|;
name|KASSERT
argument_list|(
operator|!
name|map
operator|->
name|system_map
argument_list|,
operator|(
literal|"system map"
operator|)
argument_list|)
expr_stmt|;
name|sx_assert
argument_list|(
operator|&
name|map
operator|->
name|lock
argument_list|,
name|SA_LOCKED
argument_list|)
expr_stmt|;
name|res
operator|=
literal|0
expr_stmt|;
for|for
control|(
name|entry
operator|=
name|map
operator|->
name|header
operator|.
name|next
init|;
name|entry
operator|!=
operator|&
name|map
operator|->
name|header
condition|;
name|entry
operator|=
name|entry
operator|->
name|next
control|)
block|{
if|if
condition|(
operator|(
name|entry
operator|->
name|eflags
operator|&
name|MAP_ENTRY_IS_SUB_MAP
operator|)
operator|!=
literal|0
condition|)
continue|continue;
name|obj
operator|=
name|entry
operator|->
name|object
operator|.
name|vm_object
expr_stmt|;
if|if
condition|(
name|obj
operator|==
name|NULL
condition|)
continue|continue;
if|if
condition|(
operator|(
name|entry
operator|->
name|eflags
operator|&
name|MAP_ENTRY_NEEDS_COPY
operator|)
operator|!=
literal|0
operator|&&
name|obj
operator|->
name|ref_count
operator|!=
literal|1
condition|)
continue|continue;
switch|switch
condition|(
name|obj
operator|->
name|type
condition|)
block|{
case|case
name|OBJT_DEFAULT
case|:
case|case
name|OBJT_SWAP
case|:
case|case
name|OBJT_PHYS
case|:
case|case
name|OBJT_VNODE
case|:
name|res
operator|+=
name|obj
operator|->
name|resident_page_count
expr_stmt|;
break|break;
block|}
block|}
return|return
operator|(
name|res
operator|)
return|;
block|}
end_function

begin_function
name|void
name|vm_pageout_oom
parameter_list|(
name|int
name|shortage
parameter_list|)
block|{
name|struct
name|proc
modifier|*
name|p
decl_stmt|,
modifier|*
name|bigproc
decl_stmt|;
name|vm_offset_t
name|size
decl_stmt|,
name|bigsize
decl_stmt|;
name|struct
name|thread
modifier|*
name|td
decl_stmt|;
name|struct
name|vmspace
modifier|*
name|vm
decl_stmt|;
name|bool
name|breakout
decl_stmt|;
comment|/* 	 * We keep the process bigproc locked once we find it to keep anyone 	 * from messing with it; however, there is a possibility of 	 * deadlock if process B is bigproc and one of it's child processes 	 * attempts to propagate a signal to B while we are waiting for A's 	 * lock while walking this list.  To avoid this, we don't block on 	 * the process lock but just skip a process if it is already locked. 	 */
name|bigproc
operator|=
name|NULL
expr_stmt|;
name|bigsize
operator|=
literal|0
expr_stmt|;
name|sx_slock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
name|FOREACH_PROC_IN_SYSTEM
argument_list|(
argument|p
argument_list|)
block|{
name|PROC_LOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
comment|/* 		 * If this is a system, protected or killed process, skip it. 		 */
if|if
condition|(
name|p
operator|->
name|p_state
operator|!=
name|PRS_NORMAL
operator|||
operator|(
name|p
operator|->
name|p_flag
operator|&
operator|(
name|P_INEXEC
operator||
name|P_PROTECTED
operator||
name|P_SYSTEM
operator||
name|P_WEXIT
operator|)
operator|)
operator|!=
literal|0
operator|||
name|p
operator|->
name|p_pid
operator|==
literal|1
operator|||
name|P_KILLED
argument_list|(
name|p
argument_list|)
operator|||
operator|(
name|p
operator|->
name|p_pid
operator|<
literal|48
operator|&&
name|swap_pager_avail
operator|!=
literal|0
operator|)
condition|)
block|{
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|/* 		 * If the process is in a non-running type state, 		 * don't touch it.  Check all the threads individually. 		 */
name|breakout
operator|=
name|false
expr_stmt|;
name|FOREACH_THREAD_IN_PROC
argument_list|(
argument|p
argument_list|,
argument|td
argument_list|)
block|{
name|thread_lock
argument_list|(
name|td
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|TD_ON_RUNQ
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_RUNNING
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_SLEEPING
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_SUSPENDED
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_SWAPPED
argument_list|(
name|td
argument_list|)
condition|)
block|{
name|thread_unlock
argument_list|(
name|td
argument_list|)
expr_stmt|;
name|breakout
operator|=
name|true
expr_stmt|;
break|break;
block|}
name|thread_unlock
argument_list|(
name|td
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|breakout
condition|)
block|{
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|/* 		 * get the process size 		 */
name|vm
operator|=
name|vmspace_acquire_ref
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|vm
operator|==
name|NULL
condition|)
block|{
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|_PHOLD
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|vm_map_trylock_read
argument_list|(
operator|&
name|vm
operator|->
name|vm_map
argument_list|)
condition|)
block|{
name|_PRELE
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|vmspace_free
argument_list|(
name|vm
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|size
operator|=
name|vmspace_swap_count
argument_list|(
name|vm
argument_list|)
expr_stmt|;
if|if
condition|(
name|shortage
operator|==
name|VM_OOM_MEM
condition|)
name|size
operator|+=
name|vm_pageout_oom_pagecount
argument_list|(
name|vm
argument_list|)
expr_stmt|;
name|vm_map_unlock_read
argument_list|(
operator|&
name|vm
operator|->
name|vm_map
argument_list|)
expr_stmt|;
name|vmspace_free
argument_list|(
name|vm
argument_list|)
expr_stmt|;
comment|/* 		 * If this process is bigger than the biggest one, 		 * remember it. 		 */
if|if
condition|(
name|size
operator|>
name|bigsize
condition|)
block|{
if|if
condition|(
name|bigproc
operator|!=
name|NULL
condition|)
name|PRELE
argument_list|(
name|bigproc
argument_list|)
expr_stmt|;
name|bigproc
operator|=
name|p
expr_stmt|;
name|bigsize
operator|=
name|size
expr_stmt|;
block|}
else|else
block|{
name|PRELE
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
block|}
name|sx_sunlock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
if|if
condition|(
name|bigproc
operator|!=
name|NULL
condition|)
block|{
name|PROC_LOCK
argument_list|(
name|bigproc
argument_list|)
expr_stmt|;
name|killproc
argument_list|(
name|bigproc
argument_list|,
literal|"out of swap space"
argument_list|)
expr_stmt|;
name|sched_nice
argument_list|(
name|bigproc
argument_list|,
name|PRIO_MIN
argument_list|)
expr_stmt|;
name|_PRELE
argument_list|(
name|bigproc
argument_list|)
expr_stmt|;
name|PROC_UNLOCK
argument_list|(
name|bigproc
argument_list|)
expr_stmt|;
name|wakeup
argument_list|(
operator|&
name|cnt
operator|.
name|v_free_count
argument_list|)
expr_stmt|;
block|}
block|}
end_function

begin_function
specifier|static
name|void
name|vm_pageout_worker
parameter_list|(
name|void
modifier|*
name|arg
parameter_list|)
block|{
name|struct
name|vm_domain
modifier|*
name|domain
decl_stmt|;
name|int
name|domidx
decl_stmt|;
name|domidx
operator|=
operator|(
name|uintptr_t
operator|)
name|arg
expr_stmt|;
name|domain
operator|=
operator|&
name|vm_dom
index|[
name|domidx
index|]
expr_stmt|;
comment|/* 	 * XXXKIB It could be useful to bind pageout daemon threads to 	 * the cores belonging to the domain, from which vm_page_array 	 * is allocated. 	 */
name|KASSERT
argument_list|(
name|domain
operator|->
name|vmd_segs
operator|!=
literal|0
argument_list|,
operator|(
literal|"domain without segments"
operator|)
argument_list|)
expr_stmt|;
name|domain
operator|->
name|vmd_last_active_scan
operator|=
name|ticks
expr_stmt|;
name|vm_pageout_init_marker
argument_list|(
operator|&
name|domain
operator|->
name|vmd_marker
argument_list|,
name|PQ_INACTIVE
argument_list|)
expr_stmt|;
comment|/* 	 * The pageout daemon worker is never done, so loop forever. 	 */
while|while
condition|(
name|TRUE
condition|)
block|{
comment|/* 		 * If we have enough free memory, wakeup waiters.  Do 		 * not clear vm_pages_needed until we reach our target, 		 * otherwise we may be woken up over and over again and 		 * waste a lot of cpu. 		 */
name|mtx_lock
argument_list|(
operator|&
name|vm_page_queue_free_mtx
argument_list|)
expr_stmt|;
if|if
condition|(
name|vm_pages_needed
operator|&&
operator|!
name|vm_page_count_min
argument_list|()
condition|)
block|{
if|if
condition|(
operator|!
name|vm_paging_needed
argument_list|()
condition|)
name|vm_pages_needed
operator|=
literal|0
expr_stmt|;
name|wakeup
argument_list|(
operator|&
name|cnt
operator|.
name|v_free_count
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|vm_pages_needed
condition|)
block|{
comment|/* 			 * We're still not done.  Either vm_pages_needed was 			 * set by another thread during the previous scan 			 * (typically, this happens during a level 0 scan) or 			 * vm_pages_needed was already set and the scan failed 			 * to free enough pages.  If we haven't yet performed 			 * a level>= 2 scan (unlimited dirty cleaning), then 			 * upgrade the level and scan again now.  Otherwise, 			 * sleep a bit and try again later.  While sleeping, 			 * vm_pages_needed can be cleared. 			 */
if|if
condition|(
name|domain
operator|->
name|vmd_pass
operator|>
literal|1
condition|)
name|msleep
argument_list|(
operator|&
name|vm_pages_needed
argument_list|,
operator|&
name|vm_page_queue_free_mtx
argument_list|,
name|PVM
argument_list|,
literal|"psleep"
argument_list|,
name|hz
operator|/
literal|2
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|/* 			 * Good enough, sleep until required to refresh 			 * stats. 			 */
name|msleep
argument_list|(
operator|&
name|vm_pages_needed
argument_list|,
operator|&
name|vm_page_queue_free_mtx
argument_list|,
name|PVM
argument_list|,
literal|"psleep"
argument_list|,
name|hz
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|vm_pages_needed
condition|)
block|{
name|cnt
operator|.
name|v_pdwakeups
operator|++
expr_stmt|;
name|domain
operator|->
name|vmd_pass
operator|++
expr_stmt|;
block|}
else|else
name|domain
operator|->
name|vmd_pass
operator|=
literal|0
expr_stmt|;
name|mtx_unlock
argument_list|(
operator|&
name|vm_page_queue_free_mtx
argument_list|)
expr_stmt|;
name|vm_pageout_scan
argument_list|(
name|domain
argument_list|,
name|domain
operator|->
name|vmd_pass
argument_list|)
expr_stmt|;
block|}
block|}
end_function

begin_comment
comment|/*  *	vm_pageout_init initialises basic pageout daemon settings.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout_init
parameter_list|(
name|void
parameter_list|)
block|{
comment|/* 	 * Initialize some paging parameters. 	 */
name|cnt
operator|.
name|v_interrupt_free_min
operator|=
literal|2
expr_stmt|;
if|if
condition|(
name|cnt
operator|.
name|v_page_count
operator|<
literal|2000
condition|)
name|vm_pageout_page_count
operator|=
literal|8
expr_stmt|;
comment|/* 	 * v_free_reserved needs to include enough for the largest 	 * swap pager structures plus enough for any pv_entry structs 	 * when paging.  	 */
if|if
condition|(
name|cnt
operator|.
name|v_page_count
operator|>
literal|1024
condition|)
name|cnt
operator|.
name|v_free_min
operator|=
literal|4
operator|+
operator|(
name|cnt
operator|.
name|v_page_count
operator|-
literal|1024
operator|)
operator|/
literal|200
expr_stmt|;
else|else
name|cnt
operator|.
name|v_free_min
operator|=
literal|4
expr_stmt|;
name|cnt
operator|.
name|v_pageout_free_min
operator|=
operator|(
literal|2
operator|*
name|MAXBSIZE
operator|)
operator|/
name|PAGE_SIZE
operator|+
name|cnt
operator|.
name|v_interrupt_free_min
expr_stmt|;
name|cnt
operator|.
name|v_free_reserved
operator|=
name|vm_pageout_page_count
operator|+
name|cnt
operator|.
name|v_pageout_free_min
operator|+
operator|(
name|cnt
operator|.
name|v_page_count
operator|/
literal|768
operator|)
expr_stmt|;
name|cnt
operator|.
name|v_free_severe
operator|=
name|cnt
operator|.
name|v_free_min
operator|/
literal|2
expr_stmt|;
name|cnt
operator|.
name|v_free_target
operator|=
literal|4
operator|*
name|cnt
operator|.
name|v_free_min
operator|+
name|cnt
operator|.
name|v_free_reserved
expr_stmt|;
name|cnt
operator|.
name|v_free_min
operator|+=
name|cnt
operator|.
name|v_free_reserved
expr_stmt|;
name|cnt
operator|.
name|v_free_severe
operator|+=
name|cnt
operator|.
name|v_free_reserved
expr_stmt|;
name|cnt
operator|.
name|v_inactive_target
operator|=
operator|(
literal|3
operator|*
name|cnt
operator|.
name|v_free_target
operator|)
operator|/
literal|2
expr_stmt|;
if|if
condition|(
name|cnt
operator|.
name|v_inactive_target
operator|>
name|cnt
operator|.
name|v_free_count
operator|/
literal|3
condition|)
name|cnt
operator|.
name|v_inactive_target
operator|=
name|cnt
operator|.
name|v_free_count
operator|/
literal|3
expr_stmt|;
comment|/* 	 * Set the default wakeup threshold to be 10% above the minimum 	 * page limit.  This keeps the steady state out of shortfall. 	 */
name|vm_pageout_wakeup_thresh
operator|=
operator|(
name|cnt
operator|.
name|v_free_min
operator|/
literal|10
operator|)
operator|*
literal|11
expr_stmt|;
comment|/* 	 * Set interval in seconds for active scan.  We want to visit each 	 * page at least once every ten minutes.  This is to prevent worst 	 * case paging behaviors with stale active LRU. 	 */
if|if
condition|(
name|vm_pageout_update_period
operator|==
literal|0
condition|)
name|vm_pageout_update_period
operator|=
literal|600
expr_stmt|;
comment|/* XXX does not really belong here */
if|if
condition|(
name|vm_page_max_wired
operator|==
literal|0
condition|)
name|vm_page_max_wired
operator|=
name|cnt
operator|.
name|v_free_count
operator|/
literal|3
expr_stmt|;
block|}
end_function

begin_comment
comment|/*  *     vm_pageout is the high level pageout daemon.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout
parameter_list|(
name|void
parameter_list|)
block|{
name|int
name|error
decl_stmt|;
if|#
directive|if
name|MAXMEMDOM
operator|>
literal|1
name|int
name|i
decl_stmt|;
endif|#
directive|endif
name|swap_pager_swap_init
argument_list|()
expr_stmt|;
if|#
directive|if
name|MAXMEMDOM
operator|>
literal|1
for|for
control|(
name|i
operator|=
literal|1
init|;
name|i
operator|<
name|vm_ndomains
condition|;
name|i
operator|++
control|)
block|{
name|error
operator|=
name|kthread_add
argument_list|(
name|vm_pageout_worker
argument_list|,
operator|(
name|void
operator|*
operator|)
operator|(
name|uintptr_t
operator|)
name|i
argument_list|,
name|curproc
argument_list|,
name|NULL
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|,
literal|"dom%d"
argument_list|,
name|i
argument_list|)
expr_stmt|;
if|if
condition|(
name|error
operator|!=
literal|0
condition|)
block|{
name|panic
argument_list|(
literal|"starting pageout for domain %d, error %d\n"
argument_list|,
name|i
argument_list|,
name|error
argument_list|)
expr_stmt|;
block|}
block|}
endif|#
directive|endif
name|error
operator|=
name|kthread_add
argument_list|(
name|uma_reclaim_worker
argument_list|,
name|NULL
argument_list|,
name|curproc
argument_list|,
name|NULL
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|,
literal|"uma"
argument_list|)
expr_stmt|;
if|if
condition|(
name|error
operator|!=
literal|0
condition|)
name|panic
argument_list|(
literal|"starting uma_reclaim helper, error %d\n"
argument_list|,
name|error
argument_list|)
expr_stmt|;
name|vm_pageout_worker
argument_list|(
operator|(
name|void
operator|*
operator|)
operator|(
name|uintptr_t
operator|)
literal|0
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|/*  * Unless the free page queue lock is held by the caller, this function  * should be regarded as advisory.  Specifically, the caller should  * not msleep() on&cnt.v_free_count following this function unless  * the free page queue lock is held until the msleep() is performed.  */
end_comment

begin_function
name|void
name|pagedaemon_wakeup
parameter_list|(
name|void
parameter_list|)
block|{
if|if
condition|(
operator|!
name|vm_pages_needed
operator|&&
name|curthread
operator|->
name|td_proc
operator|!=
name|pageproc
condition|)
block|{
name|vm_pages_needed
operator|=
literal|1
expr_stmt|;
name|wakeup
argument_list|(
operator|&
name|vm_pages_needed
argument_list|)
expr_stmt|;
block|}
block|}
end_function

begin_if
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_function
specifier|static
name|void
name|vm_req_vmdaemon
parameter_list|(
name|int
name|req
parameter_list|)
block|{
specifier|static
name|int
name|lastrun
init|=
literal|0
decl_stmt|;
name|mtx_lock
argument_list|(
operator|&
name|vm_daemon_mtx
argument_list|)
expr_stmt|;
name|vm_pageout_req_swapout
operator||=
name|req
expr_stmt|;
if|if
condition|(
operator|(
name|ticks
operator|>
operator|(
name|lastrun
operator|+
name|hz
operator|)
operator|)
operator|||
operator|(
name|ticks
operator|<
name|lastrun
operator|)
condition|)
block|{
name|wakeup
argument_list|(
operator|&
name|vm_daemon_needed
argument_list|)
expr_stmt|;
name|lastrun
operator|=
name|ticks
expr_stmt|;
block|}
name|mtx_unlock
argument_list|(
operator|&
name|vm_daemon_mtx
argument_list|)
expr_stmt|;
block|}
end_function

begin_function
specifier|static
name|void
name|vm_daemon
parameter_list|(
name|void
parameter_list|)
block|{
name|struct
name|rlimit
name|rsslim
decl_stmt|;
name|struct
name|proc
modifier|*
name|p
decl_stmt|;
name|struct
name|thread
modifier|*
name|td
decl_stmt|;
name|struct
name|vmspace
modifier|*
name|vm
decl_stmt|;
name|int
name|breakout
decl_stmt|,
name|swapout_flags
decl_stmt|,
name|tryagain
decl_stmt|,
name|attempts
decl_stmt|;
ifdef|#
directive|ifdef
name|RACCT
name|uint64_t
name|rsize
decl_stmt|,
name|ravailable
decl_stmt|;
endif|#
directive|endif
while|while
condition|(
name|TRUE
condition|)
block|{
name|mtx_lock
argument_list|(
operator|&
name|vm_daemon_mtx
argument_list|)
expr_stmt|;
name|msleep
argument_list|(
operator|&
name|vm_daemon_needed
argument_list|,
operator|&
name|vm_daemon_mtx
argument_list|,
name|PPAUSE
argument_list|,
literal|"psleep"
argument_list|,
ifdef|#
directive|ifdef
name|RACCT
name|racct_enable
condition|?
name|hz
else|:
literal|0
else|#
directive|else
literal|0
endif|#
directive|endif
argument_list|)
expr_stmt|;
name|swapout_flags
operator|=
name|vm_pageout_req_swapout
expr_stmt|;
name|vm_pageout_req_swapout
operator|=
literal|0
expr_stmt|;
name|mtx_unlock
argument_list|(
operator|&
name|vm_daemon_mtx
argument_list|)
expr_stmt|;
if|if
condition|(
name|swapout_flags
condition|)
name|swapout_procs
argument_list|(
name|swapout_flags
argument_list|)
expr_stmt|;
comment|/* 		 * scan the processes for exceeding their rlimits or if 		 * process is swapped out -- deactivate pages 		 */
name|tryagain
operator|=
literal|0
expr_stmt|;
name|attempts
operator|=
literal|0
expr_stmt|;
name|again
label|:
name|attempts
operator|++
expr_stmt|;
name|sx_slock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
name|FOREACH_PROC_IN_SYSTEM
argument_list|(
argument|p
argument_list|)
block|{
name|vm_pindex_t
name|limit
decl_stmt|,
name|size
decl_stmt|;
comment|/* 			 * if this is a system process or if we have already 			 * looked at this process, skip it. 			 */
name|PROC_LOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|p_state
operator|!=
name|PRS_NORMAL
operator|||
name|p
operator|->
name|p_flag
operator|&
operator|(
name|P_INEXEC
operator||
name|P_SYSTEM
operator||
name|P_WEXIT
operator|)
condition|)
block|{
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|/* 			 * if the process is in a non-running type state, 			 * don't touch it. 			 */
name|breakout
operator|=
literal|0
expr_stmt|;
name|FOREACH_THREAD_IN_PROC
argument_list|(
argument|p
argument_list|,
argument|td
argument_list|)
block|{
name|thread_lock
argument_list|(
name|td
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|TD_ON_RUNQ
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_RUNNING
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_SLEEPING
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_SUSPENDED
argument_list|(
name|td
argument_list|)
condition|)
block|{
name|thread_unlock
argument_list|(
name|td
argument_list|)
expr_stmt|;
name|breakout
operator|=
literal|1
expr_stmt|;
break|break;
block|}
name|thread_unlock
argument_list|(
name|td
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|breakout
condition|)
block|{
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|/* 			 * get a limit 			 */
name|lim_rlimit
argument_list|(
name|p
argument_list|,
name|RLIMIT_RSS
argument_list|,
operator|&
name|rsslim
argument_list|)
expr_stmt|;
name|limit
operator|=
name|OFF_TO_IDX
argument_list|(
name|qmin
argument_list|(
name|rsslim
operator|.
name|rlim_cur
argument_list|,
name|rsslim
operator|.
name|rlim_max
argument_list|)
argument_list|)
expr_stmt|;
comment|/* 			 * let processes that are swapped out really be 			 * swapped out set the limit to nothing (will force a 			 * swap-out.) 			 */
if|if
condition|(
operator|(
name|p
operator|->
name|p_flag
operator|&
name|P_INMEM
operator|)
operator|==
literal|0
condition|)
name|limit
operator|=
literal|0
expr_stmt|;
comment|/* XXX */
name|vm
operator|=
name|vmspace_acquire_ref
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|vm
operator|==
name|NULL
condition|)
continue|continue;
name|size
operator|=
name|vmspace_resident_count
argument_list|(
name|vm
argument_list|)
expr_stmt|;
if|if
condition|(
name|size
operator|>=
name|limit
condition|)
block|{
name|vm_pageout_map_deactivate_pages
argument_list|(
operator|&
name|vm
operator|->
name|vm_map
argument_list|,
name|limit
argument_list|)
expr_stmt|;
name|size
operator|=
name|vmspace_resident_count
argument_list|(
name|vm
argument_list|)
expr_stmt|;
block|}
ifdef|#
directive|ifdef
name|RACCT
if|if
condition|(
name|racct_enable
condition|)
block|{
name|rsize
operator|=
name|IDX_TO_OFF
argument_list|(
name|size
argument_list|)
expr_stmt|;
name|PROC_LOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|p_state
operator|==
name|PRS_NORMAL
condition|)
name|racct_set
argument_list|(
name|p
argument_list|,
name|RACCT_RSS
argument_list|,
name|rsize
argument_list|)
expr_stmt|;
name|ravailable
operator|=
name|racct_get_available
argument_list|(
name|p
argument_list|,
name|RACCT_RSS
argument_list|)
expr_stmt|;
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|rsize
operator|>
name|ravailable
condition|)
block|{
comment|/* 					 * Don't be overly aggressive; this 					 * might be an innocent process, 					 * and the limit could've been exceeded 					 * by some memory hog.  Don't try 					 * to deactivate more than 1/4th 					 * of process' resident set size. 					 */
if|if
condition|(
name|attempts
operator|<=
literal|8
condition|)
block|{
if|if
condition|(
name|ravailable
operator|<
name|rsize
operator|-
operator|(
name|rsize
operator|/
literal|4
operator|)
condition|)
block|{
name|ravailable
operator|=
name|rsize
operator|-
operator|(
name|rsize
operator|/
literal|4
operator|)
expr_stmt|;
block|}
block|}
name|vm_pageout_map_deactivate_pages
argument_list|(
operator|&
name|vm
operator|->
name|vm_map
argument_list|,
name|OFF_TO_IDX
argument_list|(
name|ravailable
argument_list|)
argument_list|)
expr_stmt|;
comment|/* Update RSS usage after paging out. */
name|size
operator|=
name|vmspace_resident_count
argument_list|(
name|vm
argument_list|)
expr_stmt|;
name|rsize
operator|=
name|IDX_TO_OFF
argument_list|(
name|size
argument_list|)
expr_stmt|;
name|PROC_LOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|p_state
operator|==
name|PRS_NORMAL
condition|)
name|racct_set
argument_list|(
name|p
argument_list|,
name|RACCT_RSS
argument_list|,
name|rsize
argument_list|)
expr_stmt|;
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|rsize
operator|>
name|ravailable
condition|)
name|tryagain
operator|=
literal|1
expr_stmt|;
block|}
block|}
endif|#
directive|endif
name|vmspace_free
argument_list|(
name|vm
argument_list|)
expr_stmt|;
block|}
name|sx_sunlock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
if|if
condition|(
name|tryagain
operator|!=
literal|0
operator|&&
name|attempts
operator|<=
literal|10
condition|)
goto|goto
name|again
goto|;
block|}
block|}
end_function

begin_endif
endif|#
directive|endif
end_endif

begin_comment
comment|/* !defined(NO_SWAPPING) */
end_comment

end_unit

