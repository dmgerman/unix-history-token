begin_unit|revision:0.9.5;language:C;cregit-version:0.0.1
begin_comment
comment|/*-  * Copyright (c) 1991 Regents of the University of California.  * All rights reserved.  * Copyright (c) 1994 John S. Dyson  * All rights reserved.  * Copyright (c) 1994 David Greenman  * All rights reserved.  * Copyright (c) 2005 Yahoo! Technologies Norway AS  * All rights reserved.  *  * This code is derived from software contributed to Berkeley by  * The Mach Operating System project at Carnegie-Mellon University.  *  * Redistribution and use in source and binary forms, with or without  * modification, are permitted provided that the following conditions  * are met:  * 1. Redistributions of source code must retain the above copyright  *    notice, this list of conditions and the following disclaimer.  * 2. Redistributions in binary form must reproduce the above copyright  *    notice, this list of conditions and the following disclaimer in the  *    documentation and/or other materials provided with the distribution.  * 3. All advertising materials mentioning features or use of this software  *    must display the following acknowledgement:  *	This product includes software developed by the University of  *	California, Berkeley and its contributors.  * 4. Neither the name of the University nor the names of its contributors  *    may be used to endorse or promote products derived from this software  *    without specific prior written permission.  *  * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND  * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE  * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE  * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS  * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)  * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT  * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF  * SUCH DAMAGE.  *  *	from: @(#)vm_pageout.c	7.4 (Berkeley) 5/7/91  *  *  * Copyright (c) 1987, 1990 Carnegie-Mellon University.  * All rights reserved.  *  * Authors: Avadis Tevanian, Jr., Michael Wayne Young  *  * Permission to use, copy, modify and distribute this software and  * its documentation is hereby granted, provided that both the copyright  * notice and this permission notice appear in all copies of the  * software, derivative works or modified versions, and any portions  * thereof, and that both notices appear in supporting documentation.  *  * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"  * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND  * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.  *  * Carnegie Mellon requests users of this software to return to  *  *  Software Distribution Coordinator  or  Software.Distribution@CS.CMU.EDU  *  School of Computer Science  *  Carnegie Mellon University  *  Pittsburgh PA 15213-3890  *  * any improvements or extensions that they make and grant Carnegie the  * rights to redistribute these changes.  */
end_comment

begin_comment
comment|/*  *	The proverbial page-out daemon.  */
end_comment

begin_include
include|#
directive|include
file|<sys/cdefs.h>
end_include

begin_expr_stmt
name|__FBSDID
argument_list|(
literal|"$FreeBSD$"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_include
include|#
directive|include
file|"opt_vm.h"
end_include

begin_include
include|#
directive|include
file|<sys/param.h>
end_include

begin_include
include|#
directive|include
file|<sys/systm.h>
end_include

begin_include
include|#
directive|include
file|<sys/kernel.h>
end_include

begin_include
include|#
directive|include
file|<sys/eventhandler.h>
end_include

begin_include
include|#
directive|include
file|<sys/lock.h>
end_include

begin_include
include|#
directive|include
file|<sys/mutex.h>
end_include

begin_include
include|#
directive|include
file|<sys/proc.h>
end_include

begin_include
include|#
directive|include
file|<sys/kthread.h>
end_include

begin_include
include|#
directive|include
file|<sys/ktr.h>
end_include

begin_include
include|#
directive|include
file|<sys/mount.h>
end_include

begin_include
include|#
directive|include
file|<sys/racct.h>
end_include

begin_include
include|#
directive|include
file|<sys/resourcevar.h>
end_include

begin_include
include|#
directive|include
file|<sys/sched.h>
end_include

begin_include
include|#
directive|include
file|<sys/sdt.h>
end_include

begin_include
include|#
directive|include
file|<sys/signalvar.h>
end_include

begin_include
include|#
directive|include
file|<sys/smp.h>
end_include

begin_include
include|#
directive|include
file|<sys/time.h>
end_include

begin_include
include|#
directive|include
file|<sys/vnode.h>
end_include

begin_include
include|#
directive|include
file|<sys/vmmeter.h>
end_include

begin_include
include|#
directive|include
file|<sys/rwlock.h>
end_include

begin_include
include|#
directive|include
file|<sys/sx.h>
end_include

begin_include
include|#
directive|include
file|<sys/sysctl.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_param.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_object.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_page.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_map.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_pageout.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_pager.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_phys.h>
end_include

begin_include
include|#
directive|include
file|<vm/swap_pager.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_extern.h>
end_include

begin_include
include|#
directive|include
file|<vm/uma.h>
end_include

begin_comment
comment|/*  * System initialization  */
end_comment

begin_comment
comment|/* the kernel process "vm_pageout"*/
end_comment

begin_function_decl
specifier|static
name|void
name|vm_pageout
parameter_list|(
name|void
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|void
name|vm_pageout_init
parameter_list|(
name|void
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|int
name|vm_pageout_clean
parameter_list|(
name|vm_page_t
name|m
parameter_list|,
name|int
modifier|*
name|numpagedout
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|int
name|vm_pageout_cluster
parameter_list|(
name|vm_page_t
name|m
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|bool
name|vm_pageout_scan
parameter_list|(
name|struct
name|vm_domain
modifier|*
name|vmd
parameter_list|,
name|int
name|pass
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|void
name|vm_pageout_mightbe_oom
parameter_list|(
name|struct
name|vm_domain
modifier|*
name|vmd
parameter_list|,
name|int
name|page_shortage
parameter_list|,
name|int
name|starting_page_shortage
parameter_list|)
function_decl|;
end_function_decl

begin_expr_stmt
name|SYSINIT
argument_list|(
name|pagedaemon_init
argument_list|,
name|SI_SUB_KTHREAD_PAGE
argument_list|,
name|SI_ORDER_FIRST
argument_list|,
name|vm_pageout_init
argument_list|,
name|NULL
argument_list|)
expr_stmt|;
end_expr_stmt

begin_decl_stmt
name|struct
name|proc
modifier|*
name|pageproc
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|struct
name|kproc_desc
name|page_kp
init|=
block|{
literal|"pagedaemon"
block|,
name|vm_pageout
block|,
operator|&
name|pageproc
block|}
decl_stmt|;
end_decl_stmt

begin_expr_stmt
name|SYSINIT
argument_list|(
name|pagedaemon
argument_list|,
name|SI_SUB_KTHREAD_PAGE
argument_list|,
name|SI_ORDER_SECOND
argument_list|,
name|kproc_start
argument_list|,
operator|&
name|page_kp
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SDT_PROVIDER_DEFINE
argument_list|(
name|vm
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SDT_PROBE_DEFINE
argument_list|(
name|vm
argument_list|, , ,
name|vm__lowmem_scan
argument_list|)
expr_stmt|;
end_expr_stmt

begin_if
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_comment
comment|/* the kernel process "vm_daemon"*/
end_comment

begin_function_decl
specifier|static
name|void
name|vm_daemon
parameter_list|(
name|void
parameter_list|)
function_decl|;
end_function_decl

begin_decl_stmt
specifier|static
name|struct
name|proc
modifier|*
name|vmproc
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|struct
name|kproc_desc
name|vm_kp
init|=
block|{
literal|"vmdaemon"
block|,
name|vm_daemon
block|,
operator|&
name|vmproc
block|}
decl_stmt|;
end_decl_stmt

begin_expr_stmt
name|SYSINIT
argument_list|(
name|vmdaemon
argument_list|,
name|SI_SUB_KTHREAD_VM
argument_list|,
name|SI_ORDER_FIRST
argument_list|,
name|kproc_start
argument_list|,
operator|&
name|vm_kp
argument_list|)
expr_stmt|;
end_expr_stmt

begin_endif
endif|#
directive|endif
end_endif

begin_comment
comment|/* Pagedaemon activity rates, in subdivisions of one second. */
end_comment

begin_define
define|#
directive|define
name|VM_LAUNDER_RATE
value|10
end_define

begin_define
define|#
directive|define
name|VM_INACT_SCAN_RATE
value|2
end_define

begin_decl_stmt
name|int
name|vm_pageout_deficit
decl_stmt|;
end_decl_stmt

begin_comment
comment|/* Estimated number of pages deficit */
end_comment

begin_decl_stmt
name|u_int
name|vm_pageout_wakeup_thresh
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|int
name|vm_pageout_oom_seq
init|=
literal|12
decl_stmt|;
end_decl_stmt

begin_decl_stmt
name|bool
name|vm_pageout_wanted
decl_stmt|;
end_decl_stmt

begin_comment
comment|/* Event on which pageout daemon sleeps */
end_comment

begin_decl_stmt
name|bool
name|vm_pages_needed
decl_stmt|;
end_decl_stmt

begin_comment
comment|/* Are threads waiting for free pages? */
end_comment

begin_comment
comment|/* Pending request for dirty page laundering. */
end_comment

begin_enum
specifier|static
enum|enum
block|{
name|VM_LAUNDRY_IDLE
block|,
name|VM_LAUNDRY_BACKGROUND
block|,
name|VM_LAUNDRY_SHORTFALL
block|}
name|vm_laundry_request
init|=
name|VM_LAUNDRY_IDLE
enum|;
end_enum

begin_if
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_decl_stmt
specifier|static
name|int
name|vm_pageout_req_swapout
decl_stmt|;
end_decl_stmt

begin_comment
comment|/* XXX */
end_comment

begin_decl_stmt
specifier|static
name|int
name|vm_daemon_needed
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|struct
name|mtx
name|vm_daemon_mtx
decl_stmt|;
end_decl_stmt

begin_comment
comment|/* Allow for use by vm_pageout before vm_daemon is initialized. */
end_comment

begin_expr_stmt
name|MTX_SYSINIT
argument_list|(
name|vm_daemon
argument_list|,
operator|&
name|vm_daemon_mtx
argument_list|,
literal|"vm daemon"
argument_list|,
name|MTX_DEF
argument_list|)
expr_stmt|;
end_expr_stmt

begin_endif
endif|#
directive|endif
end_endif

begin_decl_stmt
specifier|static
name|int
name|vm_pageout_update_period
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|int
name|disable_swap_pageouts
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|int
name|lowmem_period
init|=
literal|10
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|time_t
name|lowmem_uptime
decl_stmt|;
end_decl_stmt

begin_if
if|#
directive|if
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_decl_stmt
specifier|static
name|int
name|vm_swap_enabled
init|=
literal|0
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|int
name|vm_swap_idle_enabled
init|=
literal|0
decl_stmt|;
end_decl_stmt

begin_else
else|#
directive|else
end_else

begin_decl_stmt
specifier|static
name|int
name|vm_swap_enabled
init|=
literal|1
decl_stmt|;
end_decl_stmt

begin_decl_stmt
specifier|static
name|int
name|vm_swap_idle_enabled
init|=
literal|0
decl_stmt|;
end_decl_stmt

begin_endif
endif|#
directive|endif
end_endif

begin_decl_stmt
specifier|static
name|int
name|vm_panic_on_oom
init|=
literal|0
decl_stmt|;
end_decl_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|panic_on_oom
argument_list|,
name|CTLFLAG_RWTUN
argument_list|,
operator|&
name|vm_panic_on_oom
argument_list|,
literal|0
argument_list|,
literal|"panic on out of memory instead of killing the largest process"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|pageout_wakeup_thresh
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_pageout_wakeup_thresh
argument_list|,
literal|0
argument_list|,
literal|"free page threshold for waking up the pageout daemon"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|pageout_update_period
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_pageout_update_period
argument_list|,
literal|0
argument_list|,
literal|"Maximum active LRU update period"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|lowmem_period
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|lowmem_period
argument_list|,
literal|0
argument_list|,
literal|"Low memory callback period"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_if
if|#
directive|if
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|VM_SWAPPING_ENABLED
argument_list|,
name|swap_enabled
argument_list|,
name|CTLFLAG_RD
argument_list|,
operator|&
name|vm_swap_enabled
argument_list|,
literal|0
argument_list|,
literal|"Enable entire process swapout"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|swap_idle_enabled
argument_list|,
name|CTLFLAG_RD
argument_list|,
operator|&
name|vm_swap_idle_enabled
argument_list|,
literal|0
argument_list|,
literal|"Allow swapout on idle criteria"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_else
else|#
directive|else
end_else

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|VM_SWAPPING_ENABLED
argument_list|,
name|swap_enabled
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_swap_enabled
argument_list|,
literal|0
argument_list|,
literal|"Enable entire process swapout"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|swap_idle_enabled
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_swap_idle_enabled
argument_list|,
literal|0
argument_list|,
literal|"Allow swapout on idle criteria"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_endif
endif|#
directive|endif
end_endif

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|disable_swapspace_pageouts
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|disable_swap_pageouts
argument_list|,
literal|0
argument_list|,
literal|"Disallow swapout of dirty pages"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_decl_stmt
specifier|static
name|int
name|pageout_lock_miss
decl_stmt|;
end_decl_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|pageout_lock_miss
argument_list|,
name|CTLFLAG_RD
argument_list|,
operator|&
name|pageout_lock_miss
argument_list|,
literal|0
argument_list|,
literal|"vget() lock misses during pageout"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|pageout_oom_seq
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_pageout_oom_seq
argument_list|,
literal|0
argument_list|,
literal|"back-to-back calls to oom detector to start OOM"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_decl_stmt
specifier|static
name|int
name|act_scan_laundry_weight
init|=
literal|3
decl_stmt|;
end_decl_stmt

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|act_scan_laundry_weight
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|act_scan_laundry_weight
argument_list|,
literal|0
argument_list|,
literal|"weight given to clean vs. dirty pages in active queue scans"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_decl_stmt
specifier|static
name|u_int
name|vm_background_launder_target
decl_stmt|;
end_decl_stmt

begin_expr_stmt
name|SYSCTL_UINT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|background_launder_target
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_background_launder_target
argument_list|,
literal|0
argument_list|,
literal|"background laundering target, in pages"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_decl_stmt
specifier|static
name|u_int
name|vm_background_launder_rate
init|=
literal|4096
decl_stmt|;
end_decl_stmt

begin_expr_stmt
name|SYSCTL_UINT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|background_launder_rate
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_background_launder_rate
argument_list|,
literal|0
argument_list|,
literal|"background laundering rate, in kilobytes per second"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_decl_stmt
specifier|static
name|u_int
name|vm_background_launder_max
init|=
literal|20
operator|*
literal|1024
decl_stmt|;
end_decl_stmt

begin_expr_stmt
name|SYSCTL_UINT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|background_launder_max
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_background_launder_max
argument_list|,
literal|0
argument_list|,
literal|"background laundering cap, in kilobytes"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_decl_stmt
name|int
name|vm_pageout_page_count
init|=
literal|32
decl_stmt|;
end_decl_stmt

begin_decl_stmt
name|int
name|vm_page_max_wired
decl_stmt|;
end_decl_stmt

begin_comment
comment|/* XXX max # of wired pages system-wide */
end_comment

begin_expr_stmt
name|SYSCTL_INT
argument_list|(
name|_vm
argument_list|,
name|OID_AUTO
argument_list|,
name|max_wired
argument_list|,
name|CTLFLAG_RW
argument_list|,
operator|&
name|vm_page_max_wired
argument_list|,
literal|0
argument_list|,
literal|"System-wide limit to wired page count"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_function_decl
specifier|static
name|u_int
name|isqrt
parameter_list|(
name|u_int
name|num
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|boolean_t
name|vm_pageout_fallback_object_lock
parameter_list|(
name|vm_page_t
parameter_list|,
name|vm_page_t
modifier|*
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|int
name|vm_pageout_launder
parameter_list|(
name|struct
name|vm_domain
modifier|*
name|vmd
parameter_list|,
name|int
name|launder
parameter_list|,
name|bool
name|in_shortfall
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|void
name|vm_pageout_laundry_worker
parameter_list|(
name|void
modifier|*
name|arg
parameter_list|)
function_decl|;
end_function_decl

begin_if
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_function_decl
specifier|static
name|void
name|vm_pageout_map_deactivate_pages
parameter_list|(
name|vm_map_t
parameter_list|,
name|long
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|void
name|vm_pageout_object_deactivate_pages
parameter_list|(
name|pmap_t
parameter_list|,
name|vm_object_t
parameter_list|,
name|long
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|void
name|vm_req_vmdaemon
parameter_list|(
name|int
name|req
parameter_list|)
function_decl|;
end_function_decl

begin_endif
endif|#
directive|endif
end_endif

begin_function_decl
specifier|static
name|boolean_t
name|vm_pageout_page_lock
parameter_list|(
name|vm_page_t
parameter_list|,
name|vm_page_t
modifier|*
parameter_list|)
function_decl|;
end_function_decl

begin_comment
comment|/*  * Initialize a dummy page for marking the caller's place in the specified  * paging queue.  In principle, this function only needs to set the flag  * PG_MARKER.  Nonetheless, it write busies and initializes the hold count  * to one as safety precautions.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout_init_marker
parameter_list|(
name|vm_page_t
name|marker
parameter_list|,
name|u_short
name|queue
parameter_list|)
block|{
name|bzero
argument_list|(
name|marker
argument_list|,
sizeof|sizeof
argument_list|(
operator|*
name|marker
argument_list|)
argument_list|)
expr_stmt|;
name|marker
operator|->
name|flags
operator|=
name|PG_MARKER
expr_stmt|;
name|marker
operator|->
name|busy_lock
operator|=
name|VPB_SINGLE_EXCLUSIVER
expr_stmt|;
name|marker
operator|->
name|queue
operator|=
name|queue
expr_stmt|;
name|marker
operator|->
name|hold_count
operator|=
literal|1
expr_stmt|;
block|}
end_function

begin_comment
comment|/*  * vm_pageout_fallback_object_lock:  *   * Lock vm object currently associated with `m'. VM_OBJECT_TRYWLOCK is  * known to have failed and page queue must be either PQ_ACTIVE or  * PQ_INACTIVE.  To avoid lock order violation, unlock the page queue  * while locking the vm object.  Use marker page to detect page queue  * changes and maintain notion of next page on page queue.  Return  * TRUE if no changes were detected, FALSE otherwise.  vm object is  * locked on return.  *   * This function depends on both the lock portion of struct vm_object  * and normal struct vm_page being type stable.  */
end_comment

begin_function
specifier|static
name|boolean_t
name|vm_pageout_fallback_object_lock
parameter_list|(
name|vm_page_t
name|m
parameter_list|,
name|vm_page_t
modifier|*
name|next
parameter_list|)
block|{
name|struct
name|vm_page
name|marker
decl_stmt|;
name|struct
name|vm_pagequeue
modifier|*
name|pq
decl_stmt|;
name|boolean_t
name|unchanged
decl_stmt|;
name|u_short
name|queue
decl_stmt|;
name|vm_object_t
name|object
decl_stmt|;
name|queue
operator|=
name|m
operator|->
name|queue
expr_stmt|;
name|vm_pageout_init_marker
argument_list|(
operator|&
name|marker
argument_list|,
name|queue
argument_list|)
expr_stmt|;
name|pq
operator|=
name|vm_page_pagequeue
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|object
operator|=
name|m
operator|->
name|object
expr_stmt|;
name|TAILQ_INSERT_AFTER
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
name|m
argument_list|,
operator|&
name|marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|VM_OBJECT_WLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
comment|/* 	 * The page's object might have changed, and/or the page might 	 * have moved from its original position in the queue.  If the 	 * page's object has changed, then the caller should abandon 	 * processing the page because the wrong object lock was 	 * acquired.  Use the marker's plinks.q, not the page's, to 	 * determine if the page has been moved.  The state of the 	 * page's plinks.q can be indeterminate; whereas, the marker's 	 * plinks.q must be valid. 	 */
operator|*
name|next
operator|=
name|TAILQ_NEXT
argument_list|(
operator|&
name|marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|unchanged
operator|=
name|m
operator|->
name|object
operator|==
name|object
operator|&&
name|m
operator|==
name|TAILQ_PREV
argument_list|(
operator|&
name|marker
argument_list|,
name|pglist
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
operator|!
name|unchanged
operator|||
name|m
operator|->
name|queue
operator|==
name|queue
argument_list|,
operator|(
literal|"page %p queue %d %d"
operator|,
name|m
operator|,
name|queue
operator|,
name|m
operator|->
name|queue
operator|)
argument_list|)
expr_stmt|;
name|TAILQ_REMOVE
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
operator|&
name|marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
return|return
operator|(
name|unchanged
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  * Lock the page while holding the page queue lock.  Use marker page  * to detect page queue changes and maintain notion of next page on  * page queue.  Return TRUE if no changes were detected, FALSE  * otherwise.  The page is locked on return. The page queue lock might  * be dropped and reacquired.  *  * This function depends on normal struct vm_page being type stable.  */
end_comment

begin_function
specifier|static
name|boolean_t
name|vm_pageout_page_lock
parameter_list|(
name|vm_page_t
name|m
parameter_list|,
name|vm_page_t
modifier|*
name|next
parameter_list|)
block|{
name|struct
name|vm_page
name|marker
decl_stmt|;
name|struct
name|vm_pagequeue
modifier|*
name|pq
decl_stmt|;
name|boolean_t
name|unchanged
decl_stmt|;
name|u_short
name|queue
decl_stmt|;
name|vm_page_lock_assert
argument_list|(
name|m
argument_list|,
name|MA_NOTOWNED
argument_list|)
expr_stmt|;
if|if
condition|(
name|vm_page_trylock
argument_list|(
name|m
argument_list|)
condition|)
return|return
operator|(
name|TRUE
operator|)
return|;
name|queue
operator|=
name|m
operator|->
name|queue
expr_stmt|;
name|vm_pageout_init_marker
argument_list|(
operator|&
name|marker
argument_list|,
name|queue
argument_list|)
expr_stmt|;
name|pq
operator|=
name|vm_page_pagequeue
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|TAILQ_INSERT_AFTER
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
name|m
argument_list|,
operator|&
name|marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
comment|/* Page queue might have changed. */
operator|*
name|next
operator|=
name|TAILQ_NEXT
argument_list|(
operator|&
name|marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|unchanged
operator|=
name|m
operator|==
name|TAILQ_PREV
argument_list|(
operator|&
name|marker
argument_list|,
name|pglist
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
operator|!
name|unchanged
operator|||
name|m
operator|->
name|queue
operator|==
name|queue
argument_list|,
operator|(
literal|"page %p queue %d %d"
operator|,
name|m
operator|,
name|queue
operator|,
name|m
operator|->
name|queue
operator|)
argument_list|)
expr_stmt|;
name|TAILQ_REMOVE
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
operator|&
name|marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
return|return
operator|(
name|unchanged
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  * Scan for pages at adjacent offsets within the given page's object that are  * eligible for laundering, form a cluster of these pages and the given page,  * and launder that cluster.  */
end_comment

begin_function
specifier|static
name|int
name|vm_pageout_cluster
parameter_list|(
name|vm_page_t
name|m
parameter_list|)
block|{
name|vm_object_t
name|object
decl_stmt|;
name|vm_page_t
name|mc
index|[
literal|2
operator|*
name|vm_pageout_page_count
index|]
decl_stmt|,
name|p
decl_stmt|,
name|pb
decl_stmt|,
name|ps
decl_stmt|;
name|vm_pindex_t
name|pindex
decl_stmt|;
name|int
name|ib
decl_stmt|,
name|is
decl_stmt|,
name|page_base
decl_stmt|,
name|pageout_count
decl_stmt|;
name|vm_page_assert_locked
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|object
operator|=
name|m
operator|->
name|object
expr_stmt|;
name|VM_OBJECT_ASSERT_WLOCKED
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|pindex
operator|=
name|m
operator|->
name|pindex
expr_stmt|;
comment|/* 	 * We can't clean the page if it is busy or held. 	 */
name|vm_page_assert_unbusied
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
name|m
operator|->
name|hold_count
operator|==
literal|0
argument_list|,
operator|(
literal|"page %p is held"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|mc
index|[
name|vm_pageout_page_count
index|]
operator|=
name|pb
operator|=
name|ps
operator|=
name|m
expr_stmt|;
name|pageout_count
operator|=
literal|1
expr_stmt|;
name|page_base
operator|=
name|vm_pageout_page_count
expr_stmt|;
name|ib
operator|=
literal|1
expr_stmt|;
name|is
operator|=
literal|1
expr_stmt|;
comment|/* 	 * We can cluster only if the page is not clean, busy, or held, and 	 * the page is in the laundry queue. 	 * 	 * During heavy mmap/modification loads the pageout 	 * daemon can really fragment the underlying file 	 * due to flushing pages out of order and not trying to 	 * align the clusters (which leaves sporadic out-of-order 	 * holes).  To solve this problem we do the reverse scan 	 * first and attempt to align our cluster, then do a  	 * forward scan if room remains. 	 */
name|more
label|:
while|while
condition|(
name|ib
operator|!=
literal|0
operator|&&
name|pageout_count
operator|<
name|vm_pageout_page_count
condition|)
block|{
if|if
condition|(
name|ib
operator|>
name|pindex
condition|)
block|{
name|ib
operator|=
literal|0
expr_stmt|;
break|break;
block|}
if|if
condition|(
operator|(
name|p
operator|=
name|vm_page_prev
argument_list|(
name|pb
argument_list|)
operator|)
operator|==
name|NULL
operator|||
name|vm_page_busied
argument_list|(
name|p
argument_list|)
condition|)
block|{
name|ib
operator|=
literal|0
expr_stmt|;
break|break;
block|}
name|vm_page_test_dirty
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|dirty
operator|==
literal|0
condition|)
block|{
name|ib
operator|=
literal|0
expr_stmt|;
break|break;
block|}
name|vm_page_lock
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|vm_page_in_laundry
argument_list|(
name|p
argument_list|)
operator|||
name|p
operator|->
name|hold_count
operator|!=
literal|0
condition|)
block|{
comment|/* may be undergoing I/O */
name|vm_page_unlock
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|ib
operator|=
literal|0
expr_stmt|;
break|break;
block|}
name|vm_page_unlock
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|mc
index|[
operator|--
name|page_base
index|]
operator|=
name|pb
operator|=
name|p
expr_stmt|;
operator|++
name|pageout_count
expr_stmt|;
operator|++
name|ib
expr_stmt|;
comment|/* 		 * We are at an alignment boundary.  Stop here, and switch 		 * directions.  Do not clear ib. 		 */
if|if
condition|(
operator|(
name|pindex
operator|-
operator|(
name|ib
operator|-
literal|1
operator|)
operator|)
operator|%
name|vm_pageout_page_count
operator|==
literal|0
condition|)
break|break;
block|}
while|while
condition|(
name|pageout_count
operator|<
name|vm_pageout_page_count
operator|&&
name|pindex
operator|+
name|is
operator|<
name|object
operator|->
name|size
condition|)
block|{
if|if
condition|(
operator|(
name|p
operator|=
name|vm_page_next
argument_list|(
name|ps
argument_list|)
operator|)
operator|==
name|NULL
operator|||
name|vm_page_busied
argument_list|(
name|p
argument_list|)
condition|)
break|break;
name|vm_page_test_dirty
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|dirty
operator|==
literal|0
condition|)
break|break;
name|vm_page_lock
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|vm_page_in_laundry
argument_list|(
name|p
argument_list|)
operator|||
name|p
operator|->
name|hold_count
operator|!=
literal|0
condition|)
block|{
comment|/* may be undergoing I/O */
name|vm_page_unlock
argument_list|(
name|p
argument_list|)
expr_stmt|;
break|break;
block|}
name|vm_page_unlock
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|mc
index|[
name|page_base
operator|+
name|pageout_count
index|]
operator|=
name|ps
operator|=
name|p
expr_stmt|;
operator|++
name|pageout_count
expr_stmt|;
operator|++
name|is
expr_stmt|;
block|}
comment|/* 	 * If we exhausted our forward scan, continue with the reverse scan 	 * when possible, even past an alignment boundary.  This catches 	 * boundary conditions. 	 */
if|if
condition|(
name|ib
operator|!=
literal|0
operator|&&
name|pageout_count
operator|<
name|vm_pageout_page_count
condition|)
goto|goto
name|more
goto|;
return|return
operator|(
name|vm_pageout_flush
argument_list|(
operator|&
name|mc
index|[
name|page_base
index|]
argument_list|,
name|pageout_count
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|,
name|NULL
argument_list|,
name|NULL
argument_list|)
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  * vm_pageout_flush() - launder the given pages  *  *	The given pages are laundered.  Note that we setup for the start of  *	I/O ( i.e. busy the page ), mark it read-only, and bump the object  *	reference count all in here rather then in the parent.  If we want  *	the parent to do more sophisticated things we may have to change  *	the ordering.  *  *	Returned runlen is the count of pages between mreq and first  *	page after mreq with status VM_PAGER_AGAIN.  *	*eio is set to TRUE if pager returned VM_PAGER_ERROR or VM_PAGER_FAIL  *	for any page in runlen set.  */
end_comment

begin_function
name|int
name|vm_pageout_flush
parameter_list|(
name|vm_page_t
modifier|*
name|mc
parameter_list|,
name|int
name|count
parameter_list|,
name|int
name|flags
parameter_list|,
name|int
name|mreq
parameter_list|,
name|int
modifier|*
name|prunlen
parameter_list|,
name|boolean_t
modifier|*
name|eio
parameter_list|)
block|{
name|vm_object_t
name|object
init|=
name|mc
index|[
literal|0
index|]
operator|->
name|object
decl_stmt|;
name|int
name|pageout_status
index|[
name|count
index|]
decl_stmt|;
name|int
name|numpagedout
init|=
literal|0
decl_stmt|;
name|int
name|i
decl_stmt|,
name|runlen
decl_stmt|;
name|VM_OBJECT_ASSERT_WLOCKED
argument_list|(
name|object
argument_list|)
expr_stmt|;
comment|/* 	 * Initiate I/O.  Bump the vm_page_t->busy counter and 	 * mark the pages read-only. 	 * 	 * We do not have to fixup the clean/dirty bits here... we can 	 * allow the pager to do it after the I/O completes. 	 * 	 * NOTE! mc[i]->dirty may be partial or fragmented due to an 	 * edge case with file fragments. 	 */
for|for
control|(
name|i
operator|=
literal|0
init|;
name|i
operator|<
name|count
condition|;
name|i
operator|++
control|)
block|{
name|KASSERT
argument_list|(
name|mc
index|[
name|i
index|]
operator|->
name|valid
operator|==
name|VM_PAGE_BITS_ALL
argument_list|,
operator|(
literal|"vm_pageout_flush: partially invalid page %p index %d/%d"
operator|,
name|mc
index|[
name|i
index|]
operator|,
name|i
operator|,
name|count
operator|)
argument_list|)
expr_stmt|;
name|vm_page_sbusy
argument_list|(
name|mc
index|[
name|i
index|]
argument_list|)
expr_stmt|;
name|pmap_remove_write
argument_list|(
name|mc
index|[
name|i
index|]
argument_list|)
expr_stmt|;
block|}
name|vm_object_pip_add
argument_list|(
name|object
argument_list|,
name|count
argument_list|)
expr_stmt|;
name|vm_pager_put_pages
argument_list|(
name|object
argument_list|,
name|mc
argument_list|,
name|count
argument_list|,
name|flags
argument_list|,
name|pageout_status
argument_list|)
expr_stmt|;
name|runlen
operator|=
name|count
operator|-
name|mreq
expr_stmt|;
if|if
condition|(
name|eio
operator|!=
name|NULL
condition|)
operator|*
name|eio
operator|=
name|FALSE
expr_stmt|;
for|for
control|(
name|i
operator|=
literal|0
init|;
name|i
operator|<
name|count
condition|;
name|i
operator|++
control|)
block|{
name|vm_page_t
name|mt
init|=
name|mc
index|[
name|i
index|]
decl_stmt|;
name|KASSERT
argument_list|(
name|pageout_status
index|[
name|i
index|]
operator|==
name|VM_PAGER_PEND
operator|||
operator|!
name|pmap_page_is_write_mapped
argument_list|(
name|mt
argument_list|)
argument_list|,
operator|(
literal|"vm_pageout_flush: page %p is not write protected"
operator|,
name|mt
operator|)
argument_list|)
expr_stmt|;
switch|switch
condition|(
name|pageout_status
index|[
name|i
index|]
condition|)
block|{
case|case
name|VM_PAGER_OK
case|:
name|vm_page_lock
argument_list|(
name|mt
argument_list|)
expr_stmt|;
if|if
condition|(
name|vm_page_in_laundry
argument_list|(
name|mt
argument_list|)
condition|)
name|vm_page_deactivate_noreuse
argument_list|(
name|mt
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|mt
argument_list|)
expr_stmt|;
comment|/* FALLTHROUGH */
case|case
name|VM_PAGER_PEND
case|:
name|numpagedout
operator|++
expr_stmt|;
break|break;
case|case
name|VM_PAGER_BAD
case|:
comment|/* 			 * The page is outside the object's range.  We pretend 			 * that the page out worked and clean the page, so the 			 * changes will be lost if the page is reclaimed by 			 * the page daemon. 			 */
name|vm_page_undirty
argument_list|(
name|mt
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|mt
argument_list|)
expr_stmt|;
if|if
condition|(
name|vm_page_in_laundry
argument_list|(
name|mt
argument_list|)
condition|)
name|vm_page_deactivate_noreuse
argument_list|(
name|mt
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|mt
argument_list|)
expr_stmt|;
break|break;
case|case
name|VM_PAGER_ERROR
case|:
case|case
name|VM_PAGER_FAIL
case|:
comment|/* 			 * If the page couldn't be paged out, then reactivate 			 * it so that it doesn't clog the laundry and inactive 			 * queues.  (We will try paging it out again later). 			 */
name|vm_page_lock
argument_list|(
name|mt
argument_list|)
expr_stmt|;
name|vm_page_activate
argument_list|(
name|mt
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|mt
argument_list|)
expr_stmt|;
if|if
condition|(
name|eio
operator|!=
name|NULL
operator|&&
name|i
operator|>=
name|mreq
operator|&&
name|i
operator|-
name|mreq
operator|<
name|runlen
condition|)
operator|*
name|eio
operator|=
name|TRUE
expr_stmt|;
break|break;
case|case
name|VM_PAGER_AGAIN
case|:
if|if
condition|(
name|i
operator|>=
name|mreq
operator|&&
name|i
operator|-
name|mreq
operator|<
name|runlen
condition|)
name|runlen
operator|=
name|i
operator|-
name|mreq
expr_stmt|;
break|break;
block|}
comment|/* 		 * If the operation is still going, leave the page busy to 		 * block all other accesses. Also, leave the paging in 		 * progress indicator set so that we don't attempt an object 		 * collapse. 		 */
if|if
condition|(
name|pageout_status
index|[
name|i
index|]
operator|!=
name|VM_PAGER_PEND
condition|)
block|{
name|vm_object_pip_wakeup
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vm_page_sunbusy
argument_list|(
name|mt
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|prunlen
operator|!=
name|NULL
condition|)
operator|*
name|prunlen
operator|=
name|runlen
expr_stmt|;
return|return
operator|(
name|numpagedout
operator|)
return|;
block|}
end_function

begin_if
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_comment
comment|/*  *	vm_pageout_object_deactivate_pages  *  *	Deactivate enough pages to satisfy the inactive target  *	requirements.  *  *	The object and map must be locked.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout_object_deactivate_pages
parameter_list|(
name|pmap_t
name|pmap
parameter_list|,
name|vm_object_t
name|first_object
parameter_list|,
name|long
name|desired
parameter_list|)
block|{
name|vm_object_t
name|backing_object
decl_stmt|,
name|object
decl_stmt|;
name|vm_page_t
name|p
decl_stmt|;
name|int
name|act_delta
decl_stmt|,
name|remove_mode
decl_stmt|;
name|VM_OBJECT_ASSERT_LOCKED
argument_list|(
name|first_object
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|first_object
operator|->
name|flags
operator|&
name|OBJ_FICTITIOUS
operator|)
operator|!=
literal|0
condition|)
return|return;
for|for
control|(
name|object
operator|=
name|first_object
init|;
condition|;
name|object
operator|=
name|backing_object
control|)
block|{
if|if
condition|(
name|pmap_resident_count
argument_list|(
name|pmap
argument_list|)
operator|<=
name|desired
condition|)
goto|goto
name|unlock_return
goto|;
name|VM_OBJECT_ASSERT_LOCKED
argument_list|(
name|object
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|object
operator|->
name|flags
operator|&
name|OBJ_UNMANAGED
operator|)
operator|!=
literal|0
operator|||
name|object
operator|->
name|paging_in_progress
operator|!=
literal|0
condition|)
goto|goto
name|unlock_return
goto|;
name|remove_mode
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|object
operator|->
name|shadow_count
operator|>
literal|1
condition|)
name|remove_mode
operator|=
literal|1
expr_stmt|;
comment|/* 		 * Scan the object's entire memory queue. 		 */
name|TAILQ_FOREACH
argument_list|(
argument|p
argument_list|,
argument|&object->memq
argument_list|,
argument|listq
argument_list|)
block|{
if|if
condition|(
name|pmap_resident_count
argument_list|(
name|pmap
argument_list|)
operator|<=
name|desired
condition|)
goto|goto
name|unlock_return
goto|;
if|if
condition|(
name|vm_page_busied
argument_list|(
name|p
argument_list|)
condition|)
continue|continue;
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_pdpages
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|wire_count
operator|!=
literal|0
operator|||
name|p
operator|->
name|hold_count
operator|!=
literal|0
operator|||
operator|!
name|pmap_page_exists_quick
argument_list|(
name|pmap
argument_list|,
name|p
argument_list|)
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|act_delta
operator|=
name|pmap_ts_referenced
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|p
operator|->
name|aflags
operator|&
name|PGA_REFERENCED
operator|)
operator|!=
literal|0
condition|)
block|{
if|if
condition|(
name|act_delta
operator|==
literal|0
condition|)
name|act_delta
operator|=
literal|1
expr_stmt|;
name|vm_page_aflag_clear
argument_list|(
name|p
argument_list|,
name|PGA_REFERENCED
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|vm_page_active
argument_list|(
name|p
argument_list|)
operator|&&
name|act_delta
operator|!=
literal|0
condition|)
block|{
name|vm_page_activate
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|p
operator|->
name|act_count
operator|+=
name|act_delta
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|vm_page_active
argument_list|(
name|p
argument_list|)
condition|)
block|{
if|if
condition|(
name|act_delta
operator|==
literal|0
condition|)
block|{
name|p
operator|->
name|act_count
operator|-=
name|min
argument_list|(
name|p
operator|->
name|act_count
argument_list|,
name|ACT_DECLINE
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|remove_mode
operator|&&
name|p
operator|->
name|act_count
operator|==
literal|0
condition|)
block|{
name|pmap_remove_all
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|vm_page_deactivate
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
else|else
name|vm_page_requeue
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|vm_page_activate
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|act_count
operator|<
name|ACT_MAX
operator|-
name|ACT_ADVANCE
condition|)
name|p
operator|->
name|act_count
operator|+=
name|ACT_ADVANCE
expr_stmt|;
name|vm_page_requeue
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|vm_page_inactive
argument_list|(
name|p
argument_list|)
condition|)
name|pmap_remove_all
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|(
name|backing_object
operator|=
name|object
operator|->
name|backing_object
operator|)
operator|==
name|NULL
condition|)
goto|goto
name|unlock_return
goto|;
name|VM_OBJECT_RLOCK
argument_list|(
name|backing_object
argument_list|)
expr_stmt|;
if|if
condition|(
name|object
operator|!=
name|first_object
condition|)
name|VM_OBJECT_RUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
block|}
name|unlock_return
label|:
if|if
condition|(
name|object
operator|!=
name|first_object
condition|)
name|VM_OBJECT_RUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|/*  * deactivate some number of pages in a map, try to do it fairly, but  * that is really hard to do.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout_map_deactivate_pages
parameter_list|(
name|map
parameter_list|,
name|desired
parameter_list|)
name|vm_map_t
name|map
decl_stmt|;
name|long
name|desired
decl_stmt|;
block|{
name|vm_map_entry_t
name|tmpe
decl_stmt|;
name|vm_object_t
name|obj
decl_stmt|,
name|bigobj
decl_stmt|;
name|int
name|nothingwired
decl_stmt|;
if|if
condition|(
operator|!
name|vm_map_trylock
argument_list|(
name|map
argument_list|)
condition|)
return|return;
name|bigobj
operator|=
name|NULL
expr_stmt|;
name|nothingwired
operator|=
name|TRUE
expr_stmt|;
comment|/* 	 * first, search out the biggest object, and try to free pages from 	 * that. 	 */
name|tmpe
operator|=
name|map
operator|->
name|header
operator|.
name|next
expr_stmt|;
while|while
condition|(
name|tmpe
operator|!=
operator|&
name|map
operator|->
name|header
condition|)
block|{
if|if
condition|(
operator|(
name|tmpe
operator|->
name|eflags
operator|&
name|MAP_ENTRY_IS_SUB_MAP
operator|)
operator|==
literal|0
condition|)
block|{
name|obj
operator|=
name|tmpe
operator|->
name|object
operator|.
name|vm_object
expr_stmt|;
if|if
condition|(
name|obj
operator|!=
name|NULL
operator|&&
name|VM_OBJECT_TRYRLOCK
argument_list|(
name|obj
argument_list|)
condition|)
block|{
if|if
condition|(
name|obj
operator|->
name|shadow_count
operator|<=
literal|1
operator|&&
operator|(
name|bigobj
operator|==
name|NULL
operator|||
name|bigobj
operator|->
name|resident_page_count
operator|<
name|obj
operator|->
name|resident_page_count
operator|)
condition|)
block|{
if|if
condition|(
name|bigobj
operator|!=
name|NULL
condition|)
name|VM_OBJECT_RUNLOCK
argument_list|(
name|bigobj
argument_list|)
expr_stmt|;
name|bigobj
operator|=
name|obj
expr_stmt|;
block|}
else|else
name|VM_OBJECT_RUNLOCK
argument_list|(
name|obj
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|tmpe
operator|->
name|wired_count
operator|>
literal|0
condition|)
name|nothingwired
operator|=
name|FALSE
expr_stmt|;
name|tmpe
operator|=
name|tmpe
operator|->
name|next
expr_stmt|;
block|}
if|if
condition|(
name|bigobj
operator|!=
name|NULL
condition|)
block|{
name|vm_pageout_object_deactivate_pages
argument_list|(
name|map
operator|->
name|pmap
argument_list|,
name|bigobj
argument_list|,
name|desired
argument_list|)
expr_stmt|;
name|VM_OBJECT_RUNLOCK
argument_list|(
name|bigobj
argument_list|)
expr_stmt|;
block|}
comment|/* 	 * Next, hunt around for other pages to deactivate.  We actually 	 * do this search sort of wrong -- .text first is not the best idea. 	 */
name|tmpe
operator|=
name|map
operator|->
name|header
operator|.
name|next
expr_stmt|;
while|while
condition|(
name|tmpe
operator|!=
operator|&
name|map
operator|->
name|header
condition|)
block|{
if|if
condition|(
name|pmap_resident_count
argument_list|(
name|vm_map_pmap
argument_list|(
name|map
argument_list|)
argument_list|)
operator|<=
name|desired
condition|)
break|break;
if|if
condition|(
operator|(
name|tmpe
operator|->
name|eflags
operator|&
name|MAP_ENTRY_IS_SUB_MAP
operator|)
operator|==
literal|0
condition|)
block|{
name|obj
operator|=
name|tmpe
operator|->
name|object
operator|.
name|vm_object
expr_stmt|;
if|if
condition|(
name|obj
operator|!=
name|NULL
condition|)
block|{
name|VM_OBJECT_RLOCK
argument_list|(
name|obj
argument_list|)
expr_stmt|;
name|vm_pageout_object_deactivate_pages
argument_list|(
name|map
operator|->
name|pmap
argument_list|,
name|obj
argument_list|,
name|desired
argument_list|)
expr_stmt|;
name|VM_OBJECT_RUNLOCK
argument_list|(
name|obj
argument_list|)
expr_stmt|;
block|}
block|}
name|tmpe
operator|=
name|tmpe
operator|->
name|next
expr_stmt|;
block|}
comment|/* 	 * Remove all mappings if a process is swapped out, this will free page 	 * table pages. 	 */
if|if
condition|(
name|desired
operator|==
literal|0
operator|&&
name|nothingwired
condition|)
block|{
name|pmap_remove
argument_list|(
name|vm_map_pmap
argument_list|(
name|map
argument_list|)
argument_list|,
name|vm_map_min
argument_list|(
name|map
argument_list|)
argument_list|,
name|vm_map_max
argument_list|(
name|map
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|vm_map_unlock
argument_list|(
name|map
argument_list|)
expr_stmt|;
block|}
end_function

begin_endif
endif|#
directive|endif
end_endif

begin_comment
comment|/* !defined(NO_SWAPPING) */
end_comment

begin_comment
comment|/*  * Attempt to acquire all of the necessary locks to launder a page and  * then call through the clustering layer to PUTPAGES.  Wait a short  * time for a vnode lock.  *  * Requires the page and object lock on entry, releases both before return.  * Returns 0 on success and an errno otherwise.  */
end_comment

begin_function
specifier|static
name|int
name|vm_pageout_clean
parameter_list|(
name|vm_page_t
name|m
parameter_list|,
name|int
modifier|*
name|numpagedout
parameter_list|)
block|{
name|struct
name|vnode
modifier|*
name|vp
decl_stmt|;
name|struct
name|mount
modifier|*
name|mp
decl_stmt|;
name|vm_object_t
name|object
decl_stmt|;
name|vm_pindex_t
name|pindex
decl_stmt|;
name|int
name|error
decl_stmt|,
name|lockmode
decl_stmt|;
name|vm_page_assert_locked
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|object
operator|=
name|m
operator|->
name|object
expr_stmt|;
name|VM_OBJECT_ASSERT_WLOCKED
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|error
operator|=
literal|0
expr_stmt|;
name|vp
operator|=
name|NULL
expr_stmt|;
name|mp
operator|=
name|NULL
expr_stmt|;
comment|/* 	 * The object is already known NOT to be dead.   It 	 * is possible for the vget() to block the whole 	 * pageout daemon, but the new low-memory handling 	 * code should prevent it. 	 * 	 * We can't wait forever for the vnode lock, we might 	 * deadlock due to a vn_read() getting stuck in 	 * vm_wait while holding this vnode.  We skip the  	 * vnode if we can't get it in a reasonable amount 	 * of time. 	 */
if|if
condition|(
name|object
operator|->
name|type
operator|==
name|OBJT_VNODE
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vp
operator|=
name|object
operator|->
name|handle
expr_stmt|;
if|if
condition|(
name|vp
operator|->
name|v_type
operator|==
name|VREG
operator|&&
name|vn_start_write
argument_list|(
name|vp
argument_list|,
operator|&
name|mp
argument_list|,
name|V_NOWAIT
argument_list|)
operator|!=
literal|0
condition|)
block|{
name|mp
operator|=
name|NULL
expr_stmt|;
name|error
operator|=
name|EDEADLK
expr_stmt|;
goto|goto
name|unlock_all
goto|;
block|}
name|KASSERT
argument_list|(
name|mp
operator|!=
name|NULL
argument_list|,
operator|(
literal|"vp %p with NULL v_mount"
operator|,
name|vp
operator|)
argument_list|)
expr_stmt|;
name|vm_object_reference_locked
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|pindex
operator|=
name|m
operator|->
name|pindex
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|lockmode
operator|=
name|MNT_SHARED_WRITES
argument_list|(
name|vp
operator|->
name|v_mount
argument_list|)
condition|?
name|LK_SHARED
else|:
name|LK_EXCLUSIVE
expr_stmt|;
if|if
condition|(
name|vget
argument_list|(
name|vp
argument_list|,
name|lockmode
operator||
name|LK_TIMELOCK
argument_list|,
name|curthread
argument_list|)
condition|)
block|{
name|vp
operator|=
name|NULL
expr_stmt|;
name|error
operator|=
name|EDEADLK
expr_stmt|;
goto|goto
name|unlock_mp
goto|;
block|}
name|VM_OBJECT_WLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|m
argument_list|)
expr_stmt|;
comment|/* 		 * While the object and page were unlocked, the page 		 * may have been: 		 * (1) moved to a different queue, 		 * (2) reallocated to a different object, 		 * (3) reallocated to a different offset, or 		 * (4) cleaned. 		 */
if|if
condition|(
operator|!
name|vm_page_in_laundry
argument_list|(
name|m
argument_list|)
operator|||
name|m
operator|->
name|object
operator|!=
name|object
operator|||
name|m
operator|->
name|pindex
operator|!=
name|pindex
operator|||
name|m
operator|->
name|dirty
operator|==
literal|0
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|error
operator|=
name|ENXIO
expr_stmt|;
goto|goto
name|unlock_all
goto|;
block|}
comment|/* 		 * The page may have been busied or held while the object 		 * and page locks were released. 		 */
if|if
condition|(
name|vm_page_busied
argument_list|(
name|m
argument_list|)
operator|||
name|m
operator|->
name|hold_count
operator|!=
literal|0
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|error
operator|=
name|EBUSY
expr_stmt|;
goto|goto
name|unlock_all
goto|;
block|}
block|}
comment|/* 	 * If a page is dirty, then it is either being washed 	 * (but not yet cleaned) or it is still in the 	 * laundry.  If it is still in the laundry, then we 	 * start the cleaning operation.  	 */
if|if
condition|(
operator|(
operator|*
name|numpagedout
operator|=
name|vm_pageout_cluster
argument_list|(
name|m
argument_list|)
operator|)
operator|==
literal|0
condition|)
name|error
operator|=
name|EIO
expr_stmt|;
name|unlock_all
label|:
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|unlock_mp
label|:
name|vm_page_lock_assert
argument_list|(
name|m
argument_list|,
name|MA_NOTOWNED
argument_list|)
expr_stmt|;
if|if
condition|(
name|mp
operator|!=
name|NULL
condition|)
block|{
if|if
condition|(
name|vp
operator|!=
name|NULL
condition|)
name|vput
argument_list|(
name|vp
argument_list|)
expr_stmt|;
name|vm_object_deallocate
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vn_finished_write
argument_list|(
name|mp
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|error
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  * Attempt to launder the specified number of pages.  *  * Returns the number of pages successfully laundered.  */
end_comment

begin_function
specifier|static
name|int
name|vm_pageout_launder
parameter_list|(
name|struct
name|vm_domain
modifier|*
name|vmd
parameter_list|,
name|int
name|launder
parameter_list|,
name|bool
name|in_shortfall
parameter_list|)
block|{
name|struct
name|vm_pagequeue
modifier|*
name|pq
decl_stmt|;
name|vm_object_t
name|object
decl_stmt|;
name|vm_page_t
name|m
decl_stmt|,
name|next
decl_stmt|;
name|int
name|act_delta
decl_stmt|,
name|error
decl_stmt|,
name|maxscan
decl_stmt|,
name|numpagedout
decl_stmt|,
name|starting_target
decl_stmt|;
name|int
name|vnodes_skipped
decl_stmt|;
name|bool
name|pageout_ok
decl_stmt|,
name|queue_locked
decl_stmt|;
name|starting_target
operator|=
name|launder
expr_stmt|;
name|vnodes_skipped
operator|=
literal|0
expr_stmt|;
comment|/* 	 * Scan the laundry queue for pages eligible to be laundered.  We stop 	 * once the target number of dirty pages have been laundered, or once 	 * we've reached the end of the queue.  A single iteration of this loop 	 * may cause more than one page to be laundered because of clustering. 	 * 	 * maxscan ensures that we don't re-examine requeued pages.  Any 	 * additional pages written as part of a cluster are subtracted from 	 * maxscan since they must be taken from the laundry queue. 	 */
name|pq
operator|=
operator|&
name|vmd
operator|->
name|vmd_pagequeues
index|[
name|PQ_LAUNDRY
index|]
expr_stmt|;
name|maxscan
operator|=
name|pq
operator|->
name|pq_cnt
expr_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|true
expr_stmt|;
for|for
control|(
name|m
operator|=
name|TAILQ_FIRST
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|)
init|;
name|m
operator|!=
name|NULL
operator|&&
name|maxscan
operator|--
operator|>
literal|0
operator|&&
name|launder
operator|>
literal|0
condition|;
name|m
operator|=
name|next
control|)
block|{
name|vm_pagequeue_assert_locked
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
name|queue_locked
argument_list|,
operator|(
literal|"unlocked laundry queue"
operator|)
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
name|vm_page_in_laundry
argument_list|(
name|m
argument_list|)
argument_list|,
operator|(
literal|"page %p has an inconsistent queue"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
name|next
operator|=
name|TAILQ_NEXT
argument_list|(
name|m
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_MARKER
operator|)
operator|!=
literal|0
condition|)
continue|continue;
name|KASSERT
argument_list|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_FICTITIOUS
operator|)
operator|==
literal|0
argument_list|,
operator|(
literal|"PG_FICTITIOUS page %p cannot be in laundry queue"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
operator|(
name|m
operator|->
name|oflags
operator|&
name|VPO_UNMANAGED
operator|)
operator|==
literal|0
argument_list|,
operator|(
literal|"VPO_UNMANAGED page %p cannot be in laundry queue"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|vm_pageout_page_lock
argument_list|(
name|m
argument_list|,
operator|&
name|next
argument_list|)
operator|||
name|m
operator|->
name|hold_count
operator|!=
literal|0
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|object
operator|=
name|m
operator|->
name|object
expr_stmt|;
if|if
condition|(
operator|(
operator|!
name|VM_OBJECT_TRYWLOCK
argument_list|(
name|object
argument_list|)
operator|&&
operator|(
operator|!
name|vm_pageout_fallback_object_lock
argument_list|(
name|m
argument_list|,
operator|&
name|next
argument_list|)
operator|||
name|m
operator|->
name|hold_count
operator|!=
literal|0
operator|)
operator|)
operator|||
name|vm_page_busied
argument_list|(
name|m
argument_list|)
condition|)
block|{
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|/* 		 * Unlock the laundry queue, invalidating the 'next' pointer. 		 * Use a marker to remember our place in the laundry queue. 		 */
name|TAILQ_INSERT_AFTER
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
name|m
argument_list|,
operator|&
name|vmd
operator|->
name|vmd_laundry_marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|false
expr_stmt|;
comment|/* 		 * Invalid pages can be easily freed.  They cannot be 		 * mapped; vm_page_free() asserts this. 		 */
if|if
condition|(
name|m
operator|->
name|valid
operator|==
literal|0
condition|)
goto|goto
name|free_page
goto|;
comment|/* 		 * If the page has been referenced and the object is not dead, 		 * reactivate or requeue the page depending on whether the 		 * object is mapped. 		 */
if|if
condition|(
operator|(
name|m
operator|->
name|aflags
operator|&
name|PGA_REFERENCED
operator|)
operator|!=
literal|0
condition|)
block|{
name|vm_page_aflag_clear
argument_list|(
name|m
argument_list|,
name|PGA_REFERENCED
argument_list|)
expr_stmt|;
name|act_delta
operator|=
literal|1
expr_stmt|;
block|}
else|else
name|act_delta
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|object
operator|->
name|ref_count
operator|!=
literal|0
condition|)
name|act_delta
operator|+=
name|pmap_ts_referenced
argument_list|(
name|m
argument_list|)
expr_stmt|;
else|else
block|{
name|KASSERT
argument_list|(
operator|!
name|pmap_page_is_mapped
argument_list|(
name|m
argument_list|)
argument_list|,
operator|(
literal|"page %p is mapped"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|act_delta
operator|!=
literal|0
condition|)
block|{
if|if
condition|(
name|object
operator|->
name|ref_count
operator|!=
literal|0
condition|)
block|{
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_reactivated
argument_list|)
expr_stmt|;
name|vm_page_activate
argument_list|(
name|m
argument_list|)
expr_stmt|;
comment|/* 				 * Increase the activation count if the page 				 * was referenced while in the laundry queue. 				 * This makes it less likely that the page will 				 * be returned prematurely to the inactive 				 * queue.  				 */
name|m
operator|->
name|act_count
operator|+=
name|act_delta
operator|+
name|ACT_ADVANCE
expr_stmt|;
comment|/* 				 * If this was a background laundering, count 				 * activated pages towards our target.  The 				 * purpose of background laundering is to ensure 				 * that pages are eventually cycled through the 				 * laundry queue, and an activation is a valid 				 * way out. 				 */
if|if
condition|(
operator|!
name|in_shortfall
condition|)
name|launder
operator|--
expr_stmt|;
goto|goto
name|drop_page
goto|;
block|}
elseif|else
if|if
condition|(
operator|(
name|object
operator|->
name|flags
operator|&
name|OBJ_DEAD
operator|)
operator|==
literal|0
condition|)
goto|goto
name|requeue_page
goto|;
block|}
comment|/* 		 * If the page appears to be clean at the machine-independent 		 * layer, then remove all of its mappings from the pmap in 		 * anticipation of freeing it.  If, however, any of the page's 		 * mappings allow write access, then the page may still be 		 * modified until the last of those mappings are removed. 		 */
if|if
condition|(
name|object
operator|->
name|ref_count
operator|!=
literal|0
condition|)
block|{
name|vm_page_test_dirty
argument_list|(
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|m
operator|->
name|dirty
operator|==
literal|0
condition|)
name|pmap_remove_all
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
comment|/* 		 * Clean pages are freed, and dirty pages are paged out unless 		 * they belong to a dead object.  Requeueing dirty pages from 		 * dead objects is pointless, as they are being paged out and 		 * freed by the thread that destroyed the object. 		 */
if|if
condition|(
name|m
operator|->
name|dirty
operator|==
literal|0
condition|)
block|{
name|free_page
label|:
name|vm_page_free
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_dfree
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
operator|(
name|object
operator|->
name|flags
operator|&
name|OBJ_DEAD
operator|)
operator|==
literal|0
condition|)
block|{
if|if
condition|(
name|object
operator|->
name|type
operator|!=
name|OBJT_SWAP
operator|&&
name|object
operator|->
name|type
operator|!=
name|OBJT_DEFAULT
condition|)
name|pageout_ok
operator|=
name|true
expr_stmt|;
elseif|else
if|if
condition|(
name|disable_swap_pageouts
condition|)
name|pageout_ok
operator|=
name|false
expr_stmt|;
else|else
name|pageout_ok
operator|=
name|true
expr_stmt|;
if|if
condition|(
operator|!
name|pageout_ok
condition|)
block|{
name|requeue_page
label|:
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|true
expr_stmt|;
name|vm_page_requeue_locked
argument_list|(
name|m
argument_list|)
expr_stmt|;
goto|goto
name|drop_page
goto|;
block|}
comment|/* 			 * Form a cluster with adjacent, dirty pages from the 			 * same object, and page out that entire cluster. 			 * 			 * The adjacent, dirty pages must also be in the 			 * laundry.  However, their mappings are not checked 			 * for new references.  Consequently, a recently 			 * referenced page may be paged out.  However, that 			 * page will not be prematurely reclaimed.  After page 			 * out, the page will be placed in the inactive queue, 			 * where any new references will be detected and the 			 * page reactivated. 			 */
name|error
operator|=
name|vm_pageout_clean
argument_list|(
name|m
argument_list|,
operator|&
name|numpagedout
argument_list|)
expr_stmt|;
if|if
condition|(
name|error
operator|==
literal|0
condition|)
block|{
name|launder
operator|-=
name|numpagedout
expr_stmt|;
name|maxscan
operator|-=
name|numpagedout
operator|-
literal|1
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|error
operator|==
name|EDEADLK
condition|)
block|{
name|pageout_lock_miss
operator|++
expr_stmt|;
name|vnodes_skipped
operator|++
expr_stmt|;
block|}
goto|goto
name|relock_queue
goto|;
block|}
name|drop_page
label|:
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|relock_queue
label|:
if|if
condition|(
operator|!
name|queue_locked
condition|)
block|{
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|true
expr_stmt|;
block|}
name|next
operator|=
name|TAILQ_NEXT
argument_list|(
operator|&
name|vmd
operator|->
name|vmd_laundry_marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|TAILQ_REMOVE
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
operator|&
name|vmd
operator|->
name|vmd_laundry_marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
block|}
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
comment|/* 	 * Wakeup the sync daemon if we skipped a vnode in a writeable object 	 * and we didn't launder enough pages. 	 */
if|if
condition|(
name|vnodes_skipped
operator|>
literal|0
operator|&&
name|launder
operator|>
literal|0
condition|)
operator|(
name|void
operator|)
name|speedup_syncer
argument_list|()
expr_stmt|;
return|return
operator|(
name|starting_target
operator|-
name|launder
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  * Compute the integer square root.  */
end_comment

begin_function
specifier|static
name|u_int
name|isqrt
parameter_list|(
name|u_int
name|num
parameter_list|)
block|{
name|u_int
name|bit
decl_stmt|,
name|root
decl_stmt|,
name|tmp
decl_stmt|;
name|bit
operator|=
literal|1u
operator|<<
operator|(
operator|(
name|NBBY
operator|*
sizeof|sizeof
argument_list|(
name|u_int
argument_list|)
operator|)
operator|-
literal|2
operator|)
expr_stmt|;
while|while
condition|(
name|bit
operator|>
name|num
condition|)
name|bit
operator|>>=
literal|2
expr_stmt|;
name|root
operator|=
literal|0
expr_stmt|;
while|while
condition|(
name|bit
operator|!=
literal|0
condition|)
block|{
name|tmp
operator|=
name|root
operator|+
name|bit
expr_stmt|;
name|root
operator|>>=
literal|1
expr_stmt|;
if|if
condition|(
name|num
operator|>=
name|tmp
condition|)
block|{
name|num
operator|-=
name|tmp
expr_stmt|;
name|root
operator|+=
name|bit
expr_stmt|;
block|}
name|bit
operator|>>=
literal|2
expr_stmt|;
block|}
return|return
operator|(
name|root
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  * Perform the work of the laundry thread: periodically wake up and determine  * whether any pages need to be laundered.  If so, determine the number of pages  * that need to be laundered, and launder them.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout_laundry_worker
parameter_list|(
name|void
modifier|*
name|arg
parameter_list|)
block|{
name|struct
name|vm_domain
modifier|*
name|domain
decl_stmt|;
name|struct
name|vm_pagequeue
modifier|*
name|pq
decl_stmt|;
name|uint64_t
name|nclean
decl_stmt|,
name|ndirty
decl_stmt|;
name|u_int
name|last_launder
decl_stmt|,
name|wakeups
decl_stmt|;
name|int
name|domidx
decl_stmt|,
name|last_target
decl_stmt|,
name|launder
decl_stmt|,
name|shortfall
decl_stmt|,
name|shortfall_cycle
decl_stmt|,
name|target
decl_stmt|;
name|bool
name|in_shortfall
decl_stmt|;
name|domidx
operator|=
operator|(
name|uintptr_t
operator|)
name|arg
expr_stmt|;
name|domain
operator|=
operator|&
name|vm_dom
index|[
name|domidx
index|]
expr_stmt|;
name|pq
operator|=
operator|&
name|domain
operator|->
name|vmd_pagequeues
index|[
name|PQ_LAUNDRY
index|]
expr_stmt|;
name|KASSERT
argument_list|(
name|domain
operator|->
name|vmd_segs
operator|!=
literal|0
argument_list|,
operator|(
literal|"domain without segments"
operator|)
argument_list|)
expr_stmt|;
name|vm_pageout_init_marker
argument_list|(
operator|&
name|domain
operator|->
name|vmd_laundry_marker
argument_list|,
name|PQ_LAUNDRY
argument_list|)
expr_stmt|;
name|shortfall
operator|=
literal|0
expr_stmt|;
name|in_shortfall
operator|=
name|false
expr_stmt|;
name|shortfall_cycle
operator|=
literal|0
expr_stmt|;
name|target
operator|=
literal|0
expr_stmt|;
name|last_launder
operator|=
literal|0
expr_stmt|;
comment|/* 	 * The pageout laundry worker is never done, so loop forever. 	 */
for|for
control|(
init|;
condition|;
control|)
block|{
name|KASSERT
argument_list|(
name|target
operator|>=
literal|0
argument_list|,
operator|(
literal|"negative target %d"
operator|,
name|target
operator|)
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
name|shortfall_cycle
operator|>=
literal|0
argument_list|,
operator|(
literal|"negative cycle %d"
operator|,
name|shortfall_cycle
operator|)
argument_list|)
expr_stmt|;
name|launder
operator|=
literal|0
expr_stmt|;
name|wakeups
operator|=
name|VM_METER_PCPU_CNT
argument_list|(
name|v_pdwakeups
argument_list|)
expr_stmt|;
comment|/* 		 * First determine whether we need to launder pages to meet a 		 * shortage of free pages. 		 */
if|if
condition|(
name|shortfall
operator|>
literal|0
condition|)
block|{
name|in_shortfall
operator|=
name|true
expr_stmt|;
name|shortfall_cycle
operator|=
name|VM_LAUNDER_RATE
operator|/
name|VM_INACT_SCAN_RATE
expr_stmt|;
name|target
operator|=
name|shortfall
expr_stmt|;
block|}
elseif|else
if|if
condition|(
operator|!
name|in_shortfall
condition|)
goto|goto
name|trybackground
goto|;
elseif|else
if|if
condition|(
name|shortfall_cycle
operator|==
literal|0
operator|||
name|vm_laundry_target
argument_list|()
operator|<=
literal|0
condition|)
block|{
comment|/* 			 * We recently entered shortfall and began laundering 			 * pages.  If we have completed that laundering run 			 * (and we are no longer in shortfall) or we have met 			 * our laundry target through other activity, then we 			 * can stop laundering pages. 			 */
name|in_shortfall
operator|=
name|false
expr_stmt|;
name|target
operator|=
literal|0
expr_stmt|;
goto|goto
name|trybackground
goto|;
block|}
name|last_launder
operator|=
name|wakeups
expr_stmt|;
name|launder
operator|=
name|target
operator|/
name|shortfall_cycle
operator|--
expr_stmt|;
goto|goto
name|dolaundry
goto|;
comment|/* 		 * There's no immediate need to launder any pages; see if we 		 * meet the conditions to perform background laundering: 		 * 		 * 1. The ratio of dirty to clean inactive pages exceeds the 		 *    background laundering threshold and the pagedaemon has 		 *    been woken up to reclaim pages since our last 		 *    laundering, or 		 * 2. we haven't yet reached the target of the current 		 *    background laundering run. 		 * 		 * The background laundering threshold is not a constant. 		 * Instead, it is a slowly growing function of the number of 		 * page daemon wakeups since the last laundering.  Thus, as the 		 * ratio of dirty to clean inactive pages grows, the amount of 		 * memory pressure required to trigger laundering decreases. 		 */
name|trybackground
label|:
name|nclean
operator|=
name|vm_cnt
operator|.
name|v_inactive_count
operator|+
name|vm_cnt
operator|.
name|v_free_count
expr_stmt|;
name|ndirty
operator|=
name|vm_cnt
operator|.
name|v_laundry_count
expr_stmt|;
if|if
condition|(
name|target
operator|==
literal|0
operator|&&
name|wakeups
operator|!=
name|last_launder
operator|&&
name|ndirty
operator|*
name|isqrt
argument_list|(
name|wakeups
operator|-
name|last_launder
argument_list|)
operator|>=
name|nclean
condition|)
block|{
name|target
operator|=
name|vm_background_launder_target
expr_stmt|;
block|}
comment|/* 		 * We have a non-zero background laundering target.  If we've 		 * laundered up to our maximum without observing a page daemon 		 * wakeup, just stop.  This is a safety belt that ensures we 		 * don't launder an excessive amount if memory pressure is low 		 * and the ratio of dirty to clean pages is large.  Otherwise, 		 * proceed at the background laundering rate. 		 */
if|if
condition|(
name|target
operator|>
literal|0
condition|)
block|{
if|if
condition|(
name|wakeups
operator|!=
name|last_launder
condition|)
block|{
name|last_launder
operator|=
name|wakeups
expr_stmt|;
name|last_target
operator|=
name|target
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|last_target
operator|-
name|target
operator|>=
name|vm_background_launder_max
operator|*
name|PAGE_SIZE
operator|/
literal|1024
condition|)
block|{
name|target
operator|=
literal|0
expr_stmt|;
block|}
name|launder
operator|=
name|vm_background_launder_rate
operator|*
name|PAGE_SIZE
operator|/
literal|1024
expr_stmt|;
name|launder
operator|/=
name|VM_LAUNDER_RATE
expr_stmt|;
if|if
condition|(
name|launder
operator|>
name|target
condition|)
name|launder
operator|=
name|target
expr_stmt|;
block|}
name|dolaundry
label|:
if|if
condition|(
name|launder
operator|>
literal|0
condition|)
block|{
comment|/* 			 * Because of I/O clustering, the number of laundered 			 * pages could exceed "target" by the maximum size of 			 * a cluster minus one.  			 */
name|target
operator|-=
name|min
argument_list|(
name|vm_pageout_launder
argument_list|(
name|domain
argument_list|,
name|launder
argument_list|,
name|in_shortfall
argument_list|)
argument_list|,
name|target
argument_list|)
expr_stmt|;
name|pause
argument_list|(
literal|"laundp"
argument_list|,
name|hz
operator|/
name|VM_LAUNDER_RATE
argument_list|)
expr_stmt|;
block|}
comment|/* 		 * If we're not currently laundering pages and the page daemon 		 * hasn't posted a new request, sleep until the page daemon 		 * kicks us. 		 */
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
if|if
condition|(
name|target
operator|==
literal|0
operator|&&
name|vm_laundry_request
operator|==
name|VM_LAUNDRY_IDLE
condition|)
operator|(
name|void
operator|)
name|mtx_sleep
argument_list|(
operator|&
name|vm_laundry_request
argument_list|,
name|vm_pagequeue_lockptr
argument_list|(
name|pq
argument_list|)
argument_list|,
name|PVM
argument_list|,
literal|"launds"
argument_list|,
literal|0
argument_list|)
expr_stmt|;
comment|/* 		 * If the pagedaemon has indicated that it's in shortfall, start 		 * a shortfall laundering unless we're already in the middle of 		 * one.  This may preempt a background laundering. 		 */
if|if
condition|(
name|vm_laundry_request
operator|==
name|VM_LAUNDRY_SHORTFALL
operator|&&
operator|(
operator|!
name|in_shortfall
operator|||
name|shortfall_cycle
operator|==
literal|0
operator|)
condition|)
block|{
name|shortfall
operator|=
name|vm_laundry_target
argument_list|()
operator|+
name|vm_pageout_deficit
expr_stmt|;
name|target
operator|=
literal|0
expr_stmt|;
block|}
else|else
name|shortfall
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|target
operator|==
literal|0
condition|)
name|vm_laundry_request
operator|=
name|VM_LAUNDRY_IDLE
expr_stmt|;
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
block|}
block|}
end_function

begin_comment
comment|/*  *	vm_pageout_scan does the dirty work for the pageout daemon.  *  *	pass == 0: Update active LRU/deactivate pages  *	pass>= 1: Free inactive pages  *  * Returns true if pass was zero or enough pages were freed by the inactive  * queue scan to meet the target.  */
end_comment

begin_function
specifier|static
name|bool
name|vm_pageout_scan
parameter_list|(
name|struct
name|vm_domain
modifier|*
name|vmd
parameter_list|,
name|int
name|pass
parameter_list|)
block|{
name|vm_page_t
name|m
decl_stmt|,
name|next
decl_stmt|;
name|struct
name|vm_pagequeue
modifier|*
name|pq
decl_stmt|;
name|vm_object_t
name|object
decl_stmt|;
name|long
name|min_scan
decl_stmt|;
name|int
name|act_delta
decl_stmt|,
name|addl_page_shortage
decl_stmt|,
name|deficit
decl_stmt|,
name|inactq_shortage
decl_stmt|,
name|maxscan
decl_stmt|;
name|int
name|page_shortage
decl_stmt|,
name|scan_tick
decl_stmt|,
name|scanned
decl_stmt|,
name|starting_page_shortage
decl_stmt|;
name|boolean_t
name|queue_locked
decl_stmt|;
comment|/* 	 * If we need to reclaim memory ask kernel caches to return 	 * some.  We rate limit to avoid thrashing. 	 */
if|if
condition|(
name|vmd
operator|==
operator|&
name|vm_dom
index|[
literal|0
index|]
operator|&&
name|pass
operator|>
literal|0
operator|&&
operator|(
name|time_uptime
operator|-
name|lowmem_uptime
operator|)
operator|>=
name|lowmem_period
condition|)
block|{
comment|/* 		 * Decrease registered cache sizes. 		 */
name|SDT_PROBE0
argument_list|(
name|vm
argument_list|, , ,
name|vm__lowmem_scan
argument_list|)
expr_stmt|;
name|EVENTHANDLER_INVOKE
argument_list|(
name|vm_lowmem
argument_list|,
name|VM_LOW_PAGES
argument_list|)
expr_stmt|;
comment|/* 		 * We do this explicitly after the caches have been 		 * drained above. 		 */
name|uma_reclaim
argument_list|()
expr_stmt|;
name|lowmem_uptime
operator|=
name|time_uptime
expr_stmt|;
block|}
comment|/* 	 * The addl_page_shortage is the number of temporarily 	 * stuck pages in the inactive queue.  In other words, the 	 * number of pages from the inactive count that should be 	 * discounted in setting the target for the active queue scan. 	 */
name|addl_page_shortage
operator|=
literal|0
expr_stmt|;
comment|/* 	 * Calculate the number of pages that we want to free.  This number 	 * can be negative if many pages are freed between the wakeup call to 	 * the page daemon and this calculation. 	 */
if|if
condition|(
name|pass
operator|>
literal|0
condition|)
block|{
name|deficit
operator|=
name|atomic_readandclear_int
argument_list|(
operator|&
name|vm_pageout_deficit
argument_list|)
expr_stmt|;
name|page_shortage
operator|=
name|vm_paging_target
argument_list|()
operator|+
name|deficit
expr_stmt|;
block|}
else|else
name|page_shortage
operator|=
name|deficit
operator|=
literal|0
expr_stmt|;
name|starting_page_shortage
operator|=
name|page_shortage
expr_stmt|;
comment|/* 	 * Start scanning the inactive queue for pages that we can free.  The 	 * scan will stop when we reach the target or we have scanned the 	 * entire queue.  (Note that m->act_count is not used to make 	 * decisions for the inactive queue, only for the active queue.) 	 */
name|pq
operator|=
operator|&
name|vmd
operator|->
name|vmd_pagequeues
index|[
name|PQ_INACTIVE
index|]
expr_stmt|;
name|maxscan
operator|=
name|pq
operator|->
name|pq_cnt
expr_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|TRUE
expr_stmt|;
for|for
control|(
name|m
operator|=
name|TAILQ_FIRST
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|)
init|;
name|m
operator|!=
name|NULL
operator|&&
name|maxscan
operator|--
operator|>
literal|0
operator|&&
name|page_shortage
operator|>
literal|0
condition|;
name|m
operator|=
name|next
control|)
block|{
name|vm_pagequeue_assert_locked
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
name|queue_locked
argument_list|,
operator|(
literal|"unlocked inactive queue"
operator|)
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
name|vm_page_inactive
argument_list|(
name|m
argument_list|)
argument_list|,
operator|(
literal|"Inactive queue %p"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_pdpages
argument_list|)
expr_stmt|;
name|next
operator|=
name|TAILQ_NEXT
argument_list|(
name|m
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
comment|/* 		 * skip marker pages 		 */
if|if
condition|(
name|m
operator|->
name|flags
operator|&
name|PG_MARKER
condition|)
continue|continue;
name|KASSERT
argument_list|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_FICTITIOUS
operator|)
operator|==
literal|0
argument_list|,
operator|(
literal|"Fictitious page %p cannot be in inactive queue"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
operator|(
name|m
operator|->
name|oflags
operator|&
name|VPO_UNMANAGED
operator|)
operator|==
literal|0
argument_list|,
operator|(
literal|"Unmanaged page %p cannot be in inactive queue"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
comment|/* 		 * The page or object lock acquisitions fail if the 		 * page was removed from the queue or moved to a 		 * different position within the queue.  In either 		 * case, addl_page_shortage should not be incremented. 		 */
if|if
condition|(
operator|!
name|vm_pageout_page_lock
argument_list|(
name|m
argument_list|,
operator|&
name|next
argument_list|)
condition|)
goto|goto
name|unlock_page
goto|;
elseif|else
if|if
condition|(
name|m
operator|->
name|hold_count
operator|!=
literal|0
condition|)
block|{
comment|/* 			 * Held pages are essentially stuck in the 			 * queue.  So, they ought to be discounted 			 * from the inactive count.  See the 			 * calculation of inactq_shortage before the 			 * loop over the active queue below. 			 */
name|addl_page_shortage
operator|++
expr_stmt|;
goto|goto
name|unlock_page
goto|;
block|}
name|object
operator|=
name|m
operator|->
name|object
expr_stmt|;
if|if
condition|(
operator|!
name|VM_OBJECT_TRYWLOCK
argument_list|(
name|object
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|vm_pageout_fallback_object_lock
argument_list|(
name|m
argument_list|,
operator|&
name|next
argument_list|)
condition|)
goto|goto
name|unlock_object
goto|;
elseif|else
if|if
condition|(
name|m
operator|->
name|hold_count
operator|!=
literal|0
condition|)
block|{
name|addl_page_shortage
operator|++
expr_stmt|;
goto|goto
name|unlock_object
goto|;
block|}
block|}
if|if
condition|(
name|vm_page_busied
argument_list|(
name|m
argument_list|)
condition|)
block|{
comment|/* 			 * Don't mess with busy pages.  Leave them at 			 * the front of the queue.  Most likely, they 			 * are being paged out and will leave the 			 * queue shortly after the scan finishes.  So, 			 * they ought to be discounted from the 			 * inactive count. 			 */
name|addl_page_shortage
operator|++
expr_stmt|;
name|unlock_object
label|:
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|unlock_page
label|:
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|KASSERT
argument_list|(
name|m
operator|->
name|hold_count
operator|==
literal|0
argument_list|,
operator|(
literal|"Held page %p"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
comment|/* 		 * Dequeue the inactive page and unlock the inactive page 		 * queue, invalidating the 'next' pointer.  Dequeueing the 		 * page here avoids a later reacquisition (and release) of 		 * the inactive page queue lock when vm_page_activate(), 		 * vm_page_free(), or vm_page_launder() is called.  Use a 		 * marker to remember our place in the inactive queue. 		 */
name|TAILQ_INSERT_AFTER
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
name|m
argument_list|,
operator|&
name|vmd
operator|->
name|vmd_marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|vm_page_dequeue_locked
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|FALSE
expr_stmt|;
comment|/* 		 * Invalid pages can be easily freed. They cannot be 		 * mapped, vm_page_free() asserts this. 		 */
if|if
condition|(
name|m
operator|->
name|valid
operator|==
literal|0
condition|)
goto|goto
name|free_page
goto|;
comment|/* 		 * If the page has been referenced and the object is not dead, 		 * reactivate or requeue the page depending on whether the 		 * object is mapped. 		 */
if|if
condition|(
operator|(
name|m
operator|->
name|aflags
operator|&
name|PGA_REFERENCED
operator|)
operator|!=
literal|0
condition|)
block|{
name|vm_page_aflag_clear
argument_list|(
name|m
argument_list|,
name|PGA_REFERENCED
argument_list|)
expr_stmt|;
name|act_delta
operator|=
literal|1
expr_stmt|;
block|}
else|else
name|act_delta
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|object
operator|->
name|ref_count
operator|!=
literal|0
condition|)
block|{
name|act_delta
operator|+=
name|pmap_ts_referenced
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|KASSERT
argument_list|(
operator|!
name|pmap_page_is_mapped
argument_list|(
name|m
argument_list|)
argument_list|,
operator|(
literal|"vm_pageout_scan: page %p is mapped"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|act_delta
operator|!=
literal|0
condition|)
block|{
if|if
condition|(
name|object
operator|->
name|ref_count
operator|!=
literal|0
condition|)
block|{
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_reactivated
argument_list|)
expr_stmt|;
name|vm_page_activate
argument_list|(
name|m
argument_list|)
expr_stmt|;
comment|/* 				 * Increase the activation count if the page 				 * was referenced while in the inactive queue. 				 * This makes it less likely that the page will 				 * be returned prematurely to the inactive 				 * queue.  				 */
name|m
operator|->
name|act_count
operator|+=
name|act_delta
operator|+
name|ACT_ADVANCE
expr_stmt|;
goto|goto
name|drop_page
goto|;
block|}
elseif|else
if|if
condition|(
operator|(
name|object
operator|->
name|flags
operator|&
name|OBJ_DEAD
operator|)
operator|==
literal|0
condition|)
block|{
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|TRUE
expr_stmt|;
name|m
operator|->
name|queue
operator|=
name|PQ_INACTIVE
expr_stmt|;
name|TAILQ_INSERT_TAIL
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
name|m
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|vm_pagequeue_cnt_inc
argument_list|(
name|pq
argument_list|)
expr_stmt|;
goto|goto
name|drop_page
goto|;
block|}
block|}
comment|/* 		 * If the page appears to be clean at the machine-independent 		 * layer, then remove all of its mappings from the pmap in 		 * anticipation of freeing it.  If, however, any of the page's 		 * mappings allow write access, then the page may still be 		 * modified until the last of those mappings are removed. 		 */
if|if
condition|(
name|object
operator|->
name|ref_count
operator|!=
literal|0
condition|)
block|{
name|vm_page_test_dirty
argument_list|(
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|m
operator|->
name|dirty
operator|==
literal|0
condition|)
name|pmap_remove_all
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
comment|/* 		 * Clean pages can be freed, but dirty pages must be sent back 		 * to the laundry, unless they belong to a dead object. 		 * Requeueing dirty pages from dead objects is pointless, as 		 * they are being paged out and freed by the thread that 		 * destroyed the object. 		 */
if|if
condition|(
name|m
operator|->
name|dirty
operator|==
literal|0
condition|)
block|{
name|free_page
label|:
name|vm_page_free
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_dfree
argument_list|)
expr_stmt|;
operator|--
name|page_shortage
expr_stmt|;
block|}
elseif|else
if|if
condition|(
operator|(
name|object
operator|->
name|flags
operator|&
name|OBJ_DEAD
operator|)
operator|==
literal|0
condition|)
name|vm_page_launder
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|drop_page
label|:
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|queue_locked
condition|)
block|{
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|queue_locked
operator|=
name|TRUE
expr_stmt|;
block|}
name|next
operator|=
name|TAILQ_NEXT
argument_list|(
operator|&
name|vmd
operator|->
name|vmd_marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
name|TAILQ_REMOVE
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|,
operator|&
name|vmd
operator|->
name|vmd_marker
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
block|}
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
comment|/* 	 * Wake up the laundry thread so that it can perform any needed 	 * laundering.  If we didn't meet our target, we're in shortfall and 	 * need to launder more aggressively. 	 */
if|if
condition|(
name|vm_laundry_request
operator|==
name|VM_LAUNDRY_IDLE
operator|&&
name|starting_page_shortage
operator|>
literal|0
condition|)
block|{
name|pq
operator|=
operator|&
name|vm_dom
index|[
literal|0
index|]
operator|.
name|vmd_pagequeues
index|[
name|PQ_LAUNDRY
index|]
expr_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
if|if
condition|(
name|page_shortage
operator|>
literal|0
condition|)
block|{
name|vm_laundry_request
operator|=
name|VM_LAUNDRY_SHORTFALL
expr_stmt|;
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_pdshortfalls
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|vm_laundry_request
operator|!=
name|VM_LAUNDRY_SHORTFALL
condition|)
name|vm_laundry_request
operator|=
name|VM_LAUNDRY_BACKGROUND
expr_stmt|;
name|wakeup
argument_list|(
operator|&
name|vm_laundry_request
argument_list|)
expr_stmt|;
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
block|}
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
comment|/* 	 * Wakeup the swapout daemon if we didn't free the targeted number of 	 * pages. 	 */
if|if
condition|(
name|vm_swap_enabled
operator|&&
name|page_shortage
operator|>
literal|0
condition|)
name|vm_req_vmdaemon
argument_list|(
name|VM_SWAP_NORMAL
argument_list|)
expr_stmt|;
endif|#
directive|endif
comment|/* 	 * If the inactive queue scan fails repeatedly to meet its 	 * target, kill the largest process. 	 */
name|vm_pageout_mightbe_oom
argument_list|(
name|vmd
argument_list|,
name|page_shortage
argument_list|,
name|starting_page_shortage
argument_list|)
expr_stmt|;
comment|/* 	 * Compute the number of pages we want to try to move from the 	 * active queue to either the inactive or laundry queue. 	 * 	 * When scanning active pages, we make clean pages count more heavily 	 * towards the page shortage than dirty pages.  This is because dirty 	 * pages must be laundered before they can be reused and thus have less 	 * utility when attempting to quickly alleviate a shortage.  However, 	 * this weighting also causes the scan to deactivate dirty pages more 	 * more aggressively, improving the effectiveness of clustering and 	 * ensuring that they can eventually be reused. 	 */
name|inactq_shortage
operator|=
name|vm_cnt
operator|.
name|v_inactive_target
operator|-
operator|(
name|vm_cnt
operator|.
name|v_inactive_count
operator|+
name|vm_cnt
operator|.
name|v_laundry_count
operator|/
name|act_scan_laundry_weight
operator|)
operator|+
name|vm_paging_target
argument_list|()
operator|+
name|deficit
operator|+
name|addl_page_shortage
expr_stmt|;
name|page_shortage
operator|*=
name|act_scan_laundry_weight
expr_stmt|;
name|pq
operator|=
operator|&
name|vmd
operator|->
name|vmd_pagequeues
index|[
name|PQ_ACTIVE
index|]
expr_stmt|;
name|vm_pagequeue_lock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
name|maxscan
operator|=
name|pq
operator|->
name|pq_cnt
expr_stmt|;
comment|/* 	 * If we're just idle polling attempt to visit every 	 * active page within 'update_period' seconds. 	 */
name|scan_tick
operator|=
name|ticks
expr_stmt|;
if|if
condition|(
name|vm_pageout_update_period
operator|!=
literal|0
condition|)
block|{
name|min_scan
operator|=
name|pq
operator|->
name|pq_cnt
expr_stmt|;
name|min_scan
operator|*=
name|scan_tick
operator|-
name|vmd
operator|->
name|vmd_last_active_scan
expr_stmt|;
name|min_scan
operator|/=
name|hz
operator|*
name|vm_pageout_update_period
expr_stmt|;
block|}
else|else
name|min_scan
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|min_scan
operator|>
literal|0
operator|||
operator|(
name|inactq_shortage
operator|>
literal|0
operator|&&
name|maxscan
operator|>
literal|0
operator|)
condition|)
name|vmd
operator|->
name|vmd_last_active_scan
operator|=
name|scan_tick
expr_stmt|;
comment|/* 	 * Scan the active queue for pages that can be deactivated.  Update 	 * the per-page activity counter and use it to identify deactivation 	 * candidates.  Held pages may be deactivated. 	 */
for|for
control|(
name|m
operator|=
name|TAILQ_FIRST
argument_list|(
operator|&
name|pq
operator|->
name|pq_pl
argument_list|)
operator|,
name|scanned
operator|=
literal|0
init|;
name|m
operator|!=
name|NULL
operator|&&
operator|(
name|scanned
operator|<
name|min_scan
operator|||
operator|(
name|inactq_shortage
operator|>
literal|0
operator|&&
name|scanned
operator|<
name|maxscan
operator|)
operator|)
condition|;
name|m
operator|=
name|next
operator|,
name|scanned
operator|++
control|)
block|{
name|KASSERT
argument_list|(
name|m
operator|->
name|queue
operator|==
name|PQ_ACTIVE
argument_list|,
operator|(
literal|"vm_pageout_scan: page %p isn't active"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
name|next
operator|=
name|TAILQ_NEXT
argument_list|(
name|m
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_MARKER
operator|)
operator|!=
literal|0
condition|)
continue|continue;
name|KASSERT
argument_list|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_FICTITIOUS
operator|)
operator|==
literal|0
argument_list|,
operator|(
literal|"Fictitious page %p cannot be in active queue"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
operator|(
name|m
operator|->
name|oflags
operator|&
name|VPO_UNMANAGED
operator|)
operator|==
literal|0
argument_list|,
operator|(
literal|"Unmanaged page %p cannot be in active queue"
operator|,
name|m
operator|)
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|vm_pageout_page_lock
argument_list|(
name|m
argument_list|,
operator|&
name|next
argument_list|)
condition|)
block|{
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|/* 		 * The count for page daemon pages is updated after checking 		 * the page for eligibility. 		 */
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_pdpages
argument_list|)
expr_stmt|;
comment|/* 		 * Check to see "how much" the page has been used. 		 */
if|if
condition|(
operator|(
name|m
operator|->
name|aflags
operator|&
name|PGA_REFERENCED
operator|)
operator|!=
literal|0
condition|)
block|{
name|vm_page_aflag_clear
argument_list|(
name|m
argument_list|,
name|PGA_REFERENCED
argument_list|)
expr_stmt|;
name|act_delta
operator|=
literal|1
expr_stmt|;
block|}
else|else
name|act_delta
operator|=
literal|0
expr_stmt|;
comment|/* 		 * Perform an unsynchronized object ref count check.  While 		 * the page lock ensures that the page is not reallocated to 		 * another object, in particular, one with unmanaged mappings 		 * that cannot support pmap_ts_referenced(), two races are, 		 * nonetheless, possible: 		 * 1) The count was transitioning to zero, but we saw a non- 		 *    zero value.  pmap_ts_referenced() will return zero 		 *    because the page is not mapped. 		 * 2) The count was transitioning to one, but we saw zero.  		 *    This race delays the detection of a new reference.  At 		 *    worst, we will deactivate and reactivate the page. 		 */
if|if
condition|(
name|m
operator|->
name|object
operator|->
name|ref_count
operator|!=
literal|0
condition|)
name|act_delta
operator|+=
name|pmap_ts_referenced
argument_list|(
name|m
argument_list|)
expr_stmt|;
comment|/* 		 * Advance or decay the act_count based on recent usage. 		 */
if|if
condition|(
name|act_delta
operator|!=
literal|0
condition|)
block|{
name|m
operator|->
name|act_count
operator|+=
name|ACT_ADVANCE
operator|+
name|act_delta
expr_stmt|;
if|if
condition|(
name|m
operator|->
name|act_count
operator|>
name|ACT_MAX
condition|)
name|m
operator|->
name|act_count
operator|=
name|ACT_MAX
expr_stmt|;
block|}
else|else
name|m
operator|->
name|act_count
operator|-=
name|min
argument_list|(
name|m
operator|->
name|act_count
argument_list|,
name|ACT_DECLINE
argument_list|)
expr_stmt|;
comment|/* 		 * Move this page to the tail of the active, inactive or laundry 		 * queue depending on usage. 		 */
if|if
condition|(
name|m
operator|->
name|act_count
operator|==
literal|0
condition|)
block|{
comment|/* Dequeue to avoid later lock recursion. */
name|vm_page_dequeue_locked
argument_list|(
name|m
argument_list|)
expr_stmt|;
comment|/* 			 * When not short for inactive pages, let dirty pages go 			 * through the inactive queue before moving to the 			 * laundry queues.  This gives them some extra time to 			 * be reactivated, potentially avoiding an expensive 			 * pageout.  During a page shortage, the inactive queue 			 * is necessarily small, so we may move dirty pages 			 * directly to the laundry queue. 			 */
if|if
condition|(
name|inactq_shortage
operator|<=
literal|0
condition|)
name|vm_page_deactivate
argument_list|(
name|m
argument_list|)
expr_stmt|;
else|else
block|{
comment|/* 				 * Calling vm_page_test_dirty() here would 				 * require acquisition of the object's write 				 * lock.  However, during a page shortage, 				 * directing dirty pages into the laundry 				 * queue is only an optimization and not a 				 * requirement.  Therefore, we simply rely on 				 * the opportunistic updates to the page's 				 * dirty field by the pmap. 				 */
if|if
condition|(
name|m
operator|->
name|dirty
operator|==
literal|0
condition|)
block|{
name|vm_page_deactivate
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|inactq_shortage
operator|-=
name|act_scan_laundry_weight
expr_stmt|;
block|}
else|else
block|{
name|vm_page_launder
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|inactq_shortage
operator|--
expr_stmt|;
block|}
block|}
block|}
else|else
name|vm_page_requeue_locked
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
name|vm_pagequeue_unlock
argument_list|(
name|pq
argument_list|)
expr_stmt|;
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
comment|/* 	 * Idle process swapout -- run once per second. 	 */
if|if
condition|(
name|vm_swap_idle_enabled
condition|)
block|{
specifier|static
name|long
name|lsec
decl_stmt|;
if|if
condition|(
name|time_second
operator|!=
name|lsec
condition|)
block|{
name|vm_req_vmdaemon
argument_list|(
name|VM_SWAP_IDLE
argument_list|)
expr_stmt|;
name|lsec
operator|=
name|time_second
expr_stmt|;
block|}
block|}
endif|#
directive|endif
return|return
operator|(
name|page_shortage
operator|<=
literal|0
operator|)
return|;
block|}
end_function

begin_decl_stmt
specifier|static
name|int
name|vm_pageout_oom_vote
decl_stmt|;
end_decl_stmt

begin_comment
comment|/*  * The pagedaemon threads randlomly select one to perform the  * OOM.  Trying to kill processes before all pagedaemons  * failed to reach free target is premature.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout_mightbe_oom
parameter_list|(
name|struct
name|vm_domain
modifier|*
name|vmd
parameter_list|,
name|int
name|page_shortage
parameter_list|,
name|int
name|starting_page_shortage
parameter_list|)
block|{
name|int
name|old_vote
decl_stmt|;
if|if
condition|(
name|starting_page_shortage
operator|<=
literal|0
operator|||
name|starting_page_shortage
operator|!=
name|page_shortage
condition|)
name|vmd
operator|->
name|vmd_oom_seq
operator|=
literal|0
expr_stmt|;
else|else
name|vmd
operator|->
name|vmd_oom_seq
operator|++
expr_stmt|;
if|if
condition|(
name|vmd
operator|->
name|vmd_oom_seq
operator|<
name|vm_pageout_oom_seq
condition|)
block|{
if|if
condition|(
name|vmd
operator|->
name|vmd_oom
condition|)
block|{
name|vmd
operator|->
name|vmd_oom
operator|=
name|FALSE
expr_stmt|;
name|atomic_subtract_int
argument_list|(
operator|&
name|vm_pageout_oom_vote
argument_list|,
literal|1
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
comment|/* 	 * Do not follow the call sequence until OOM condition is 	 * cleared. 	 */
name|vmd
operator|->
name|vmd_oom_seq
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|vmd
operator|->
name|vmd_oom
condition|)
return|return;
name|vmd
operator|->
name|vmd_oom
operator|=
name|TRUE
expr_stmt|;
name|old_vote
operator|=
name|atomic_fetchadd_int
argument_list|(
operator|&
name|vm_pageout_oom_vote
argument_list|,
literal|1
argument_list|)
expr_stmt|;
if|if
condition|(
name|old_vote
operator|!=
name|vm_ndomains
operator|-
literal|1
condition|)
return|return;
comment|/* 	 * The current pagedaemon thread is the last in the quorum to 	 * start OOM.  Initiate the selection and signaling of the 	 * victim. 	 */
name|vm_pageout_oom
argument_list|(
name|VM_OOM_MEM
argument_list|)
expr_stmt|;
comment|/* 	 * After one round of OOM terror, recall our vote.  On the 	 * next pass, current pagedaemon would vote again if the low 	 * memory condition is still there, due to vmd_oom being 	 * false. 	 */
name|vmd
operator|->
name|vmd_oom
operator|=
name|FALSE
expr_stmt|;
name|atomic_subtract_int
argument_list|(
operator|&
name|vm_pageout_oom_vote
argument_list|,
literal|1
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|/*  * The OOM killer is the page daemon's action of last resort when  * memory allocation requests have been stalled for a prolonged period  * of time because it cannot reclaim memory.  This function computes  * the approximate number of physical pages that could be reclaimed if  * the specified address space is destroyed.  *  * Private, anonymous memory owned by the address space is the  * principal resource that we expect to recover after an OOM kill.  * Since the physical pages mapped by the address space's COW entries  * are typically shared pages, they are unlikely to be released and so  * they are not counted.  *  * To get to the point where the page daemon runs the OOM killer, its  * efforts to write-back vnode-backed pages may have stalled.  This  * could be caused by a memory allocation deadlock in the write path  * that might be resolved by an OOM kill.  Therefore, physical pages  * belonging to vnode-backed objects are counted, because they might  * be freed without being written out first if the address space holds  * the last reference to an unlinked vnode.  *  * Similarly, physical pages belonging to OBJT_PHYS objects are  * counted because the address space might hold the last reference to  * the object.  */
end_comment

begin_function
specifier|static
name|long
name|vm_pageout_oom_pagecount
parameter_list|(
name|struct
name|vmspace
modifier|*
name|vmspace
parameter_list|)
block|{
name|vm_map_t
name|map
decl_stmt|;
name|vm_map_entry_t
name|entry
decl_stmt|;
name|vm_object_t
name|obj
decl_stmt|;
name|long
name|res
decl_stmt|;
name|map
operator|=
operator|&
name|vmspace
operator|->
name|vm_map
expr_stmt|;
name|KASSERT
argument_list|(
operator|!
name|map
operator|->
name|system_map
argument_list|,
operator|(
literal|"system map"
operator|)
argument_list|)
expr_stmt|;
name|sx_assert
argument_list|(
operator|&
name|map
operator|->
name|lock
argument_list|,
name|SA_LOCKED
argument_list|)
expr_stmt|;
name|res
operator|=
literal|0
expr_stmt|;
for|for
control|(
name|entry
operator|=
name|map
operator|->
name|header
operator|.
name|next
init|;
name|entry
operator|!=
operator|&
name|map
operator|->
name|header
condition|;
name|entry
operator|=
name|entry
operator|->
name|next
control|)
block|{
if|if
condition|(
operator|(
name|entry
operator|->
name|eflags
operator|&
name|MAP_ENTRY_IS_SUB_MAP
operator|)
operator|!=
literal|0
condition|)
continue|continue;
name|obj
operator|=
name|entry
operator|->
name|object
operator|.
name|vm_object
expr_stmt|;
if|if
condition|(
name|obj
operator|==
name|NULL
condition|)
continue|continue;
if|if
condition|(
operator|(
name|entry
operator|->
name|eflags
operator|&
name|MAP_ENTRY_NEEDS_COPY
operator|)
operator|!=
literal|0
operator|&&
name|obj
operator|->
name|ref_count
operator|!=
literal|1
condition|)
continue|continue;
switch|switch
condition|(
name|obj
operator|->
name|type
condition|)
block|{
case|case
name|OBJT_DEFAULT
case|:
case|case
name|OBJT_SWAP
case|:
case|case
name|OBJT_PHYS
case|:
case|case
name|OBJT_VNODE
case|:
name|res
operator|+=
name|obj
operator|->
name|resident_page_count
expr_stmt|;
break|break;
block|}
block|}
return|return
operator|(
name|res
operator|)
return|;
block|}
end_function

begin_function
name|void
name|vm_pageout_oom
parameter_list|(
name|int
name|shortage
parameter_list|)
block|{
name|struct
name|proc
modifier|*
name|p
decl_stmt|,
modifier|*
name|bigproc
decl_stmt|;
name|vm_offset_t
name|size
decl_stmt|,
name|bigsize
decl_stmt|;
name|struct
name|thread
modifier|*
name|td
decl_stmt|;
name|struct
name|vmspace
modifier|*
name|vm
decl_stmt|;
name|bool
name|breakout
decl_stmt|;
comment|/* 	 * We keep the process bigproc locked once we find it to keep anyone 	 * from messing with it; however, there is a possibility of 	 * deadlock if process B is bigproc and one of it's child processes 	 * attempts to propagate a signal to B while we are waiting for A's 	 * lock while walking this list.  To avoid this, we don't block on 	 * the process lock but just skip a process if it is already locked. 	 */
name|bigproc
operator|=
name|NULL
expr_stmt|;
name|bigsize
operator|=
literal|0
expr_stmt|;
name|sx_slock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
name|FOREACH_PROC_IN_SYSTEM
argument_list|(
argument|p
argument_list|)
block|{
name|PROC_LOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
comment|/* 		 * If this is a system, protected or killed process, skip it. 		 */
if|if
condition|(
name|p
operator|->
name|p_state
operator|!=
name|PRS_NORMAL
operator|||
operator|(
name|p
operator|->
name|p_flag
operator|&
operator|(
name|P_INEXEC
operator||
name|P_PROTECTED
operator||
name|P_SYSTEM
operator||
name|P_WEXIT
operator|)
operator|)
operator|!=
literal|0
operator|||
name|p
operator|->
name|p_pid
operator|==
literal|1
operator|||
name|P_KILLED
argument_list|(
name|p
argument_list|)
operator|||
operator|(
name|p
operator|->
name|p_pid
operator|<
literal|48
operator|&&
name|swap_pager_avail
operator|!=
literal|0
operator|)
condition|)
block|{
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|/* 		 * If the process is in a non-running type state, 		 * don't touch it.  Check all the threads individually. 		 */
name|breakout
operator|=
name|false
expr_stmt|;
name|FOREACH_THREAD_IN_PROC
argument_list|(
argument|p
argument_list|,
argument|td
argument_list|)
block|{
name|thread_lock
argument_list|(
name|td
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|TD_ON_RUNQ
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_RUNNING
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_SLEEPING
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_SUSPENDED
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_SWAPPED
argument_list|(
name|td
argument_list|)
condition|)
block|{
name|thread_unlock
argument_list|(
name|td
argument_list|)
expr_stmt|;
name|breakout
operator|=
name|true
expr_stmt|;
break|break;
block|}
name|thread_unlock
argument_list|(
name|td
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|breakout
condition|)
block|{
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|/* 		 * get the process size 		 */
name|vm
operator|=
name|vmspace_acquire_ref
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|vm
operator|==
name|NULL
condition|)
block|{
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|_PHOLD_LITE
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|sx_sunlock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|vm_map_trylock_read
argument_list|(
operator|&
name|vm
operator|->
name|vm_map
argument_list|)
condition|)
block|{
name|vmspace_free
argument_list|(
name|vm
argument_list|)
expr_stmt|;
name|sx_slock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
name|PRELE
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|size
operator|=
name|vmspace_swap_count
argument_list|(
name|vm
argument_list|)
expr_stmt|;
if|if
condition|(
name|shortage
operator|==
name|VM_OOM_MEM
condition|)
name|size
operator|+=
name|vm_pageout_oom_pagecount
argument_list|(
name|vm
argument_list|)
expr_stmt|;
name|vm_map_unlock_read
argument_list|(
operator|&
name|vm
operator|->
name|vm_map
argument_list|)
expr_stmt|;
name|vmspace_free
argument_list|(
name|vm
argument_list|)
expr_stmt|;
name|sx_slock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
comment|/* 		 * If this process is bigger than the biggest one, 		 * remember it. 		 */
if|if
condition|(
name|size
operator|>
name|bigsize
condition|)
block|{
if|if
condition|(
name|bigproc
operator|!=
name|NULL
condition|)
name|PRELE
argument_list|(
name|bigproc
argument_list|)
expr_stmt|;
name|bigproc
operator|=
name|p
expr_stmt|;
name|bigsize
operator|=
name|size
expr_stmt|;
block|}
else|else
block|{
name|PRELE
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
block|}
name|sx_sunlock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
if|if
condition|(
name|bigproc
operator|!=
name|NULL
condition|)
block|{
if|if
condition|(
name|vm_panic_on_oom
operator|!=
literal|0
condition|)
name|panic
argument_list|(
literal|"out of swap space"
argument_list|)
expr_stmt|;
name|PROC_LOCK
argument_list|(
name|bigproc
argument_list|)
expr_stmt|;
name|killproc
argument_list|(
name|bigproc
argument_list|,
literal|"out of swap space"
argument_list|)
expr_stmt|;
name|sched_nice
argument_list|(
name|bigproc
argument_list|,
name|PRIO_MIN
argument_list|)
expr_stmt|;
name|_PRELE
argument_list|(
name|bigproc
argument_list|)
expr_stmt|;
name|PROC_UNLOCK
argument_list|(
name|bigproc
argument_list|)
expr_stmt|;
name|wakeup
argument_list|(
operator|&
name|vm_cnt
operator|.
name|v_free_count
argument_list|)
expr_stmt|;
block|}
block|}
end_function

begin_function
specifier|static
name|void
name|vm_pageout_worker
parameter_list|(
name|void
modifier|*
name|arg
parameter_list|)
block|{
name|struct
name|vm_domain
modifier|*
name|domain
decl_stmt|;
name|int
name|domidx
decl_stmt|,
name|pass
decl_stmt|;
name|bool
name|target_met
decl_stmt|;
name|domidx
operator|=
operator|(
name|uintptr_t
operator|)
name|arg
expr_stmt|;
name|domain
operator|=
operator|&
name|vm_dom
index|[
name|domidx
index|]
expr_stmt|;
name|pass
operator|=
literal|0
expr_stmt|;
name|target_met
operator|=
name|true
expr_stmt|;
comment|/* 	 * XXXKIB It could be useful to bind pageout daemon threads to 	 * the cores belonging to the domain, from which vm_page_array 	 * is allocated. 	 */
name|KASSERT
argument_list|(
name|domain
operator|->
name|vmd_segs
operator|!=
literal|0
argument_list|,
operator|(
literal|"domain without segments"
operator|)
argument_list|)
expr_stmt|;
name|domain
operator|->
name|vmd_last_active_scan
operator|=
name|ticks
expr_stmt|;
name|vm_pageout_init_marker
argument_list|(
operator|&
name|domain
operator|->
name|vmd_marker
argument_list|,
name|PQ_INACTIVE
argument_list|)
expr_stmt|;
name|vm_pageout_init_marker
argument_list|(
operator|&
name|domain
operator|->
name|vmd_inacthead
argument_list|,
name|PQ_INACTIVE
argument_list|)
expr_stmt|;
name|TAILQ_INSERT_HEAD
argument_list|(
operator|&
name|domain
operator|->
name|vmd_pagequeues
index|[
name|PQ_INACTIVE
index|]
operator|.
name|pq_pl
argument_list|,
operator|&
name|domain
operator|->
name|vmd_inacthead
argument_list|,
name|plinks
operator|.
name|q
argument_list|)
expr_stmt|;
comment|/* 	 * The pageout daemon worker is never done, so loop forever. 	 */
while|while
condition|(
name|TRUE
condition|)
block|{
name|mtx_lock
argument_list|(
operator|&
name|vm_page_queue_free_mtx
argument_list|)
expr_stmt|;
comment|/* 		 * Generally, after a level>= 1 scan, if there are enough 		 * free pages to wakeup the waiters, then they are already 		 * awake.  A call to vm_page_free() during the scan awakened 		 * them.  However, in the following case, this wakeup serves 		 * to bound the amount of time that a thread might wait. 		 * Suppose a thread's call to vm_page_alloc() fails, but 		 * before that thread calls VM_WAIT, enough pages are freed by 		 * other threads to alleviate the free page shortage.  The 		 * thread will, nonetheless, wait until another page is freed 		 * or this wakeup is performed. 		 */
if|if
condition|(
name|vm_pages_needed
operator|&&
operator|!
name|vm_page_count_min
argument_list|()
condition|)
block|{
name|vm_pages_needed
operator|=
name|false
expr_stmt|;
name|wakeup
argument_list|(
operator|&
name|vm_cnt
operator|.
name|v_free_count
argument_list|)
expr_stmt|;
block|}
comment|/* 		 * Do not clear vm_pageout_wanted until we reach our free page 		 * target.  Otherwise, we may be awakened over and over again, 		 * wasting CPU time. 		 */
if|if
condition|(
name|vm_pageout_wanted
operator|&&
name|target_met
condition|)
name|vm_pageout_wanted
operator|=
name|false
expr_stmt|;
comment|/* 		 * Might the page daemon receive a wakeup call? 		 */
if|if
condition|(
name|vm_pageout_wanted
condition|)
block|{
comment|/* 			 * No.  Either vm_pageout_wanted was set by another 			 * thread during the previous scan, which must have 			 * been a level 0 scan, or vm_pageout_wanted was 			 * already set and the scan failed to free enough 			 * pages.  If we haven't yet performed a level>= 1 			 * (page reclamation) scan, then increase the level 			 * and scan again now.  Otherwise, sleep a bit and 			 * try again later. 			 */
name|mtx_unlock
argument_list|(
operator|&
name|vm_page_queue_free_mtx
argument_list|)
expr_stmt|;
if|if
condition|(
name|pass
operator|>=
literal|1
condition|)
name|pause
argument_list|(
literal|"psleep"
argument_list|,
name|hz
operator|/
name|VM_INACT_SCAN_RATE
argument_list|)
expr_stmt|;
name|pass
operator|++
expr_stmt|;
block|}
else|else
block|{
comment|/* 			 * Yes.  Sleep until pages need to be reclaimed or 			 * have their reference stats updated. 			 */
if|if
condition|(
name|mtx_sleep
argument_list|(
operator|&
name|vm_pageout_wanted
argument_list|,
operator|&
name|vm_page_queue_free_mtx
argument_list|,
name|PDROP
operator||
name|PVM
argument_list|,
literal|"psleep"
argument_list|,
name|hz
argument_list|)
operator|==
literal|0
condition|)
block|{
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_pdwakeups
argument_list|)
expr_stmt|;
name|pass
operator|=
literal|1
expr_stmt|;
block|}
else|else
name|pass
operator|=
literal|0
expr_stmt|;
block|}
name|target_met
operator|=
name|vm_pageout_scan
argument_list|(
name|domain
argument_list|,
name|pass
argument_list|)
expr_stmt|;
block|}
block|}
end_function

begin_comment
comment|/*  *	vm_pageout_init initialises basic pageout daemon settings.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout_init
parameter_list|(
name|void
parameter_list|)
block|{
comment|/* 	 * Initialize some paging parameters. 	 */
name|vm_cnt
operator|.
name|v_interrupt_free_min
operator|=
literal|2
expr_stmt|;
if|if
condition|(
name|vm_cnt
operator|.
name|v_page_count
operator|<
literal|2000
condition|)
name|vm_pageout_page_count
operator|=
literal|8
expr_stmt|;
comment|/* 	 * v_free_reserved needs to include enough for the largest 	 * swap pager structures plus enough for any pv_entry structs 	 * when paging.  	 */
if|if
condition|(
name|vm_cnt
operator|.
name|v_page_count
operator|>
literal|1024
condition|)
name|vm_cnt
operator|.
name|v_free_min
operator|=
literal|4
operator|+
operator|(
name|vm_cnt
operator|.
name|v_page_count
operator|-
literal|1024
operator|)
operator|/
literal|200
expr_stmt|;
else|else
name|vm_cnt
operator|.
name|v_free_min
operator|=
literal|4
expr_stmt|;
name|vm_cnt
operator|.
name|v_pageout_free_min
operator|=
operator|(
literal|2
operator|*
name|MAXBSIZE
operator|)
operator|/
name|PAGE_SIZE
operator|+
name|vm_cnt
operator|.
name|v_interrupt_free_min
expr_stmt|;
name|vm_cnt
operator|.
name|v_free_reserved
operator|=
name|vm_pageout_page_count
operator|+
name|vm_cnt
operator|.
name|v_pageout_free_min
operator|+
operator|(
name|vm_cnt
operator|.
name|v_page_count
operator|/
literal|768
operator|)
expr_stmt|;
name|vm_cnt
operator|.
name|v_free_severe
operator|=
name|vm_cnt
operator|.
name|v_free_min
operator|/
literal|2
expr_stmt|;
name|vm_cnt
operator|.
name|v_free_target
operator|=
literal|4
operator|*
name|vm_cnt
operator|.
name|v_free_min
operator|+
name|vm_cnt
operator|.
name|v_free_reserved
expr_stmt|;
name|vm_cnt
operator|.
name|v_free_min
operator|+=
name|vm_cnt
operator|.
name|v_free_reserved
expr_stmt|;
name|vm_cnt
operator|.
name|v_free_severe
operator|+=
name|vm_cnt
operator|.
name|v_free_reserved
expr_stmt|;
name|vm_cnt
operator|.
name|v_inactive_target
operator|=
operator|(
literal|3
operator|*
name|vm_cnt
operator|.
name|v_free_target
operator|)
operator|/
literal|2
expr_stmt|;
if|if
condition|(
name|vm_cnt
operator|.
name|v_inactive_target
operator|>
name|vm_cnt
operator|.
name|v_free_count
operator|/
literal|3
condition|)
name|vm_cnt
operator|.
name|v_inactive_target
operator|=
name|vm_cnt
operator|.
name|v_free_count
operator|/
literal|3
expr_stmt|;
comment|/* 	 * Set the default wakeup threshold to be 10% above the minimum 	 * page limit.  This keeps the steady state out of shortfall. 	 */
name|vm_pageout_wakeup_thresh
operator|=
operator|(
name|vm_cnt
operator|.
name|v_free_min
operator|/
literal|10
operator|)
operator|*
literal|11
expr_stmt|;
comment|/* 	 * Set interval in seconds for active scan.  We want to visit each 	 * page at least once every ten minutes.  This is to prevent worst 	 * case paging behaviors with stale active LRU. 	 */
if|if
condition|(
name|vm_pageout_update_period
operator|==
literal|0
condition|)
name|vm_pageout_update_period
operator|=
literal|600
expr_stmt|;
comment|/* XXX does not really belong here */
if|if
condition|(
name|vm_page_max_wired
operator|==
literal|0
condition|)
name|vm_page_max_wired
operator|=
name|vm_cnt
operator|.
name|v_free_count
operator|/
literal|3
expr_stmt|;
comment|/* 	 * Target amount of memory to move out of the laundry queue during a 	 * background laundering.  This is proportional to the amount of system 	 * memory. 	 */
name|vm_background_launder_target
operator|=
operator|(
name|vm_cnt
operator|.
name|v_free_target
operator|-
name|vm_cnt
operator|.
name|v_free_min
operator|)
operator|/
literal|10
expr_stmt|;
block|}
end_function

begin_comment
comment|/*  *     vm_pageout is the high level pageout daemon.  */
end_comment

begin_function
specifier|static
name|void
name|vm_pageout
parameter_list|(
name|void
parameter_list|)
block|{
name|int
name|error
decl_stmt|;
ifdef|#
directive|ifdef
name|VM_NUMA_ALLOC
name|int
name|i
decl_stmt|;
endif|#
directive|endif
name|swap_pager_swap_init
argument_list|()
expr_stmt|;
name|error
operator|=
name|kthread_add
argument_list|(
name|vm_pageout_laundry_worker
argument_list|,
name|NULL
argument_list|,
name|curproc
argument_list|,
name|NULL
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|,
literal|"laundry: dom0"
argument_list|)
expr_stmt|;
if|if
condition|(
name|error
operator|!=
literal|0
condition|)
name|panic
argument_list|(
literal|"starting laundry for domain 0, error %d"
argument_list|,
name|error
argument_list|)
expr_stmt|;
ifdef|#
directive|ifdef
name|VM_NUMA_ALLOC
for|for
control|(
name|i
operator|=
literal|1
init|;
name|i
operator|<
name|vm_ndomains
condition|;
name|i
operator|++
control|)
block|{
name|error
operator|=
name|kthread_add
argument_list|(
name|vm_pageout_worker
argument_list|,
operator|(
name|void
operator|*
operator|)
operator|(
name|uintptr_t
operator|)
name|i
argument_list|,
name|curproc
argument_list|,
name|NULL
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|,
literal|"dom%d"
argument_list|,
name|i
argument_list|)
expr_stmt|;
if|if
condition|(
name|error
operator|!=
literal|0
condition|)
block|{
name|panic
argument_list|(
literal|"starting pageout for domain %d, error %d\n"
argument_list|,
name|i
argument_list|,
name|error
argument_list|)
expr_stmt|;
block|}
block|}
endif|#
directive|endif
name|error
operator|=
name|kthread_add
argument_list|(
name|uma_reclaim_worker
argument_list|,
name|NULL
argument_list|,
name|curproc
argument_list|,
name|NULL
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|,
literal|"uma"
argument_list|)
expr_stmt|;
if|if
condition|(
name|error
operator|!=
literal|0
condition|)
name|panic
argument_list|(
literal|"starting uma_reclaim helper, error %d\n"
argument_list|,
name|error
argument_list|)
expr_stmt|;
name|vm_pageout_worker
argument_list|(
operator|(
name|void
operator|*
operator|)
operator|(
name|uintptr_t
operator|)
literal|0
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|/*  * Unless the free page queue lock is held by the caller, this function  * should be regarded as advisory.  Specifically, the caller should  * not msleep() on&vm_cnt.v_free_count following this function unless  * the free page queue lock is held until the msleep() is performed.  */
end_comment

begin_function
name|void
name|pagedaemon_wakeup
parameter_list|(
name|void
parameter_list|)
block|{
if|if
condition|(
operator|!
name|vm_pageout_wanted
operator|&&
name|curthread
operator|->
name|td_proc
operator|!=
name|pageproc
condition|)
block|{
name|vm_pageout_wanted
operator|=
name|true
expr_stmt|;
name|wakeup
argument_list|(
operator|&
name|vm_pageout_wanted
argument_list|)
expr_stmt|;
block|}
block|}
end_function

begin_if
if|#
directive|if
operator|!
name|defined
argument_list|(
name|NO_SWAPPING
argument_list|)
end_if

begin_function
specifier|static
name|void
name|vm_req_vmdaemon
parameter_list|(
name|int
name|req
parameter_list|)
block|{
specifier|static
name|int
name|lastrun
init|=
literal|0
decl_stmt|;
name|mtx_lock
argument_list|(
operator|&
name|vm_daemon_mtx
argument_list|)
expr_stmt|;
name|vm_pageout_req_swapout
operator||=
name|req
expr_stmt|;
if|if
condition|(
operator|(
name|ticks
operator|>
operator|(
name|lastrun
operator|+
name|hz
operator|)
operator|)
operator|||
operator|(
name|ticks
operator|<
name|lastrun
operator|)
condition|)
block|{
name|wakeup
argument_list|(
operator|&
name|vm_daemon_needed
argument_list|)
expr_stmt|;
name|lastrun
operator|=
name|ticks
expr_stmt|;
block|}
name|mtx_unlock
argument_list|(
operator|&
name|vm_daemon_mtx
argument_list|)
expr_stmt|;
block|}
end_function

begin_function
specifier|static
name|void
name|vm_daemon
parameter_list|(
name|void
parameter_list|)
block|{
name|struct
name|rlimit
name|rsslim
decl_stmt|;
name|struct
name|proc
modifier|*
name|p
decl_stmt|;
name|struct
name|thread
modifier|*
name|td
decl_stmt|;
name|struct
name|vmspace
modifier|*
name|vm
decl_stmt|;
name|int
name|breakout
decl_stmt|,
name|swapout_flags
decl_stmt|,
name|tryagain
decl_stmt|,
name|attempts
decl_stmt|;
ifdef|#
directive|ifdef
name|RACCT
name|uint64_t
name|rsize
decl_stmt|,
name|ravailable
decl_stmt|;
endif|#
directive|endif
while|while
condition|(
name|TRUE
condition|)
block|{
name|mtx_lock
argument_list|(
operator|&
name|vm_daemon_mtx
argument_list|)
expr_stmt|;
name|msleep
argument_list|(
operator|&
name|vm_daemon_needed
argument_list|,
operator|&
name|vm_daemon_mtx
argument_list|,
name|PPAUSE
argument_list|,
literal|"psleep"
argument_list|,
ifdef|#
directive|ifdef
name|RACCT
name|racct_enable
condition|?
name|hz
else|:
literal|0
else|#
directive|else
literal|0
endif|#
directive|endif
argument_list|)
expr_stmt|;
name|swapout_flags
operator|=
name|vm_pageout_req_swapout
expr_stmt|;
name|vm_pageout_req_swapout
operator|=
literal|0
expr_stmt|;
name|mtx_unlock
argument_list|(
operator|&
name|vm_daemon_mtx
argument_list|)
expr_stmt|;
if|if
condition|(
name|swapout_flags
condition|)
name|swapout_procs
argument_list|(
name|swapout_flags
argument_list|)
expr_stmt|;
comment|/* 		 * scan the processes for exceeding their rlimits or if 		 * process is swapped out -- deactivate pages 		 */
name|tryagain
operator|=
literal|0
expr_stmt|;
name|attempts
operator|=
literal|0
expr_stmt|;
name|again
label|:
name|attempts
operator|++
expr_stmt|;
name|sx_slock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
name|FOREACH_PROC_IN_SYSTEM
argument_list|(
argument|p
argument_list|)
block|{
name|vm_pindex_t
name|limit
decl_stmt|,
name|size
decl_stmt|;
comment|/* 			 * if this is a system process or if we have already 			 * looked at this process, skip it. 			 */
name|PROC_LOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|p_state
operator|!=
name|PRS_NORMAL
operator|||
name|p
operator|->
name|p_flag
operator|&
operator|(
name|P_INEXEC
operator||
name|P_SYSTEM
operator||
name|P_WEXIT
operator|)
condition|)
block|{
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|/* 			 * if the process is in a non-running type state, 			 * don't touch it. 			 */
name|breakout
operator|=
literal|0
expr_stmt|;
name|FOREACH_THREAD_IN_PROC
argument_list|(
argument|p
argument_list|,
argument|td
argument_list|)
block|{
name|thread_lock
argument_list|(
name|td
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|TD_ON_RUNQ
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_RUNNING
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_SLEEPING
argument_list|(
name|td
argument_list|)
operator|&&
operator|!
name|TD_IS_SUSPENDED
argument_list|(
name|td
argument_list|)
condition|)
block|{
name|thread_unlock
argument_list|(
name|td
argument_list|)
expr_stmt|;
name|breakout
operator|=
literal|1
expr_stmt|;
break|break;
block|}
name|thread_unlock
argument_list|(
name|td
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|breakout
condition|)
block|{
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|/* 			 * get a limit 			 */
name|lim_rlimit_proc
argument_list|(
name|p
argument_list|,
name|RLIMIT_RSS
argument_list|,
operator|&
name|rsslim
argument_list|)
expr_stmt|;
name|limit
operator|=
name|OFF_TO_IDX
argument_list|(
name|qmin
argument_list|(
name|rsslim
operator|.
name|rlim_cur
argument_list|,
name|rsslim
operator|.
name|rlim_max
argument_list|)
argument_list|)
expr_stmt|;
comment|/* 			 * let processes that are swapped out really be 			 * swapped out set the limit to nothing (will force a 			 * swap-out.) 			 */
if|if
condition|(
operator|(
name|p
operator|->
name|p_flag
operator|&
name|P_INMEM
operator|)
operator|==
literal|0
condition|)
name|limit
operator|=
literal|0
expr_stmt|;
comment|/* XXX */
name|vm
operator|=
name|vmspace_acquire_ref
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|_PHOLD_LITE
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|vm
operator|==
name|NULL
condition|)
block|{
name|PRELE
argument_list|(
name|p
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|sx_sunlock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
name|size
operator|=
name|vmspace_resident_count
argument_list|(
name|vm
argument_list|)
expr_stmt|;
if|if
condition|(
name|size
operator|>=
name|limit
condition|)
block|{
name|vm_pageout_map_deactivate_pages
argument_list|(
operator|&
name|vm
operator|->
name|vm_map
argument_list|,
name|limit
argument_list|)
expr_stmt|;
name|size
operator|=
name|vmspace_resident_count
argument_list|(
name|vm
argument_list|)
expr_stmt|;
block|}
ifdef|#
directive|ifdef
name|RACCT
if|if
condition|(
name|racct_enable
condition|)
block|{
name|rsize
operator|=
name|IDX_TO_OFF
argument_list|(
name|size
argument_list|)
expr_stmt|;
name|PROC_LOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|p_state
operator|==
name|PRS_NORMAL
condition|)
name|racct_set
argument_list|(
name|p
argument_list|,
name|RACCT_RSS
argument_list|,
name|rsize
argument_list|)
expr_stmt|;
name|ravailable
operator|=
name|racct_get_available
argument_list|(
name|p
argument_list|,
name|RACCT_RSS
argument_list|)
expr_stmt|;
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|rsize
operator|>
name|ravailable
condition|)
block|{
comment|/* 					 * Don't be overly aggressive; this 					 * might be an innocent process, 					 * and the limit could've been exceeded 					 * by some memory hog.  Don't try 					 * to deactivate more than 1/4th 					 * of process' resident set size. 					 */
if|if
condition|(
name|attempts
operator|<=
literal|8
condition|)
block|{
if|if
condition|(
name|ravailable
operator|<
name|rsize
operator|-
operator|(
name|rsize
operator|/
literal|4
operator|)
condition|)
block|{
name|ravailable
operator|=
name|rsize
operator|-
operator|(
name|rsize
operator|/
literal|4
operator|)
expr_stmt|;
block|}
block|}
name|vm_pageout_map_deactivate_pages
argument_list|(
operator|&
name|vm
operator|->
name|vm_map
argument_list|,
name|OFF_TO_IDX
argument_list|(
name|ravailable
argument_list|)
argument_list|)
expr_stmt|;
comment|/* Update RSS usage after paging out. */
name|size
operator|=
name|vmspace_resident_count
argument_list|(
name|vm
argument_list|)
expr_stmt|;
name|rsize
operator|=
name|IDX_TO_OFF
argument_list|(
name|size
argument_list|)
expr_stmt|;
name|PROC_LOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|p
operator|->
name|p_state
operator|==
name|PRS_NORMAL
condition|)
name|racct_set
argument_list|(
name|p
argument_list|,
name|RACCT_RSS
argument_list|,
name|rsize
argument_list|)
expr_stmt|;
name|PROC_UNLOCK
argument_list|(
name|p
argument_list|)
expr_stmt|;
if|if
condition|(
name|rsize
operator|>
name|ravailable
condition|)
name|tryagain
operator|=
literal|1
expr_stmt|;
block|}
block|}
endif|#
directive|endif
name|vmspace_free
argument_list|(
name|vm
argument_list|)
expr_stmt|;
name|sx_slock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
name|PRELE
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
name|sx_sunlock
argument_list|(
operator|&
name|allproc_lock
argument_list|)
expr_stmt|;
if|if
condition|(
name|tryagain
operator|!=
literal|0
operator|&&
name|attempts
operator|<=
literal|10
condition|)
goto|goto
name|again
goto|;
block|}
block|}
end_function

begin_endif
endif|#
directive|endif
end_endif

begin_comment
comment|/* !defined(NO_SWAPPING) */
end_comment

end_unit

