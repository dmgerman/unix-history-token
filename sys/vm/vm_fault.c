begin_unit|revision:0.9.5;language:C;cregit-version:0.0.1
begin_comment
comment|/*-  * Copyright (c) 1991, 1993  *	The Regents of the University of California.  All rights reserved.  * Copyright (c) 1994 John S. Dyson  * All rights reserved.  * Copyright (c) 1994 David Greenman  * All rights reserved.  *  *  * This code is derived from software contributed to Berkeley by  * The Mach Operating System project at Carnegie-Mellon University.  *  * Redistribution and use in source and binary forms, with or without  * modification, are permitted provided that the following conditions  * are met:  * 1. Redistributions of source code must retain the above copyright  *    notice, this list of conditions and the following disclaimer.  * 2. Redistributions in binary form must reproduce the above copyright  *    notice, this list of conditions and the following disclaimer in the  *    documentation and/or other materials provided with the distribution.  * 3. All advertising materials mentioning features or use of this software  *    must display the following acknowledgement:  *	This product includes software developed by the University of  *	California, Berkeley and its contributors.  * 4. Neither the name of the University nor the names of its contributors  *    may be used to endorse or promote products derived from this software  *    without specific prior written permission.  *  * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND  * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE  * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE  * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS  * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)  * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT  * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF  * SUCH DAMAGE.  *  *	from: @(#)vm_fault.c	8.4 (Berkeley) 1/12/94  *  *  * Copyright (c) 1987, 1990 Carnegie-Mellon University.  * All rights reserved.  *  * Authors: Avadis Tevanian, Jr., Michael Wayne Young  *  * Permission to use, copy, modify and distribute this software and  * its documentation is hereby granted, provided that both the copyright  * notice and this permission notice appear in all copies of the  * software, derivative works or modified versions, and any portions  * thereof, and that both notices appear in supporting documentation.  *  * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"  * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND  * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.  *  * Carnegie Mellon requests users of this software to return to  *  *  Software Distribution Coordinator  or  Software.Distribution@CS.CMU.EDU  *  School of Computer Science  *  Carnegie Mellon University  *  Pittsburgh PA 15213-3890  *  * any improvements or extensions that they make and grant Carnegie the  * rights to redistribute these changes.  */
end_comment

begin_comment
comment|/*  *	Page fault handling module.  */
end_comment

begin_include
include|#
directive|include
file|<sys/cdefs.h>
end_include

begin_expr_stmt
name|__FBSDID
argument_list|(
literal|"$FreeBSD$"
argument_list|)
expr_stmt|;
end_expr_stmt

begin_include
include|#
directive|include
file|"opt_ktrace.h"
end_include

begin_include
include|#
directive|include
file|"opt_vm.h"
end_include

begin_include
include|#
directive|include
file|<sys/param.h>
end_include

begin_include
include|#
directive|include
file|<sys/systm.h>
end_include

begin_include
include|#
directive|include
file|<sys/kernel.h>
end_include

begin_include
include|#
directive|include
file|<sys/lock.h>
end_include

begin_include
include|#
directive|include
file|<sys/proc.h>
end_include

begin_include
include|#
directive|include
file|<sys/resourcevar.h>
end_include

begin_include
include|#
directive|include
file|<sys/rwlock.h>
end_include

begin_include
include|#
directive|include
file|<sys/sysctl.h>
end_include

begin_include
include|#
directive|include
file|<sys/vmmeter.h>
end_include

begin_include
include|#
directive|include
file|<sys/vnode.h>
end_include

begin_ifdef
ifdef|#
directive|ifdef
name|KTRACE
end_ifdef

begin_include
include|#
directive|include
file|<sys/ktrace.h>
end_include

begin_endif
endif|#
directive|endif
end_endif

begin_include
include|#
directive|include
file|<vm/vm.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_param.h>
end_include

begin_include
include|#
directive|include
file|<vm/pmap.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_map.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_object.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_page.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_pageout.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_kern.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_pager.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_extern.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_reserv.h>
end_include

begin_define
define|#
directive|define
name|PFBAK
value|4
end_define

begin_define
define|#
directive|define
name|PFFOR
value|4
end_define

begin_function_decl
specifier|static
name|int
name|vm_fault_additional_pages
parameter_list|(
name|vm_page_t
parameter_list|,
name|int
parameter_list|,
name|int
parameter_list|,
name|vm_page_t
modifier|*
parameter_list|,
name|int
modifier|*
parameter_list|)
function_decl|;
end_function_decl

begin_define
define|#
directive|define
name|VM_FAULT_READ_BEHIND
value|8
end_define

begin_define
define|#
directive|define
name|VM_FAULT_READ_MAX
value|(1 + VM_FAULT_READ_AHEAD_MAX)
end_define

begin_define
define|#
directive|define
name|VM_FAULT_NINCR
value|(VM_FAULT_READ_MAX / VM_FAULT_READ_BEHIND)
end_define

begin_define
define|#
directive|define
name|VM_FAULT_SUM
value|(VM_FAULT_NINCR * (VM_FAULT_NINCR + 1) / 2)
end_define

begin_define
define|#
directive|define
name|VM_FAULT_CACHE_BEHIND
value|(VM_FAULT_READ_BEHIND * VM_FAULT_SUM)
end_define

begin_struct
struct|struct
name|faultstate
block|{
name|vm_page_t
name|m
decl_stmt|;
name|vm_object_t
name|object
decl_stmt|;
name|vm_pindex_t
name|pindex
decl_stmt|;
name|vm_page_t
name|first_m
decl_stmt|;
name|vm_object_t
name|first_object
decl_stmt|;
name|vm_pindex_t
name|first_pindex
decl_stmt|;
name|vm_map_t
name|map
decl_stmt|;
name|vm_map_entry_t
name|entry
decl_stmt|;
name|int
name|lookup_still_valid
decl_stmt|;
name|int
name|map_generation
decl_stmt|;
name|struct
name|vnode
modifier|*
name|vp
decl_stmt|;
block|}
struct|;
end_struct

begin_function_decl
specifier|static
name|void
name|vm_fault_cache_behind
parameter_list|(
specifier|const
name|struct
name|faultstate
modifier|*
name|fs
parameter_list|,
name|int
name|distance
parameter_list|)
function_decl|;
end_function_decl

begin_function_decl
specifier|static
name|void
name|vm_fault_prefault
parameter_list|(
specifier|const
name|struct
name|faultstate
modifier|*
name|fs
parameter_list|,
name|vm_offset_t
name|addra
parameter_list|,
name|int
name|faultcount
parameter_list|,
name|int
name|reqpage
parameter_list|)
function_decl|;
end_function_decl

begin_function
specifier|static
specifier|inline
name|void
name|release_page
parameter_list|(
name|struct
name|faultstate
modifier|*
name|fs
parameter_list|)
block|{
name|vm_page_xunbusy
argument_list|(
name|fs
operator|->
name|m
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|fs
operator|->
name|m
argument_list|)
expr_stmt|;
name|vm_page_deactivate
argument_list|(
name|fs
operator|->
name|m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|fs
operator|->
name|m
argument_list|)
expr_stmt|;
name|fs
operator|->
name|m
operator|=
name|NULL
expr_stmt|;
block|}
end_function

begin_function
specifier|static
specifier|inline
name|void
name|unlock_map
parameter_list|(
name|struct
name|faultstate
modifier|*
name|fs
parameter_list|)
block|{
if|if
condition|(
name|fs
operator|->
name|lookup_still_valid
condition|)
block|{
name|vm_map_lookup_done
argument_list|(
name|fs
operator|->
name|map
argument_list|,
name|fs
operator|->
name|entry
argument_list|)
expr_stmt|;
name|fs
operator|->
name|lookup_still_valid
operator|=
name|FALSE
expr_stmt|;
block|}
block|}
end_function

begin_function
specifier|static
name|void
name|unlock_vp
parameter_list|(
name|struct
name|faultstate
modifier|*
name|fs
parameter_list|)
block|{
if|if
condition|(
name|fs
operator|->
name|vp
operator|!=
name|NULL
condition|)
block|{
name|vput
argument_list|(
name|fs
operator|->
name|vp
argument_list|)
expr_stmt|;
name|fs
operator|->
name|vp
operator|=
name|NULL
expr_stmt|;
block|}
block|}
end_function

begin_function
specifier|static
name|void
name|unlock_and_deallocate
parameter_list|(
name|struct
name|faultstate
modifier|*
name|fs
parameter_list|)
block|{
name|vm_object_pip_wakeup
argument_list|(
name|fs
operator|->
name|object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|fs
operator|->
name|object
argument_list|)
expr_stmt|;
if|if
condition|(
name|fs
operator|->
name|object
operator|!=
name|fs
operator|->
name|first_object
condition|)
block|{
name|VM_OBJECT_WLOCK
argument_list|(
name|fs
operator|->
name|first_object
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|fs
operator|->
name|first_m
argument_list|)
expr_stmt|;
name|vm_page_free
argument_list|(
name|fs
operator|->
name|first_m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|fs
operator|->
name|first_m
argument_list|)
expr_stmt|;
name|vm_object_pip_wakeup
argument_list|(
name|fs
operator|->
name|first_object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|fs
operator|->
name|first_object
argument_list|)
expr_stmt|;
name|fs
operator|->
name|first_m
operator|=
name|NULL
expr_stmt|;
block|}
name|vm_object_deallocate
argument_list|(
name|fs
operator|->
name|first_object
argument_list|)
expr_stmt|;
name|unlock_map
argument_list|(
name|fs
argument_list|)
expr_stmt|;
name|unlock_vp
argument_list|(
name|fs
argument_list|)
expr_stmt|;
block|}
end_function

begin_function
specifier|static
name|void
name|vm_fault_dirty
parameter_list|(
name|vm_map_entry_t
name|entry
parameter_list|,
name|vm_page_t
name|m
parameter_list|,
name|vm_prot_t
name|prot
parameter_list|,
name|vm_prot_t
name|fault_type
parameter_list|,
name|int
name|fault_flags
parameter_list|,
name|bool
name|set_wd
parameter_list|)
block|{
name|bool
name|need_dirty
decl_stmt|;
if|if
condition|(
operator|(
operator|(
name|prot
operator|&
name|VM_PROT_WRITE
operator|)
operator|==
literal|0
operator|&&
operator|(
name|fault_flags
operator|&
name|VM_FAULT_DIRTY
operator|)
operator|==
literal|0
operator|)
operator|||
operator|(
name|m
operator|->
name|oflags
operator|&
name|VPO_UNMANAGED
operator|)
operator|!=
literal|0
condition|)
return|return;
name|VM_OBJECT_ASSERT_LOCKED
argument_list|(
name|m
operator|->
name|object
argument_list|)
expr_stmt|;
name|need_dirty
operator|=
operator|(
operator|(
name|fault_type
operator|&
name|VM_PROT_WRITE
operator|)
operator|!=
literal|0
operator|&&
operator|(
name|fault_flags
operator|&
name|VM_FAULT_WIRE
operator|)
operator|==
literal|0
operator|)
operator|||
operator|(
name|fault_flags
operator|&
name|VM_FAULT_DIRTY
operator|)
operator|!=
literal|0
expr_stmt|;
if|if
condition|(
name|set_wd
condition|)
name|vm_object_set_writeable_dirty
argument_list|(
name|m
operator|->
name|object
argument_list|)
expr_stmt|;
else|else
comment|/* 		 * If two callers of vm_fault_dirty() with set_wd == 		 * FALSE, one for the map entry with MAP_ENTRY_NOSYNC 		 * flag set, other with flag clear, race, it is 		 * possible for the no-NOSYNC thread to see m->dirty 		 * != 0 and not clear VPO_NOSYNC.  Take vm_page lock 		 * around manipulation of VPO_NOSYNC and 		 * vm_page_dirty() call, to avoid the race and keep 		 * m->oflags consistent. 		 */
name|vm_page_lock
argument_list|(
name|m
argument_list|)
expr_stmt|;
comment|/* 	 * If this is a NOSYNC mmap we do not want to set VPO_NOSYNC 	 * if the page is already dirty to prevent data written with 	 * the expectation of being synced from not being synced. 	 * Likewise if this entry does not request NOSYNC then make 	 * sure the page isn't marked NOSYNC.  Applications sharing 	 * data should use the same flags to avoid ping ponging. 	 */
if|if
condition|(
operator|(
name|entry
operator|->
name|eflags
operator|&
name|MAP_ENTRY_NOSYNC
operator|)
operator|!=
literal|0
condition|)
block|{
if|if
condition|(
name|m
operator|->
name|dirty
operator|==
literal|0
condition|)
block|{
name|m
operator|->
name|oflags
operator||=
name|VPO_NOSYNC
expr_stmt|;
block|}
block|}
else|else
block|{
name|m
operator|->
name|oflags
operator|&=
operator|~
name|VPO_NOSYNC
expr_stmt|;
block|}
comment|/* 	 * If the fault is a write, we know that this page is being 	 * written NOW so dirty it explicitly to save on 	 * pmap_is_modified() calls later. 	 * 	 * Also tell the backing pager, if any, that it should remove 	 * any swap backing since the page is now dirty. 	 */
if|if
condition|(
name|need_dirty
condition|)
name|vm_page_dirty
argument_list|(
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|set_wd
condition|)
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|need_dirty
condition|)
name|vm_pager_page_unswapped
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
end_function

begin_function
specifier|static
name|void
name|vm_fault_fill_hold
parameter_list|(
name|vm_page_t
modifier|*
name|m_hold
parameter_list|,
name|vm_page_t
name|m
parameter_list|)
block|{
if|if
condition|(
name|m_hold
operator|!=
name|NULL
condition|)
block|{
operator|*
name|m_hold
operator|=
name|m
expr_stmt|;
name|vm_page_lock
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_page_hold
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
block|}
end_function

begin_comment
comment|/*  * Unlocks fs.first_object and fs.map on success.  */
end_comment

begin_function
specifier|static
name|int
name|vm_fault_soft_fast
parameter_list|(
name|struct
name|faultstate
modifier|*
name|fs
parameter_list|,
name|vm_offset_t
name|vaddr
parameter_list|,
name|vm_prot_t
name|prot
parameter_list|,
name|int
name|fault_type
parameter_list|,
name|int
name|fault_flags
parameter_list|,
name|boolean_t
name|wired
parameter_list|,
name|vm_page_t
modifier|*
name|m_hold
parameter_list|)
block|{
name|vm_page_t
name|m
decl_stmt|;
name|int
name|rv
decl_stmt|;
name|MPASS
argument_list|(
name|fs
operator|->
name|vp
operator|==
name|NULL
argument_list|)
expr_stmt|;
name|m
operator|=
name|vm_page_lookup
argument_list|(
name|fs
operator|->
name|first_object
argument_list|,
name|fs
operator|->
name|first_pindex
argument_list|)
expr_stmt|;
comment|/* A busy page can be mapped for read|execute access. */
if|if
condition|(
name|m
operator|==
name|NULL
operator|||
operator|(
operator|(
name|prot
operator|&
name|VM_PROT_WRITE
operator|)
operator|!=
literal|0
operator|&&
name|vm_page_busied
argument_list|(
name|m
argument_list|)
operator|)
operator|||
name|m
operator|->
name|valid
operator|!=
name|VM_PAGE_BITS_ALL
condition|)
return|return
operator|(
name|KERN_FAILURE
operator|)
return|;
name|rv
operator|=
name|pmap_enter
argument_list|(
name|fs
operator|->
name|map
operator|->
name|pmap
argument_list|,
name|vaddr
argument_list|,
name|m
argument_list|,
name|prot
argument_list|,
name|fault_type
operator||
name|PMAP_ENTER_NOSLEEP
operator||
operator|(
name|wired
condition|?
name|PMAP_ENTER_WIRED
else|:
literal|0
operator|)
argument_list|,
literal|0
argument_list|)
expr_stmt|;
if|if
condition|(
name|rv
operator|!=
name|KERN_SUCCESS
condition|)
return|return
operator|(
name|rv
operator|)
return|;
name|vm_fault_fill_hold
argument_list|(
name|m_hold
argument_list|,
name|m
argument_list|)
expr_stmt|;
name|vm_fault_dirty
argument_list|(
name|fs
operator|->
name|entry
argument_list|,
name|m
argument_list|,
name|prot
argument_list|,
name|fault_type
argument_list|,
name|fault_flags
argument_list|,
name|false
argument_list|)
expr_stmt|;
name|VM_OBJECT_RUNLOCK
argument_list|(
name|fs
operator|->
name|first_object
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|wired
condition|)
name|vm_fault_prefault
argument_list|(
name|fs
argument_list|,
name|vaddr
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|)
expr_stmt|;
name|vm_map_lookup_done
argument_list|(
name|fs
operator|->
name|map
argument_list|,
name|fs
operator|->
name|entry
argument_list|)
expr_stmt|;
name|curthread
operator|->
name|td_ru
operator|.
name|ru_minflt
operator|++
expr_stmt|;
return|return
operator|(
name|KERN_SUCCESS
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  *	vm_fault:  *  *	Handle a page fault occurring at the given address,  *	requiring the given permissions, in the map specified.  *	If successful, the page is inserted into the  *	associated physical map.  *  *	NOTE: the given address should be truncated to the  *	proper page address.  *  *	KERN_SUCCESS is returned if the page fault is handled; otherwise,  *	a standard error specifying why the fault is fatal is returned.  *  *	The map in question must be referenced, and remains so.  *	Caller may hold no locks.  */
end_comment

begin_function
name|int
name|vm_fault
parameter_list|(
name|vm_map_t
name|map
parameter_list|,
name|vm_offset_t
name|vaddr
parameter_list|,
name|vm_prot_t
name|fault_type
parameter_list|,
name|int
name|fault_flags
parameter_list|)
block|{
name|struct
name|thread
modifier|*
name|td
decl_stmt|;
name|int
name|result
decl_stmt|;
name|td
operator|=
name|curthread
expr_stmt|;
if|if
condition|(
operator|(
name|td
operator|->
name|td_pflags
operator|&
name|TDP_NOFAULTING
operator|)
operator|!=
literal|0
condition|)
return|return
operator|(
name|KERN_PROTECTION_FAILURE
operator|)
return|;
ifdef|#
directive|ifdef
name|KTRACE
if|if
condition|(
name|map
operator|!=
name|kernel_map
operator|&&
name|KTRPOINT
argument_list|(
name|td
argument_list|,
name|KTR_FAULT
argument_list|)
condition|)
name|ktrfault
argument_list|(
name|vaddr
argument_list|,
name|fault_type
argument_list|)
expr_stmt|;
endif|#
directive|endif
name|result
operator|=
name|vm_fault_hold
argument_list|(
name|map
argument_list|,
name|trunc_page
argument_list|(
name|vaddr
argument_list|)
argument_list|,
name|fault_type
argument_list|,
name|fault_flags
argument_list|,
name|NULL
argument_list|)
expr_stmt|;
ifdef|#
directive|ifdef
name|KTRACE
if|if
condition|(
name|map
operator|!=
name|kernel_map
operator|&&
name|KTRPOINT
argument_list|(
name|td
argument_list|,
name|KTR_FAULTEND
argument_list|)
condition|)
name|ktrfaultend
argument_list|(
name|result
argument_list|)
expr_stmt|;
endif|#
directive|endif
return|return
operator|(
name|result
operator|)
return|;
block|}
end_function

begin_function
name|int
name|vm_fault_hold
parameter_list|(
name|vm_map_t
name|map
parameter_list|,
name|vm_offset_t
name|vaddr
parameter_list|,
name|vm_prot_t
name|fault_type
parameter_list|,
name|int
name|fault_flags
parameter_list|,
name|vm_page_t
modifier|*
name|m_hold
parameter_list|)
block|{
name|vm_prot_t
name|prot
decl_stmt|;
name|long
name|ahead
decl_stmt|,
name|behind
decl_stmt|;
name|int
name|alloc_req
decl_stmt|,
name|era
decl_stmt|,
name|faultcount
decl_stmt|,
name|nera
decl_stmt|,
name|reqpage
decl_stmt|,
name|result
decl_stmt|;
name|boolean_t
name|dead
decl_stmt|,
name|is_first_object_locked
decl_stmt|,
name|wired
decl_stmt|;
name|vm_object_t
name|next_object
decl_stmt|;
name|vm_page_t
name|marray
index|[
name|VM_FAULT_READ_MAX
index|]
decl_stmt|;
name|int
name|hardfault
decl_stmt|;
name|struct
name|faultstate
name|fs
decl_stmt|;
name|struct
name|vnode
modifier|*
name|vp
decl_stmt|;
name|int
name|locked
decl_stmt|,
name|error
decl_stmt|;
name|hardfault
operator|=
literal|0
expr_stmt|;
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_vm_faults
argument_list|)
expr_stmt|;
name|fs
operator|.
name|vp
operator|=
name|NULL
expr_stmt|;
name|faultcount
operator|=
name|reqpage
operator|=
literal|0
expr_stmt|;
name|RetryFault
label|:
empty_stmt|;
comment|/* 	 * Find the backing store object and offset into it to begin the 	 * search. 	 */
name|fs
operator|.
name|map
operator|=
name|map
expr_stmt|;
name|result
operator|=
name|vm_map_lookup
argument_list|(
operator|&
name|fs
operator|.
name|map
argument_list|,
name|vaddr
argument_list|,
name|fault_type
operator||
name|VM_PROT_FAULT_LOOKUP
argument_list|,
operator|&
name|fs
operator|.
name|entry
argument_list|,
operator|&
name|fs
operator|.
name|first_object
argument_list|,
operator|&
name|fs
operator|.
name|first_pindex
argument_list|,
operator|&
name|prot
argument_list|,
operator|&
name|wired
argument_list|)
expr_stmt|;
if|if
condition|(
name|result
operator|!=
name|KERN_SUCCESS
condition|)
block|{
name|unlock_vp
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
return|return
operator|(
name|result
operator|)
return|;
block|}
name|fs
operator|.
name|map_generation
operator|=
name|fs
operator|.
name|map
operator|->
name|timestamp
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|entry
operator|->
name|eflags
operator|&
name|MAP_ENTRY_NOFAULT
condition|)
block|{
name|panic
argument_list|(
literal|"vm_fault: fault on nofault entry, addr: %lx"
argument_list|,
operator|(
name|u_long
operator|)
name|vaddr
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|fs
operator|.
name|entry
operator|->
name|eflags
operator|&
name|MAP_ENTRY_IN_TRANSITION
operator|&&
name|fs
operator|.
name|entry
operator|->
name|wiring_thread
operator|!=
name|curthread
condition|)
block|{
name|vm_map_unlock_read
argument_list|(
name|fs
operator|.
name|map
argument_list|)
expr_stmt|;
name|vm_map_lock
argument_list|(
name|fs
operator|.
name|map
argument_list|)
expr_stmt|;
if|if
condition|(
name|vm_map_lookup_entry
argument_list|(
name|fs
operator|.
name|map
argument_list|,
name|vaddr
argument_list|,
operator|&
name|fs
operator|.
name|entry
argument_list|)
operator|&&
operator|(
name|fs
operator|.
name|entry
operator|->
name|eflags
operator|&
name|MAP_ENTRY_IN_TRANSITION
operator|)
condition|)
block|{
name|unlock_vp
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
name|fs
operator|.
name|entry
operator|->
name|eflags
operator||=
name|MAP_ENTRY_NEEDS_WAKEUP
expr_stmt|;
name|vm_map_unlock_and_wait
argument_list|(
name|fs
operator|.
name|map
argument_list|,
literal|0
argument_list|)
expr_stmt|;
block|}
else|else
name|vm_map_unlock
argument_list|(
name|fs
operator|.
name|map
argument_list|)
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
name|MPASS
argument_list|(
operator|(
name|fs
operator|.
name|entry
operator|->
name|eflags
operator|&
name|MAP_ENTRY_GUARD
operator|)
operator|==
literal|0
argument_list|)
expr_stmt|;
if|if
condition|(
name|wired
condition|)
name|fault_type
operator|=
name|prot
operator||
operator|(
name|fault_type
operator|&
name|VM_PROT_COPY
operator|)
expr_stmt|;
else|else
name|KASSERT
argument_list|(
operator|(
name|fault_flags
operator|&
name|VM_FAULT_WIRE
operator|)
operator|==
literal|0
argument_list|,
operator|(
literal|"!wired&& VM_FAULT_WIRE"
operator|)
argument_list|)
expr_stmt|;
comment|/* 	 * Try to avoid lock contention on the top-level object through 	 * special-case handling of some types of page faults, specifically, 	 * those that are both (1) mapping an existing page from the top- 	 * level object and (2) not having to mark that object as containing 	 * dirty pages.  Under these conditions, a read lock on the top-level 	 * object suffices, allowing multiple page faults of a similar type to 	 * run in parallel on the same top-level object. 	 */
if|if
condition|(
name|fs
operator|.
name|vp
operator|==
name|NULL
comment|/* avoid locked vnode leak */
operator|&&
operator|(
name|fault_flags
operator|&
operator|(
name|VM_FAULT_WIRE
operator||
name|VM_FAULT_DIRTY
operator|)
operator|)
operator|==
literal|0
operator|&&
comment|/* avoid calling vm_object_set_writeable_dirty() */
operator|(
operator|(
name|prot
operator|&
name|VM_PROT_WRITE
operator|)
operator|==
literal|0
operator|||
operator|(
name|fs
operator|.
name|first_object
operator|->
name|type
operator|!=
name|OBJT_VNODE
operator|&&
operator|(
name|fs
operator|.
name|first_object
operator|->
name|flags
operator|&
name|OBJ_TMPFS_NODE
operator|)
operator|==
literal|0
operator|)
operator|||
operator|(
name|fs
operator|.
name|first_object
operator|->
name|flags
operator|&
name|OBJ_MIGHTBEDIRTY
operator|)
operator|!=
literal|0
operator|)
condition|)
block|{
name|VM_OBJECT_RLOCK
argument_list|(
name|fs
operator|.
name|first_object
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|prot
operator|&
name|VM_PROT_WRITE
operator|)
operator|==
literal|0
operator|||
operator|(
name|fs
operator|.
name|first_object
operator|->
name|type
operator|!=
name|OBJT_VNODE
operator|&&
operator|(
name|fs
operator|.
name|first_object
operator|->
name|flags
operator|&
name|OBJ_TMPFS_NODE
operator|)
operator|==
literal|0
operator|)
operator|||
operator|(
name|fs
operator|.
name|first_object
operator|->
name|flags
operator|&
name|OBJ_MIGHTBEDIRTY
operator|)
operator|!=
literal|0
condition|)
block|{
name|result
operator|=
name|vm_fault_soft_fast
argument_list|(
operator|&
name|fs
argument_list|,
name|vaddr
argument_list|,
name|prot
argument_list|,
name|fault_type
argument_list|,
name|fault_flags
argument_list|,
name|wired
argument_list|,
name|m_hold
argument_list|)
expr_stmt|;
if|if
condition|(
name|result
operator|==
name|KERN_SUCCESS
condition|)
return|return
operator|(
name|result
operator|)
return|;
block|}
if|if
condition|(
operator|!
name|VM_OBJECT_TRYUPGRADE
argument_list|(
name|fs
operator|.
name|first_object
argument_list|)
condition|)
block|{
name|VM_OBJECT_RUNLOCK
argument_list|(
name|fs
operator|.
name|first_object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WLOCK
argument_list|(
name|fs
operator|.
name|first_object
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|VM_OBJECT_WLOCK
argument_list|(
name|fs
operator|.
name|first_object
argument_list|)
expr_stmt|;
block|}
comment|/* 	 * Make a reference to this object to prevent its disposal while we 	 * are messing with it.  Once we have the reference, the map is free 	 * to be diddled.  Since objects reference their shadows (and copies), 	 * they will stay around as well. 	 * 	 * Bump the paging-in-progress count to prevent size changes (e.g.  	 * truncation operations) during I/O.  This must be done after 	 * obtaining the vnode lock in order to avoid possible deadlocks. 	 */
name|vm_object_reference_locked
argument_list|(
name|fs
operator|.
name|first_object
argument_list|)
expr_stmt|;
name|vm_object_pip_add
argument_list|(
name|fs
operator|.
name|first_object
argument_list|,
literal|1
argument_list|)
expr_stmt|;
name|fs
operator|.
name|lookup_still_valid
operator|=
name|TRUE
expr_stmt|;
name|fs
operator|.
name|first_m
operator|=
name|NULL
expr_stmt|;
comment|/* 	 * Search for the page at object/offset. 	 */
name|fs
operator|.
name|object
operator|=
name|fs
operator|.
name|first_object
expr_stmt|;
name|fs
operator|.
name|pindex
operator|=
name|fs
operator|.
name|first_pindex
expr_stmt|;
while|while
condition|(
name|TRUE
condition|)
block|{
comment|/* 		 * If the object is marked for imminent termination, 		 * we retry here, since the collapse pass has raced 		 * with us.  Otherwise, if we see terminally dead 		 * object, return fail. 		 */
if|if
condition|(
operator|(
name|fs
operator|.
name|object
operator|->
name|flags
operator|&
name|OBJ_DEAD
operator|)
operator|!=
literal|0
condition|)
block|{
name|dead
operator|=
name|fs
operator|.
name|object
operator|->
name|type
operator|==
name|OBJT_DEAD
expr_stmt|;
name|unlock_and_deallocate
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
if|if
condition|(
name|dead
condition|)
return|return
operator|(
name|KERN_PROTECTION_FAILURE
operator|)
return|;
name|pause
argument_list|(
literal|"vmf_de"
argument_list|,
literal|1
argument_list|)
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
comment|/* 		 * See if page is resident 		 */
name|fs
operator|.
name|m
operator|=
name|vm_page_lookup
argument_list|(
name|fs
operator|.
name|object
argument_list|,
name|fs
operator|.
name|pindex
argument_list|)
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|m
operator|!=
name|NULL
condition|)
block|{
comment|/* 			 * Wait/Retry if the page is busy.  We have to do this 			 * if the page is either exclusive or shared busy 			 * because the vm_pager may be using read busy for 			 * pageouts (and even pageins if it is the vnode 			 * pager), and we could end up trying to pagein and 			 * pageout the same page simultaneously. 			 * 			 * We can theoretically allow the busy case on a read 			 * fault if the page is marked valid, but since such 			 * pages are typically already pmap'd, putting that 			 * special case in might be more effort then it is  			 * worth.  We cannot under any circumstances mess 			 * around with a shared busied page except, perhaps, 			 * to pmap it. 			 */
if|if
condition|(
name|vm_page_busied
argument_list|(
name|fs
operator|.
name|m
argument_list|)
condition|)
block|{
comment|/* 				 * Reference the page before unlocking and 				 * sleeping so that the page daemon is less 				 * likely to reclaim it.  				 */
name|vm_page_aflag_set
argument_list|(
name|fs
operator|.
name|m
argument_list|,
name|PGA_REFERENCED
argument_list|)
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|object
operator|!=
name|fs
operator|.
name|first_object
condition|)
block|{
if|if
condition|(
operator|!
name|VM_OBJECT_TRYWLOCK
argument_list|(
name|fs
operator|.
name|first_object
argument_list|)
condition|)
block|{
name|VM_OBJECT_WUNLOCK
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WLOCK
argument_list|(
name|fs
operator|.
name|first_object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WLOCK
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
block|}
name|vm_page_lock
argument_list|(
name|fs
operator|.
name|first_m
argument_list|)
expr_stmt|;
name|vm_page_free
argument_list|(
name|fs
operator|.
name|first_m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|fs
operator|.
name|first_m
argument_list|)
expr_stmt|;
name|vm_object_pip_wakeup
argument_list|(
name|fs
operator|.
name|first_object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|fs
operator|.
name|first_object
argument_list|)
expr_stmt|;
name|fs
operator|.
name|first_m
operator|=
name|NULL
expr_stmt|;
block|}
name|unlock_map
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|m
operator|==
name|vm_page_lookup
argument_list|(
name|fs
operator|.
name|object
argument_list|,
name|fs
operator|.
name|pindex
argument_list|)
condition|)
block|{
name|vm_page_sleep_if_busy
argument_list|(
name|fs
operator|.
name|m
argument_list|,
literal|"vmpfw"
argument_list|)
expr_stmt|;
block|}
name|vm_object_pip_wakeup
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_intrans
argument_list|)
expr_stmt|;
name|vm_object_deallocate
argument_list|(
name|fs
operator|.
name|first_object
argument_list|)
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
name|vm_page_lock
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
name|vm_page_remque
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
comment|/* 			 * Mark page busy for other processes, and the  			 * pagedaemon.  If it still isn't completely valid 			 * (readable), jump to readrest, else break-out ( we 			 * found the page ). 			 */
name|vm_page_xbusy
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|m
operator|->
name|valid
operator|!=
name|VM_PAGE_BITS_ALL
condition|)
goto|goto
name|readrest
goto|;
break|break;
block|}
comment|/* 		 * Page is not resident.  If this is the search termination 		 * or the pager might contain the page, allocate a new page. 		 * Default objects are zero-fill, there is no real pager. 		 */
if|if
condition|(
name|fs
operator|.
name|object
operator|->
name|type
operator|!=
name|OBJT_DEFAULT
operator|||
name|fs
operator|.
name|object
operator|==
name|fs
operator|.
name|first_object
condition|)
block|{
if|if
condition|(
name|fs
operator|.
name|pindex
operator|>=
name|fs
operator|.
name|object
operator|->
name|size
condition|)
block|{
name|unlock_and_deallocate
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
return|return
operator|(
name|KERN_PROTECTION_FAILURE
operator|)
return|;
block|}
comment|/* 			 * Allocate a new page for this object/offset pair. 			 * 			 * Unlocked read of the p_flag is harmless. At 			 * worst, the P_KILLED might be not observed 			 * there, and allocation can fail, causing 			 * restart and new reading of the p_flag. 			 */
name|fs
operator|.
name|m
operator|=
name|NULL
expr_stmt|;
if|if
condition|(
operator|!
name|vm_page_count_severe
argument_list|()
operator|||
name|P_KILLED
argument_list|(
name|curproc
argument_list|)
condition|)
block|{
if|#
directive|if
name|VM_NRESERVLEVEL
operator|>
literal|0
if|if
condition|(
operator|(
name|fs
operator|.
name|object
operator|->
name|flags
operator|&
name|OBJ_COLORED
operator|)
operator|==
literal|0
condition|)
block|{
name|fs
operator|.
name|object
operator|->
name|flags
operator||=
name|OBJ_COLORED
expr_stmt|;
name|fs
operator|.
name|object
operator|->
name|pg_color
operator|=
name|atop
argument_list|(
name|vaddr
argument_list|)
operator|-
name|fs
operator|.
name|pindex
expr_stmt|;
block|}
endif|#
directive|endif
name|alloc_req
operator|=
name|P_KILLED
argument_list|(
name|curproc
argument_list|)
condition|?
name|VM_ALLOC_SYSTEM
else|:
name|VM_ALLOC_NORMAL
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|object
operator|->
name|type
operator|!=
name|OBJT_VNODE
operator|&&
name|fs
operator|.
name|object
operator|->
name|backing_object
operator|==
name|NULL
condition|)
name|alloc_req
operator||=
name|VM_ALLOC_ZERO
expr_stmt|;
name|fs
operator|.
name|m
operator|=
name|vm_page_alloc
argument_list|(
name|fs
operator|.
name|object
argument_list|,
name|fs
operator|.
name|pindex
argument_list|,
name|alloc_req
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|fs
operator|.
name|m
operator|==
name|NULL
condition|)
block|{
name|unlock_and_deallocate
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
name|VM_WAITPFAULT
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
elseif|else
if|if
condition|(
name|fs
operator|.
name|m
operator|->
name|valid
operator|==
name|VM_PAGE_BITS_ALL
condition|)
break|break;
block|}
name|readrest
label|:
comment|/* 		 * We have found a valid page or we have allocated a new page. 		 * The page thus may not be valid or may not be entirely  		 * valid. 		 * 		 * Attempt to fault-in the page if there is a chance that the 		 * pager has it, and potentially fault in additional pages 		 * at the same time.  For default objects simply provide 		 * zero-filled pages. 		 */
if|if
condition|(
name|fs
operator|.
name|object
operator|->
name|type
operator|!=
name|OBJT_DEFAULT
condition|)
block|{
name|int
name|rv
decl_stmt|;
name|u_char
name|behavior
init|=
name|vm_map_entry_behavior
argument_list|(
name|fs
operator|.
name|entry
argument_list|)
decl_stmt|;
if|if
condition|(
name|behavior
operator|==
name|MAP_ENTRY_BEHAV_RANDOM
operator|||
name|P_KILLED
argument_list|(
name|curproc
argument_list|)
condition|)
block|{
name|behind
operator|=
literal|0
expr_stmt|;
name|ahead
operator|=
literal|0
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|behavior
operator|==
name|MAP_ENTRY_BEHAV_SEQUENTIAL
condition|)
block|{
name|behind
operator|=
literal|0
expr_stmt|;
name|ahead
operator|=
name|atop
argument_list|(
name|fs
operator|.
name|entry
operator|->
name|end
operator|-
name|vaddr
argument_list|)
operator|-
literal|1
expr_stmt|;
if|if
condition|(
name|ahead
operator|>
name|VM_FAULT_READ_AHEAD_MAX
condition|)
name|ahead
operator|=
name|VM_FAULT_READ_AHEAD_MAX
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|pindex
operator|==
name|fs
operator|.
name|entry
operator|->
name|next_read
condition|)
name|vm_fault_cache_behind
argument_list|(
operator|&
name|fs
argument_list|,
name|VM_FAULT_READ_MAX
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|/* 				 * If this is a sequential page fault, then 				 * arithmetically increase the number of pages 				 * in the read-ahead window.  Otherwise, reset 				 * the read-ahead window to its smallest size. 				 */
name|behind
operator|=
name|atop
argument_list|(
name|vaddr
operator|-
name|fs
operator|.
name|entry
operator|->
name|start
argument_list|)
expr_stmt|;
if|if
condition|(
name|behind
operator|>
name|VM_FAULT_READ_BEHIND
condition|)
name|behind
operator|=
name|VM_FAULT_READ_BEHIND
expr_stmt|;
name|ahead
operator|=
name|atop
argument_list|(
name|fs
operator|.
name|entry
operator|->
name|end
operator|-
name|vaddr
argument_list|)
operator|-
literal|1
expr_stmt|;
name|era
operator|=
name|fs
operator|.
name|entry
operator|->
name|read_ahead
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|pindex
operator|==
name|fs
operator|.
name|entry
operator|->
name|next_read
condition|)
block|{
name|nera
operator|=
name|era
operator|+
name|behind
expr_stmt|;
if|if
condition|(
name|nera
operator|>
name|VM_FAULT_READ_AHEAD_MAX
condition|)
name|nera
operator|=
name|VM_FAULT_READ_AHEAD_MAX
expr_stmt|;
name|behind
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|ahead
operator|>
name|nera
condition|)
name|ahead
operator|=
name|nera
expr_stmt|;
if|if
condition|(
name|era
operator|==
name|VM_FAULT_READ_AHEAD_MAX
condition|)
name|vm_fault_cache_behind
argument_list|(
operator|&
name|fs
argument_list|,
name|VM_FAULT_CACHE_BEHIND
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|ahead
operator|>
name|VM_FAULT_READ_AHEAD_MIN
condition|)
name|ahead
operator|=
name|VM_FAULT_READ_AHEAD_MIN
expr_stmt|;
if|if
condition|(
name|era
operator|!=
name|ahead
condition|)
name|fs
operator|.
name|entry
operator|->
name|read_ahead
operator|=
name|ahead
expr_stmt|;
block|}
comment|/* 			 * Call the pager to retrieve the data, if any, after 			 * releasing the lock on the map.  We hold a ref on 			 * fs.object and the pages are exclusive busied. 			 */
name|unlock_map
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|object
operator|->
name|type
operator|==
name|OBJT_VNODE
operator|&&
operator|(
name|vp
operator|=
name|fs
operator|.
name|object
operator|->
name|handle
operator|)
operator|!=
name|fs
operator|.
name|vp
condition|)
block|{
name|unlock_vp
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
name|locked
operator|=
name|VOP_ISLOCKED
argument_list|(
name|vp
argument_list|)
expr_stmt|;
if|if
condition|(
name|locked
operator|!=
name|LK_EXCLUSIVE
condition|)
name|locked
operator|=
name|LK_SHARED
expr_stmt|;
comment|/* Do not sleep for vnode lock while fs.m is busy */
name|error
operator|=
name|vget
argument_list|(
name|vp
argument_list|,
name|locked
operator||
name|LK_CANRECURSE
operator||
name|LK_NOWAIT
argument_list|,
name|curthread
argument_list|)
expr_stmt|;
if|if
condition|(
name|error
operator|!=
literal|0
condition|)
block|{
name|vhold
argument_list|(
name|vp
argument_list|)
expr_stmt|;
name|release_page
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
name|unlock_and_deallocate
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
name|error
operator|=
name|vget
argument_list|(
name|vp
argument_list|,
name|locked
operator||
name|LK_RETRY
operator||
name|LK_CANRECURSE
argument_list|,
name|curthread
argument_list|)
expr_stmt|;
name|vdrop
argument_list|(
name|vp
argument_list|)
expr_stmt|;
name|fs
operator|.
name|vp
operator|=
name|vp
expr_stmt|;
name|KASSERT
argument_list|(
name|error
operator|==
literal|0
argument_list|,
operator|(
literal|"vm_fault: vget failed"
operator|)
argument_list|)
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
name|fs
operator|.
name|vp
operator|=
name|vp
expr_stmt|;
block|}
name|KASSERT
argument_list|(
name|fs
operator|.
name|vp
operator|==
name|NULL
operator|||
operator|!
name|fs
operator|.
name|map
operator|->
name|system_map
argument_list|,
operator|(
literal|"vm_fault: vnode-backed object mapped by system map"
operator|)
argument_list|)
expr_stmt|;
comment|/* 			 * now we find out if any other pages should be paged 			 * in at this time this routine checks to see if the 			 * pages surrounding this fault reside in the same 			 * object as the page for this fault.  If they do, 			 * then they are faulted in also into the object.  The 			 * array "marray" returned contains an array of 			 * vm_page_t structs where one of them is the 			 * vm_page_t passed to the routine.  The reqpage 			 * return value is the index into the marray for the 			 * vm_page_t passed to the routine. 			 * 			 * fs.m plus the additional pages are exclusive busied. 			 */
name|faultcount
operator|=
name|vm_fault_additional_pages
argument_list|(
name|fs
operator|.
name|m
argument_list|,
name|behind
argument_list|,
name|ahead
argument_list|,
name|marray
argument_list|,
operator|&
name|reqpage
argument_list|)
expr_stmt|;
name|rv
operator|=
name|faultcount
condition|?
name|vm_pager_get_pages
argument_list|(
name|fs
operator|.
name|object
argument_list|,
name|marray
argument_list|,
name|faultcount
argument_list|,
name|reqpage
argument_list|)
else|:
name|VM_PAGER_FAIL
expr_stmt|;
if|if
condition|(
name|rv
operator|==
name|VM_PAGER_OK
condition|)
block|{
comment|/* 				 * Found the page. Leave it busy while we play 				 * with it. 				 */
comment|/* 				 * Relookup in case pager changed page. Pager 				 * is responsible for disposition of old page 				 * if moved. 				 */
name|fs
operator|.
name|m
operator|=
name|vm_page_lookup
argument_list|(
name|fs
operator|.
name|object
argument_list|,
name|fs
operator|.
name|pindex
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|m
condition|)
block|{
name|unlock_and_deallocate
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
name|hardfault
operator|++
expr_stmt|;
break|break;
comment|/* break to PAGE HAS BEEN FOUND */
block|}
comment|/* 			 * Remove the bogus page (which does not exist at this 			 * object/offset); before doing so, we must get back 			 * our object lock to preserve our invariant. 			 * 			 * Also wake up any other process that may want to bring 			 * in this page. 			 * 			 * If this is the top-level object, we must leave the 			 * busy page to prevent another process from rushing 			 * past us, and inserting the page in that object at 			 * the same time that we are. 			 */
if|if
condition|(
name|rv
operator|==
name|VM_PAGER_ERROR
condition|)
name|printf
argument_list|(
literal|"vm_fault: pager read error, pid %d (%s)\n"
argument_list|,
name|curproc
operator|->
name|p_pid
argument_list|,
name|curproc
operator|->
name|p_comm
argument_list|)
expr_stmt|;
comment|/* 			 * Data outside the range of the pager or an I/O error 			 */
comment|/* 			 * XXX - the check for kernel_map is a kludge to work 			 * around having the machine panic on a kernel space 			 * fault w/ I/O error. 			 */
if|if
condition|(
operator|(
operator|(
name|fs
operator|.
name|map
operator|!=
name|kernel_map
operator|)
operator|&&
operator|(
name|rv
operator|==
name|VM_PAGER_ERROR
operator|)
operator|)
operator|||
operator|(
name|rv
operator|==
name|VM_PAGER_BAD
operator|)
condition|)
block|{
name|vm_page_lock
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
name|vm_page_free
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
name|fs
operator|.
name|m
operator|=
name|NULL
expr_stmt|;
name|unlock_and_deallocate
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
return|return
operator|(
operator|(
name|rv
operator|==
name|VM_PAGER_ERROR
operator|)
condition|?
name|KERN_FAILURE
else|:
name|KERN_PROTECTION_FAILURE
operator|)
return|;
block|}
if|if
condition|(
name|fs
operator|.
name|object
operator|!=
name|fs
operator|.
name|first_object
condition|)
block|{
name|vm_page_lock
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
name|vm_page_free
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
name|fs
operator|.
name|m
operator|=
name|NULL
expr_stmt|;
comment|/* 				 * XXX - we cannot just fall out at this 				 * point, m has been freed and is invalid! 				 */
block|}
block|}
comment|/* 		 * We get here if the object has default pager (or unwiring)  		 * or the pager doesn't have the page. 		 */
if|if
condition|(
name|fs
operator|.
name|object
operator|==
name|fs
operator|.
name|first_object
condition|)
name|fs
operator|.
name|first_m
operator|=
name|fs
operator|.
name|m
expr_stmt|;
comment|/* 		 * Move on to the next object.  Lock the next object before 		 * unlocking the current one. 		 */
name|fs
operator|.
name|pindex
operator|+=
name|OFF_TO_IDX
argument_list|(
name|fs
operator|.
name|object
operator|->
name|backing_object_offset
argument_list|)
expr_stmt|;
name|next_object
operator|=
name|fs
operator|.
name|object
operator|->
name|backing_object
expr_stmt|;
if|if
condition|(
name|next_object
operator|==
name|NULL
condition|)
block|{
comment|/* 			 * If there's no object left, fill the page in the top 			 * object with zeros. 			 */
if|if
condition|(
name|fs
operator|.
name|object
operator|!=
name|fs
operator|.
name|first_object
condition|)
block|{
name|vm_object_pip_wakeup
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
name|fs
operator|.
name|object
operator|=
name|fs
operator|.
name|first_object
expr_stmt|;
name|fs
operator|.
name|pindex
operator|=
name|fs
operator|.
name|first_pindex
expr_stmt|;
name|fs
operator|.
name|m
operator|=
name|fs
operator|.
name|first_m
expr_stmt|;
name|VM_OBJECT_WLOCK
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
block|}
name|fs
operator|.
name|first_m
operator|=
name|NULL
expr_stmt|;
comment|/* 			 * Zero the page if necessary and mark it valid. 			 */
if|if
condition|(
operator|(
name|fs
operator|.
name|m
operator|->
name|flags
operator|&
name|PG_ZERO
operator|)
operator|==
literal|0
condition|)
block|{
name|pmap_zero_page
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_ozfod
argument_list|)
expr_stmt|;
block|}
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_zfod
argument_list|)
expr_stmt|;
name|fs
operator|.
name|m
operator|->
name|valid
operator|=
name|VM_PAGE_BITS_ALL
expr_stmt|;
comment|/* Don't try to prefault neighboring pages. */
name|faultcount
operator|=
literal|1
expr_stmt|;
break|break;
comment|/* break to PAGE HAS BEEN FOUND */
block|}
else|else
block|{
name|KASSERT
argument_list|(
name|fs
operator|.
name|object
operator|!=
name|next_object
argument_list|,
operator|(
literal|"object loop %p"
operator|,
name|next_object
operator|)
argument_list|)
expr_stmt|;
name|VM_OBJECT_WLOCK
argument_list|(
name|next_object
argument_list|)
expr_stmt|;
name|vm_object_pip_add
argument_list|(
name|next_object
argument_list|,
literal|1
argument_list|)
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|object
operator|!=
name|fs
operator|.
name|first_object
condition|)
name|vm_object_pip_wakeup
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
name|fs
operator|.
name|object
operator|=
name|next_object
expr_stmt|;
block|}
block|}
name|vm_page_assert_xbusied
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
comment|/* 	 * PAGE HAS BEEN FOUND. [Loop invariant still holds -- the object lock 	 * is held.] 	 */
comment|/* 	 * If the page is being written, but isn't already owned by the 	 * top-level object, we have to copy it into a new page owned by the 	 * top-level object. 	 */
if|if
condition|(
name|fs
operator|.
name|object
operator|!=
name|fs
operator|.
name|first_object
condition|)
block|{
comment|/* 		 * We only really need to copy if we want to write it. 		 */
if|if
condition|(
operator|(
name|fault_type
operator|&
operator|(
name|VM_PROT_COPY
operator||
name|VM_PROT_WRITE
operator|)
operator|)
operator|!=
literal|0
condition|)
block|{
comment|/* 			 * This allows pages to be virtually copied from a  			 * backing_object into the first_object, where the  			 * backing object has no other refs to it, and cannot 			 * gain any more refs.  Instead of a bcopy, we just  			 * move the page from the backing object to the  			 * first object.  Note that we must mark the page  			 * dirty in the first object so that it will go out  			 * to swap when needed. 			 */
name|is_first_object_locked
operator|=
name|FALSE
expr_stmt|;
if|if
condition|(
comment|/* 				 * Only one shadow object 				 */
operator|(
name|fs
operator|.
name|object
operator|->
name|shadow_count
operator|==
literal|1
operator|)
operator|&&
comment|/* 				 * No COW refs, except us 				 */
operator|(
name|fs
operator|.
name|object
operator|->
name|ref_count
operator|==
literal|1
operator|)
operator|&&
comment|/* 				 * No one else can look this object up 				 */
operator|(
name|fs
operator|.
name|object
operator|->
name|handle
operator|==
name|NULL
operator|)
operator|&&
comment|/* 				 * No other ways to look the object up 				 */
operator|(
operator|(
name|fs
operator|.
name|object
operator|->
name|type
operator|==
name|OBJT_DEFAULT
operator|)
operator|||
operator|(
name|fs
operator|.
name|object
operator|->
name|type
operator|==
name|OBJT_SWAP
operator|)
operator|)
operator|&&
operator|(
name|is_first_object_locked
operator|=
name|VM_OBJECT_TRYWLOCK
argument_list|(
name|fs
operator|.
name|first_object
argument_list|)
operator|)
operator|&&
comment|/* 				 * We don't chase down the shadow chain 				 */
name|fs
operator|.
name|object
operator|==
name|fs
operator|.
name|first_object
operator|->
name|backing_object
condition|)
block|{
comment|/* 				 * get rid of the unnecessary page 				 */
name|vm_page_lock
argument_list|(
name|fs
operator|.
name|first_m
argument_list|)
expr_stmt|;
name|vm_page_free
argument_list|(
name|fs
operator|.
name|first_m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|fs
operator|.
name|first_m
argument_list|)
expr_stmt|;
comment|/* 				 * grab the page and put it into the  				 * process'es object.  The page is  				 * automatically made dirty. 				 */
if|if
condition|(
name|vm_page_rename
argument_list|(
name|fs
operator|.
name|m
argument_list|,
name|fs
operator|.
name|first_object
argument_list|,
name|fs
operator|.
name|first_pindex
argument_list|)
condition|)
block|{
name|unlock_and_deallocate
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
if|#
directive|if
name|VM_NRESERVLEVEL
operator|>
literal|0
comment|/* 				 * Rename the reservation. 				 */
name|vm_reserv_rename
argument_list|(
name|fs
operator|.
name|m
argument_list|,
name|fs
operator|.
name|first_object
argument_list|,
name|fs
operator|.
name|object
argument_list|,
name|OFF_TO_IDX
argument_list|(
name|fs
operator|.
name|first_object
operator|->
name|backing_object_offset
argument_list|)
argument_list|)
expr_stmt|;
endif|#
directive|endif
name|vm_page_xbusy
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
name|fs
operator|.
name|first_m
operator|=
name|fs
operator|.
name|m
expr_stmt|;
name|fs
operator|.
name|m
operator|=
name|NULL
expr_stmt|;
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_cow_optim
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|/* 				 * Oh, well, lets copy it. 				 */
name|pmap_copy_page
argument_list|(
name|fs
operator|.
name|m
argument_list|,
name|fs
operator|.
name|first_m
argument_list|)
expr_stmt|;
name|fs
operator|.
name|first_m
operator|->
name|valid
operator|=
name|VM_PAGE_BITS_ALL
expr_stmt|;
if|if
condition|(
name|wired
operator|&&
operator|(
name|fault_flags
operator|&
name|VM_FAULT_WIRE
operator|)
operator|==
literal|0
condition|)
block|{
name|vm_page_lock
argument_list|(
name|fs
operator|.
name|first_m
argument_list|)
expr_stmt|;
name|vm_page_wire
argument_list|(
name|fs
operator|.
name|first_m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|fs
operator|.
name|first_m
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
name|vm_page_unwire
argument_list|(
name|fs
operator|.
name|m
argument_list|,
name|FALSE
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
block|}
comment|/* 				 * We no longer need the old page or object. 				 */
name|release_page
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
block|}
comment|/* 			 * fs.object != fs.first_object due to above  			 * conditional 			 */
name|vm_object_pip_wakeup
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
comment|/* 			 * Only use the new page below... 			 */
name|fs
operator|.
name|object
operator|=
name|fs
operator|.
name|first_object
expr_stmt|;
name|fs
operator|.
name|pindex
operator|=
name|fs
operator|.
name|first_pindex
expr_stmt|;
name|fs
operator|.
name|m
operator|=
name|fs
operator|.
name|first_m
expr_stmt|;
if|if
condition|(
operator|!
name|is_first_object_locked
condition|)
name|VM_OBJECT_WLOCK
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_cow_faults
argument_list|)
expr_stmt|;
name|curthread
operator|->
name|td_cow
operator|++
expr_stmt|;
block|}
else|else
block|{
name|prot
operator|&=
operator|~
name|VM_PROT_WRITE
expr_stmt|;
block|}
block|}
comment|/* 	 * We must verify that the maps have not changed since our last 	 * lookup. 	 */
if|if
condition|(
operator|!
name|fs
operator|.
name|lookup_still_valid
condition|)
block|{
name|vm_object_t
name|retry_object
decl_stmt|;
name|vm_pindex_t
name|retry_pindex
decl_stmt|;
name|vm_prot_t
name|retry_prot
decl_stmt|;
if|if
condition|(
operator|!
name|vm_map_trylock_read
argument_list|(
name|fs
operator|.
name|map
argument_list|)
condition|)
block|{
name|release_page
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
name|unlock_and_deallocate
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
name|fs
operator|.
name|lookup_still_valid
operator|=
name|TRUE
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|map
operator|->
name|timestamp
operator|!=
name|fs
operator|.
name|map_generation
condition|)
block|{
name|result
operator|=
name|vm_map_lookup_locked
argument_list|(
operator|&
name|fs
operator|.
name|map
argument_list|,
name|vaddr
argument_list|,
name|fault_type
argument_list|,
operator|&
name|fs
operator|.
name|entry
argument_list|,
operator|&
name|retry_object
argument_list|,
operator|&
name|retry_pindex
argument_list|,
operator|&
name|retry_prot
argument_list|,
operator|&
name|wired
argument_list|)
expr_stmt|;
comment|/* 			 * If we don't need the page any longer, put it on the inactive 			 * list (the easiest thing to do here).  If no one needs it, 			 * pageout will grab it eventually. 			 */
if|if
condition|(
name|result
operator|!=
name|KERN_SUCCESS
condition|)
block|{
name|release_page
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
name|unlock_and_deallocate
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
comment|/* 				 * If retry of map lookup would have blocked then 				 * retry fault from start. 				 */
if|if
condition|(
name|result
operator|==
name|KERN_FAILURE
condition|)
goto|goto
name|RetryFault
goto|;
return|return
operator|(
name|result
operator|)
return|;
block|}
if|if
condition|(
operator|(
name|retry_object
operator|!=
name|fs
operator|.
name|first_object
operator|)
operator|||
operator|(
name|retry_pindex
operator|!=
name|fs
operator|.
name|first_pindex
operator|)
condition|)
block|{
name|release_page
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
name|unlock_and_deallocate
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
comment|/* 			 * Check whether the protection has changed or the object has 			 * been copied while we left the map unlocked. Changing from 			 * read to write permission is OK - we leave the page 			 * write-protected, and catch the write fault. Changing from 			 * write to read permission means that we can't mark the page 			 * write-enabled after all. 			 */
name|prot
operator|&=
name|retry_prot
expr_stmt|;
block|}
block|}
comment|/* 	 * If the page was filled by a pager, update the map entry's 	 * last read offset.  Since the pager does not return the 	 * actual set of pages that it read, this update is based on 	 * the requested set.  Typically, the requested and actual 	 * sets are the same. 	 * 	 * XXX The following assignment modifies the map 	 * without holding a write lock on it. 	 */
if|if
condition|(
name|hardfault
condition|)
name|fs
operator|.
name|entry
operator|->
name|next_read
operator|=
name|fs
operator|.
name|pindex
operator|+
name|faultcount
operator|-
name|reqpage
expr_stmt|;
name|vm_fault_dirty
argument_list|(
name|fs
operator|.
name|entry
argument_list|,
name|fs
operator|.
name|m
argument_list|,
name|prot
argument_list|,
name|fault_type
argument_list|,
name|fault_flags
argument_list|,
name|true
argument_list|)
expr_stmt|;
name|vm_page_assert_xbusied
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
comment|/* 	 * Page must be completely valid or it is not fit to 	 * map into user space.  vm_pager_get_pages() ensures this. 	 */
name|KASSERT
argument_list|(
name|fs
operator|.
name|m
operator|->
name|valid
operator|==
name|VM_PAGE_BITS_ALL
argument_list|,
operator|(
literal|"vm_fault: page %p partially invalid"
operator|,
name|fs
operator|.
name|m
operator|)
argument_list|)
expr_stmt|;
name|VM_OBJECT_WUNLOCK
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
comment|/* 	 * Put this page into the physical map.  We had to do the unlock above 	 * because pmap_enter() may sleep.  We don't put the page 	 * back on the active queue until later so that the pageout daemon 	 * won't find it (yet). 	 */
name|pmap_enter
argument_list|(
name|fs
operator|.
name|map
operator|->
name|pmap
argument_list|,
name|vaddr
argument_list|,
name|fs
operator|.
name|m
argument_list|,
name|prot
argument_list|,
name|fault_type
operator||
operator|(
name|wired
condition|?
name|PMAP_ENTER_WIRED
else|:
literal|0
operator|)
argument_list|,
literal|0
argument_list|)
expr_stmt|;
if|if
condition|(
name|faultcount
operator|!=
literal|1
operator|&&
operator|(
name|fault_flags
operator|&
name|VM_FAULT_WIRE
operator|)
operator|==
literal|0
operator|&&
name|wired
operator|==
literal|0
condition|)
name|vm_fault_prefault
argument_list|(
operator|&
name|fs
argument_list|,
name|vaddr
argument_list|,
name|faultcount
argument_list|,
name|reqpage
argument_list|)
expr_stmt|;
name|VM_OBJECT_WLOCK
argument_list|(
name|fs
operator|.
name|object
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
comment|/* 	 * If the page is not wired down, then put it where the pageout daemon 	 * can find it. 	 */
if|if
condition|(
operator|(
name|fault_flags
operator|&
name|VM_FAULT_WIRE
operator|)
operator|!=
literal|0
condition|)
block|{
name|KASSERT
argument_list|(
name|wired
argument_list|,
operator|(
literal|"VM_FAULT_WIRE&& !wired"
operator|)
argument_list|)
expr_stmt|;
name|vm_page_wire
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
block|}
else|else
name|vm_page_activate
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|m_hold
operator|!=
name|NULL
condition|)
block|{
operator|*
name|m_hold
operator|=
name|fs
operator|.
name|m
expr_stmt|;
name|vm_page_hold
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
block|}
name|vm_page_unlock
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
name|vm_page_xunbusy
argument_list|(
name|fs
operator|.
name|m
argument_list|)
expr_stmt|;
comment|/* 	 * Unlock everything, and return 	 */
name|unlock_and_deallocate
argument_list|(
operator|&
name|fs
argument_list|)
expr_stmt|;
if|if
condition|(
name|hardfault
condition|)
block|{
name|PCPU_INC
argument_list|(
name|cnt
operator|.
name|v_io_faults
argument_list|)
expr_stmt|;
name|curthread
operator|->
name|td_ru
operator|.
name|ru_majflt
operator|++
expr_stmt|;
block|}
else|else
name|curthread
operator|->
name|td_ru
operator|.
name|ru_minflt
operator|++
expr_stmt|;
return|return
operator|(
name|KERN_SUCCESS
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  * Speed up the reclamation of up to "distance" pages that precede the  * faulting pindex within the first object of the shadow chain.  */
end_comment

begin_function
specifier|static
name|void
name|vm_fault_cache_behind
parameter_list|(
specifier|const
name|struct
name|faultstate
modifier|*
name|fs
parameter_list|,
name|int
name|distance
parameter_list|)
block|{
name|vm_object_t
name|first_object
decl_stmt|,
name|object
decl_stmt|;
name|vm_page_t
name|m
decl_stmt|,
name|m_prev
decl_stmt|;
name|vm_pindex_t
name|pindex
decl_stmt|;
name|object
operator|=
name|fs
operator|->
name|object
expr_stmt|;
name|VM_OBJECT_ASSERT_WLOCKED
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|first_object
operator|=
name|fs
operator|->
name|first_object
expr_stmt|;
if|if
condition|(
name|first_object
operator|!=
name|object
condition|)
block|{
if|if
condition|(
operator|!
name|VM_OBJECT_TRYWLOCK
argument_list|(
name|first_object
argument_list|)
condition|)
block|{
name|VM_OBJECT_WUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WLOCK
argument_list|(
name|first_object
argument_list|)
expr_stmt|;
name|VM_OBJECT_WLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
block|}
block|}
comment|/* Neither fictitious nor unmanaged pages can be cached. */
if|if
condition|(
operator|(
name|first_object
operator|->
name|flags
operator|&
operator|(
name|OBJ_FICTITIOUS
operator||
name|OBJ_UNMANAGED
operator|)
operator|)
operator|==
literal|0
condition|)
block|{
if|if
condition|(
name|fs
operator|->
name|first_pindex
operator|<
name|distance
condition|)
name|pindex
operator|=
literal|0
expr_stmt|;
else|else
name|pindex
operator|=
name|fs
operator|->
name|first_pindex
operator|-
name|distance
expr_stmt|;
if|if
condition|(
name|pindex
operator|<
name|OFF_TO_IDX
argument_list|(
name|fs
operator|->
name|entry
operator|->
name|offset
argument_list|)
condition|)
name|pindex
operator|=
name|OFF_TO_IDX
argument_list|(
name|fs
operator|->
name|entry
operator|->
name|offset
argument_list|)
expr_stmt|;
name|m
operator|=
name|first_object
operator|!=
name|object
condition|?
name|fs
operator|->
name|first_m
else|:
name|fs
operator|->
name|m
expr_stmt|;
name|vm_page_assert_xbusied
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|m_prev
operator|=
name|vm_page_prev
argument_list|(
name|m
argument_list|)
expr_stmt|;
while|while
condition|(
operator|(
name|m
operator|=
name|m_prev
operator|)
operator|!=
name|NULL
operator|&&
name|m
operator|->
name|pindex
operator|>=
name|pindex
operator|&&
name|m
operator|->
name|valid
operator|==
name|VM_PAGE_BITS_ALL
condition|)
block|{
name|m_prev
operator|=
name|vm_page_prev
argument_list|(
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|vm_page_busied
argument_list|(
name|m
argument_list|)
condition|)
continue|continue;
name|vm_page_lock
argument_list|(
name|m
argument_list|)
expr_stmt|;
if|if
condition|(
name|m
operator|->
name|hold_count
operator|==
literal|0
operator|&&
name|m
operator|->
name|wire_count
operator|==
literal|0
condition|)
block|{
name|pmap_remove_all
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_page_aflag_clear
argument_list|(
name|m
argument_list|,
name|PGA_REFERENCED
argument_list|)
expr_stmt|;
if|if
condition|(
name|m
operator|->
name|dirty
operator|!=
literal|0
condition|)
name|vm_page_deactivate
argument_list|(
name|m
argument_list|)
expr_stmt|;
else|else
name|vm_page_cache
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
name|vm_page_unlock
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|first_object
operator|!=
name|object
condition|)
name|VM_OBJECT_WUNLOCK
argument_list|(
name|first_object
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|/*  * vm_fault_prefault provides a quick way of clustering  * pagefaults into a processes address space.  It is a "cousin"  * of vm_map_pmap_enter, except it runs at page fault time instead  * of mmap time.  */
end_comment

begin_function
specifier|static
name|void
name|vm_fault_prefault
parameter_list|(
specifier|const
name|struct
name|faultstate
modifier|*
name|fs
parameter_list|,
name|vm_offset_t
name|addra
parameter_list|,
name|int
name|faultcount
parameter_list|,
name|int
name|reqpage
parameter_list|)
block|{
name|pmap_t
name|pmap
decl_stmt|;
name|vm_map_entry_t
name|entry
decl_stmt|;
name|vm_object_t
name|backing_object
decl_stmt|,
name|lobject
decl_stmt|;
name|vm_offset_t
name|addr
decl_stmt|,
name|starta
decl_stmt|;
name|vm_pindex_t
name|pindex
decl_stmt|;
name|vm_page_t
name|m
decl_stmt|;
name|int
name|backward
decl_stmt|,
name|forward
decl_stmt|,
name|i
decl_stmt|;
name|pmap
operator|=
name|fs
operator|->
name|map
operator|->
name|pmap
expr_stmt|;
if|if
condition|(
name|pmap
operator|!=
name|vmspace_pmap
argument_list|(
name|curthread
operator|->
name|td_proc
operator|->
name|p_vmspace
argument_list|)
condition|)
return|return;
if|if
condition|(
name|faultcount
operator|>
literal|0
condition|)
block|{
name|backward
operator|=
name|reqpage
expr_stmt|;
name|forward
operator|=
name|faultcount
operator|-
name|reqpage
operator|-
literal|1
expr_stmt|;
block|}
else|else
block|{
name|backward
operator|=
name|PFBAK
expr_stmt|;
name|forward
operator|=
name|PFFOR
expr_stmt|;
block|}
name|entry
operator|=
name|fs
operator|->
name|entry
expr_stmt|;
if|if
condition|(
name|addra
operator|<
name|backward
operator|*
name|PAGE_SIZE
condition|)
block|{
name|starta
operator|=
name|entry
operator|->
name|start
expr_stmt|;
block|}
else|else
block|{
name|starta
operator|=
name|addra
operator|-
name|backward
operator|*
name|PAGE_SIZE
expr_stmt|;
if|if
condition|(
name|starta
operator|<
name|entry
operator|->
name|start
condition|)
name|starta
operator|=
name|entry
operator|->
name|start
expr_stmt|;
block|}
comment|/* 	 * Generate the sequence of virtual addresses that are candidates for 	 * prefaulting in an outward spiral from the faulting virtual address, 	 * "addra".  Specifically, the sequence is "addra - PAGE_SIZE", "addra 	 * + PAGE_SIZE", "addra - 2 * PAGE_SIZE", "addra + 2 * PAGE_SIZE", ... 	 * If the candidate address doesn't have a backing physical page, then 	 * the loop immediately terminates. 	 */
for|for
control|(
name|i
operator|=
literal|0
init|;
name|i
operator|<
literal|2
operator|*
name|imax
argument_list|(
name|backward
argument_list|,
name|forward
argument_list|)
condition|;
name|i
operator|++
control|)
block|{
name|addr
operator|=
name|addra
operator|+
operator|(
operator|(
name|i
operator|>>
literal|1
operator|)
operator|+
literal|1
operator|)
operator|*
operator|(
operator|(
name|i
operator|&
literal|1
operator|)
operator|==
literal|0
condition|?
operator|-
name|PAGE_SIZE
else|:
name|PAGE_SIZE
operator|)
expr_stmt|;
if|if
condition|(
name|addr
operator|>
name|addra
operator|+
name|forward
operator|*
name|PAGE_SIZE
condition|)
name|addr
operator|=
literal|0
expr_stmt|;
if|if
condition|(
name|addr
operator|<
name|starta
operator|||
name|addr
operator|>=
name|entry
operator|->
name|end
condition|)
continue|continue;
if|if
condition|(
operator|!
name|pmap_is_prefaultable
argument_list|(
name|pmap
argument_list|,
name|addr
argument_list|)
condition|)
continue|continue;
name|pindex
operator|=
operator|(
operator|(
name|addr
operator|-
name|entry
operator|->
name|start
operator|)
operator|+
name|entry
operator|->
name|offset
operator|)
operator|>>
name|PAGE_SHIFT
expr_stmt|;
name|lobject
operator|=
name|entry
operator|->
name|object
operator|.
name|vm_object
expr_stmt|;
name|VM_OBJECT_RLOCK
argument_list|(
name|lobject
argument_list|)
expr_stmt|;
while|while
condition|(
operator|(
name|m
operator|=
name|vm_page_lookup
argument_list|(
name|lobject
argument_list|,
name|pindex
argument_list|)
operator|)
operator|==
name|NULL
operator|&&
name|lobject
operator|->
name|type
operator|==
name|OBJT_DEFAULT
operator|&&
operator|(
name|backing_object
operator|=
name|lobject
operator|->
name|backing_object
operator|)
operator|!=
name|NULL
condition|)
block|{
name|KASSERT
argument_list|(
operator|(
name|lobject
operator|->
name|backing_object_offset
operator|&
name|PAGE_MASK
operator|)
operator|==
literal|0
argument_list|,
operator|(
literal|"vm_fault_prefault: unaligned object offset"
operator|)
argument_list|)
expr_stmt|;
name|pindex
operator|+=
name|lobject
operator|->
name|backing_object_offset
operator|>>
name|PAGE_SHIFT
expr_stmt|;
name|VM_OBJECT_RLOCK
argument_list|(
name|backing_object
argument_list|)
expr_stmt|;
name|VM_OBJECT_RUNLOCK
argument_list|(
name|lobject
argument_list|)
expr_stmt|;
name|lobject
operator|=
name|backing_object
expr_stmt|;
block|}
if|if
condition|(
name|m
operator|==
name|NULL
condition|)
block|{
name|VM_OBJECT_RUNLOCK
argument_list|(
name|lobject
argument_list|)
expr_stmt|;
break|break;
block|}
if|if
condition|(
name|m
operator|->
name|valid
operator|==
name|VM_PAGE_BITS_ALL
operator|&&
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_FICTITIOUS
operator|)
operator|==
literal|0
condition|)
name|pmap_enter_quick
argument_list|(
name|pmap
argument_list|,
name|addr
argument_list|,
name|m
argument_list|,
name|entry
operator|->
name|protection
argument_list|)
expr_stmt|;
name|VM_OBJECT_RUNLOCK
argument_list|(
name|lobject
argument_list|)
expr_stmt|;
block|}
block|}
end_function

begin_comment
comment|/*  * Hold each of the physical pages that are mapped by the specified range of  * virtual addresses, ["addr", "addr" + "len"), if those mappings are valid  * and allow the specified types of access, "prot".  If all of the implied  * pages are successfully held, then the number of held pages is returned  * together with pointers to those pages in the array "ma".  However, if any  * of the pages cannot be held, -1 is returned.  */
end_comment

begin_function
name|int
name|vm_fault_quick_hold_pages
parameter_list|(
name|vm_map_t
name|map
parameter_list|,
name|vm_offset_t
name|addr
parameter_list|,
name|vm_size_t
name|len
parameter_list|,
name|vm_prot_t
name|prot
parameter_list|,
name|vm_page_t
modifier|*
name|ma
parameter_list|,
name|int
name|max_count
parameter_list|)
block|{
name|vm_offset_t
name|end
decl_stmt|,
name|va
decl_stmt|;
name|vm_page_t
modifier|*
name|mp
decl_stmt|;
name|int
name|count
decl_stmt|;
name|boolean_t
name|pmap_failed
decl_stmt|;
if|if
condition|(
name|len
operator|==
literal|0
condition|)
return|return
operator|(
literal|0
operator|)
return|;
name|end
operator|=
name|round_page
argument_list|(
name|addr
operator|+
name|len
argument_list|)
expr_stmt|;
name|addr
operator|=
name|trunc_page
argument_list|(
name|addr
argument_list|)
expr_stmt|;
comment|/* 	 * Check for illegal addresses. 	 */
if|if
condition|(
name|addr
operator|<
name|vm_map_min
argument_list|(
name|map
argument_list|)
operator|||
name|addr
operator|>
name|end
operator|||
name|end
operator|>
name|vm_map_max
argument_list|(
name|map
argument_list|)
condition|)
return|return
operator|(
operator|-
literal|1
operator|)
return|;
if|if
condition|(
name|atop
argument_list|(
name|end
operator|-
name|addr
argument_list|)
operator|>
name|max_count
condition|)
name|panic
argument_list|(
literal|"vm_fault_quick_hold_pages: count> max_count"
argument_list|)
expr_stmt|;
name|count
operator|=
name|atop
argument_list|(
name|end
operator|-
name|addr
argument_list|)
expr_stmt|;
comment|/* 	 * Most likely, the physical pages are resident in the pmap, so it is 	 * faster to try pmap_extract_and_hold() first. 	 */
name|pmap_failed
operator|=
name|FALSE
expr_stmt|;
for|for
control|(
name|mp
operator|=
name|ma
operator|,
name|va
operator|=
name|addr
init|;
name|va
operator|<
name|end
condition|;
name|mp
operator|++
operator|,
name|va
operator|+=
name|PAGE_SIZE
control|)
block|{
operator|*
name|mp
operator|=
name|pmap_extract_and_hold
argument_list|(
name|map
operator|->
name|pmap
argument_list|,
name|va
argument_list|,
name|prot
argument_list|)
expr_stmt|;
if|if
condition|(
operator|*
name|mp
operator|==
name|NULL
condition|)
name|pmap_failed
operator|=
name|TRUE
expr_stmt|;
elseif|else
if|if
condition|(
operator|(
name|prot
operator|&
name|VM_PROT_WRITE
operator|)
operator|!=
literal|0
operator|&&
operator|(
operator|*
name|mp
operator|)
operator|->
name|dirty
operator|!=
name|VM_PAGE_BITS_ALL
condition|)
block|{
comment|/* 			 * Explicitly dirty the physical page.  Otherwise, the 			 * caller's changes may go unnoticed because they are 			 * performed through an unmanaged mapping or by a DMA 			 * operation. 			 * 			 * The object lock is not held here. 			 * See vm_page_clear_dirty_mask(). 			 */
name|vm_page_dirty
argument_list|(
operator|*
name|mp
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|pmap_failed
condition|)
block|{
comment|/* 		 * One or more pages could not be held by the pmap.  Either no 		 * page was mapped at the specified virtual address or that 		 * mapping had insufficient permissions.  Attempt to fault in 		 * and hold these pages. 		 */
for|for
control|(
name|mp
operator|=
name|ma
operator|,
name|va
operator|=
name|addr
init|;
name|va
operator|<
name|end
condition|;
name|mp
operator|++
operator|,
name|va
operator|+=
name|PAGE_SIZE
control|)
if|if
condition|(
operator|*
name|mp
operator|==
name|NULL
operator|&&
name|vm_fault_hold
argument_list|(
name|map
argument_list|,
name|va
argument_list|,
name|prot
argument_list|,
name|VM_FAULT_NORMAL
argument_list|,
name|mp
argument_list|)
operator|!=
name|KERN_SUCCESS
condition|)
goto|goto
name|error
goto|;
block|}
return|return
operator|(
name|count
operator|)
return|;
name|error
label|:
for|for
control|(
name|mp
operator|=
name|ma
init|;
name|mp
operator|<
name|ma
operator|+
name|count
condition|;
name|mp
operator|++
control|)
if|if
condition|(
operator|*
name|mp
operator|!=
name|NULL
condition|)
block|{
name|vm_page_lock
argument_list|(
operator|*
name|mp
argument_list|)
expr_stmt|;
name|vm_page_unhold
argument_list|(
operator|*
name|mp
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
operator|*
name|mp
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
operator|-
literal|1
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  *	Routine:  *		vm_fault_copy_entry  *	Function:  *		Create new shadow object backing dst_entry with private copy of  *		all underlying pages. When src_entry is equal to dst_entry,  *		function implements COW for wired-down map entry. Otherwise,  *		it forks wired entry into dst_map.  *  *	In/out conditions:  *		The source and destination maps must be locked for write.  *		The source map entry must be wired down (or be a sharing map  *		entry corresponding to a main map entry that is wired down).  */
end_comment

begin_function
name|void
name|vm_fault_copy_entry
parameter_list|(
name|vm_map_t
name|dst_map
parameter_list|,
name|vm_map_t
name|src_map
parameter_list|,
name|vm_map_entry_t
name|dst_entry
parameter_list|,
name|vm_map_entry_t
name|src_entry
parameter_list|,
name|vm_ooffset_t
modifier|*
name|fork_charge
parameter_list|)
block|{
name|vm_object_t
name|backing_object
decl_stmt|,
name|dst_object
decl_stmt|,
name|object
decl_stmt|,
name|src_object
decl_stmt|;
name|vm_pindex_t
name|dst_pindex
decl_stmt|,
name|pindex
decl_stmt|,
name|src_pindex
decl_stmt|;
name|vm_prot_t
name|access
decl_stmt|,
name|prot
decl_stmt|;
name|vm_offset_t
name|vaddr
decl_stmt|;
name|vm_page_t
name|dst_m
decl_stmt|;
name|vm_page_t
name|src_m
decl_stmt|;
name|boolean_t
name|upgrade
decl_stmt|;
ifdef|#
directive|ifdef
name|lint
name|src_map
operator|++
expr_stmt|;
endif|#
directive|endif
comment|/* lint */
name|upgrade
operator|=
name|src_entry
operator|==
name|dst_entry
expr_stmt|;
name|access
operator|=
name|prot
operator|=
name|dst_entry
operator|->
name|protection
expr_stmt|;
name|src_object
operator|=
name|src_entry
operator|->
name|object
operator|.
name|vm_object
expr_stmt|;
name|src_pindex
operator|=
name|OFF_TO_IDX
argument_list|(
name|src_entry
operator|->
name|offset
argument_list|)
expr_stmt|;
if|if
condition|(
name|upgrade
operator|&&
operator|(
name|dst_entry
operator|->
name|eflags
operator|&
name|MAP_ENTRY_NEEDS_COPY
operator|)
operator|==
literal|0
condition|)
block|{
name|dst_object
operator|=
name|src_object
expr_stmt|;
name|vm_object_reference
argument_list|(
name|dst_object
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|/* 		 * Create the top-level object for the destination entry. (Doesn't 		 * actually shadow anything - we copy the pages directly.) 		 */
name|dst_object
operator|=
name|vm_object_allocate
argument_list|(
name|OBJT_DEFAULT
argument_list|,
name|OFF_TO_IDX
argument_list|(
name|dst_entry
operator|->
name|end
operator|-
name|dst_entry
operator|->
name|start
argument_list|)
argument_list|)
expr_stmt|;
if|#
directive|if
name|VM_NRESERVLEVEL
operator|>
literal|0
name|dst_object
operator|->
name|flags
operator||=
name|OBJ_COLORED
expr_stmt|;
name|dst_object
operator|->
name|pg_color
operator|=
name|atop
argument_list|(
name|dst_entry
operator|->
name|start
argument_list|)
expr_stmt|;
endif|#
directive|endif
block|}
name|VM_OBJECT_WLOCK
argument_list|(
name|dst_object
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
name|upgrade
operator|||
name|dst_entry
operator|->
name|object
operator|.
name|vm_object
operator|==
name|NULL
argument_list|,
operator|(
literal|"vm_fault_copy_entry: vm_object not NULL"
operator|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|src_object
operator|!=
name|dst_object
condition|)
block|{
name|dst_entry
operator|->
name|object
operator|.
name|vm_object
operator|=
name|dst_object
expr_stmt|;
name|dst_entry
operator|->
name|offset
operator|=
literal|0
expr_stmt|;
name|dst_object
operator|->
name|charge
operator|=
name|dst_entry
operator|->
name|end
operator|-
name|dst_entry
operator|->
name|start
expr_stmt|;
block|}
if|if
condition|(
name|fork_charge
operator|!=
name|NULL
condition|)
block|{
name|KASSERT
argument_list|(
name|dst_entry
operator|->
name|cred
operator|==
name|NULL
argument_list|,
operator|(
literal|"vm_fault_copy_entry: leaked swp charge"
operator|)
argument_list|)
expr_stmt|;
name|dst_object
operator|->
name|cred
operator|=
name|curthread
operator|->
name|td_ucred
expr_stmt|;
name|crhold
argument_list|(
name|dst_object
operator|->
name|cred
argument_list|)
expr_stmt|;
operator|*
name|fork_charge
operator|+=
name|dst_object
operator|->
name|charge
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|dst_object
operator|->
name|cred
operator|==
name|NULL
condition|)
block|{
name|KASSERT
argument_list|(
name|dst_entry
operator|->
name|cred
operator|!=
name|NULL
argument_list|,
operator|(
literal|"no cred for entry %p"
operator|,
name|dst_entry
operator|)
argument_list|)
expr_stmt|;
name|dst_object
operator|->
name|cred
operator|=
name|dst_entry
operator|->
name|cred
expr_stmt|;
name|dst_entry
operator|->
name|cred
operator|=
name|NULL
expr_stmt|;
block|}
comment|/* 	 * If not an upgrade, then enter the mappings in the pmap as 	 * read and/or execute accesses.  Otherwise, enter them as 	 * write accesses. 	 * 	 * A writeable large page mapping is only created if all of 	 * the constituent small page mappings are modified. Marking 	 * PTEs as modified on inception allows promotion to happen 	 * without taking potentially large number of soft faults. 	 */
if|if
condition|(
operator|!
name|upgrade
condition|)
name|access
operator|&=
operator|~
name|VM_PROT_WRITE
expr_stmt|;
comment|/* 	 * Loop through all of the virtual pages within the entry's 	 * range, copying each page from the source object to the 	 * destination object.  Since the source is wired, those pages 	 * must exist.  In contrast, the destination is pageable. 	 * Since the destination object does share any backing storage 	 * with the source object, all of its pages must be dirtied, 	 * regardless of whether they can be written. 	 */
for|for
control|(
name|vaddr
operator|=
name|dst_entry
operator|->
name|start
operator|,
name|dst_pindex
operator|=
literal|0
init|;
name|vaddr
operator|<
name|dst_entry
operator|->
name|end
condition|;
name|vaddr
operator|+=
name|PAGE_SIZE
operator|,
name|dst_pindex
operator|++
control|)
block|{
name|again
label|:
comment|/* 		 * Find the page in the source object, and copy it in. 		 * Because the source is wired down, the page will be 		 * in memory. 		 */
if|if
condition|(
name|src_object
operator|!=
name|dst_object
condition|)
name|VM_OBJECT_RLOCK
argument_list|(
name|src_object
argument_list|)
expr_stmt|;
name|object
operator|=
name|src_object
expr_stmt|;
name|pindex
operator|=
name|src_pindex
operator|+
name|dst_pindex
expr_stmt|;
while|while
condition|(
operator|(
name|src_m
operator|=
name|vm_page_lookup
argument_list|(
name|object
argument_list|,
name|pindex
argument_list|)
operator|)
operator|==
name|NULL
operator|&&
operator|(
name|backing_object
operator|=
name|object
operator|->
name|backing_object
operator|)
operator|!=
name|NULL
condition|)
block|{
comment|/* 			 * Unless the source mapping is read-only or 			 * it is presently being upgraded from 			 * read-only, the first object in the shadow 			 * chain should provide all of the pages.  In 			 * other words, this loop body should never be 			 * executed when the source mapping is already 			 * read/write. 			 */
name|KASSERT
argument_list|(
operator|(
name|src_entry
operator|->
name|protection
operator|&
name|VM_PROT_WRITE
operator|)
operator|==
literal|0
operator|||
name|upgrade
argument_list|,
operator|(
literal|"vm_fault_copy_entry: main object missing page"
operator|)
argument_list|)
expr_stmt|;
name|VM_OBJECT_RLOCK
argument_list|(
name|backing_object
argument_list|)
expr_stmt|;
name|pindex
operator|+=
name|OFF_TO_IDX
argument_list|(
name|object
operator|->
name|backing_object_offset
argument_list|)
expr_stmt|;
if|if
condition|(
name|object
operator|!=
name|dst_object
condition|)
name|VM_OBJECT_RUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|object
operator|=
name|backing_object
expr_stmt|;
block|}
name|KASSERT
argument_list|(
name|src_m
operator|!=
name|NULL
argument_list|,
operator|(
literal|"vm_fault_copy_entry: page missing"
operator|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|object
operator|!=
name|dst_object
condition|)
block|{
comment|/* 			 * Allocate a page in the destination object. 			 */
name|dst_m
operator|=
name|vm_page_alloc
argument_list|(
name|dst_object
argument_list|,
operator|(
name|src_object
operator|==
name|dst_object
condition|?
name|src_pindex
else|:
literal|0
operator|)
operator|+
name|dst_pindex
argument_list|,
name|VM_ALLOC_NORMAL
argument_list|)
expr_stmt|;
if|if
condition|(
name|dst_m
operator|==
name|NULL
condition|)
block|{
name|VM_OBJECT_WUNLOCK
argument_list|(
name|dst_object
argument_list|)
expr_stmt|;
name|VM_OBJECT_RUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|VM_WAIT
expr_stmt|;
name|VM_OBJECT_WLOCK
argument_list|(
name|dst_object
argument_list|)
expr_stmt|;
goto|goto
name|again
goto|;
block|}
name|pmap_copy_page
argument_list|(
name|src_m
argument_list|,
name|dst_m
argument_list|)
expr_stmt|;
name|VM_OBJECT_RUNLOCK
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|dst_m
operator|->
name|valid
operator|=
name|VM_PAGE_BITS_ALL
expr_stmt|;
name|dst_m
operator|->
name|dirty
operator|=
name|VM_PAGE_BITS_ALL
expr_stmt|;
block|}
else|else
block|{
name|dst_m
operator|=
name|src_m
expr_stmt|;
if|if
condition|(
name|vm_page_sleep_if_busy
argument_list|(
name|dst_m
argument_list|,
literal|"fltupg"
argument_list|)
condition|)
goto|goto
name|again
goto|;
name|vm_page_xbusy
argument_list|(
name|dst_m
argument_list|)
expr_stmt|;
name|KASSERT
argument_list|(
name|dst_m
operator|->
name|valid
operator|==
name|VM_PAGE_BITS_ALL
argument_list|,
operator|(
literal|"invalid dst page %p"
operator|,
name|dst_m
operator|)
argument_list|)
expr_stmt|;
block|}
name|VM_OBJECT_WUNLOCK
argument_list|(
name|dst_object
argument_list|)
expr_stmt|;
comment|/* 		 * Enter it in the pmap. If a wired, copy-on-write 		 * mapping is being replaced by a write-enabled 		 * mapping, then wire that new mapping. 		 */
name|pmap_enter
argument_list|(
name|dst_map
operator|->
name|pmap
argument_list|,
name|vaddr
argument_list|,
name|dst_m
argument_list|,
name|prot
argument_list|,
name|access
operator||
operator|(
name|upgrade
condition|?
name|PMAP_ENTER_WIRED
else|:
literal|0
operator|)
argument_list|,
literal|0
argument_list|)
expr_stmt|;
comment|/* 		 * Mark it no longer busy, and put it on the active list. 		 */
name|VM_OBJECT_WLOCK
argument_list|(
name|dst_object
argument_list|)
expr_stmt|;
if|if
condition|(
name|upgrade
condition|)
block|{
if|if
condition|(
name|src_m
operator|!=
name|dst_m
condition|)
block|{
name|vm_page_lock
argument_list|(
name|src_m
argument_list|)
expr_stmt|;
name|vm_page_unwire
argument_list|(
name|src_m
argument_list|,
literal|0
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|src_m
argument_list|)
expr_stmt|;
name|vm_page_lock
argument_list|(
name|dst_m
argument_list|)
expr_stmt|;
name|vm_page_wire
argument_list|(
name|dst_m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|dst_m
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|KASSERT
argument_list|(
name|dst_m
operator|->
name|wire_count
operator|>
literal|0
argument_list|,
operator|(
literal|"dst_m %p is not wired"
operator|,
name|dst_m
operator|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|vm_page_lock
argument_list|(
name|dst_m
argument_list|)
expr_stmt|;
name|vm_page_activate
argument_list|(
name|dst_m
argument_list|)
expr_stmt|;
name|vm_page_unlock
argument_list|(
name|dst_m
argument_list|)
expr_stmt|;
block|}
name|vm_page_xunbusy
argument_list|(
name|dst_m
argument_list|)
expr_stmt|;
block|}
name|VM_OBJECT_WUNLOCK
argument_list|(
name|dst_object
argument_list|)
expr_stmt|;
if|if
condition|(
name|upgrade
condition|)
block|{
name|dst_entry
operator|->
name|eflags
operator|&=
operator|~
operator|(
name|MAP_ENTRY_COW
operator||
name|MAP_ENTRY_NEEDS_COPY
operator|)
expr_stmt|;
name|vm_object_deallocate
argument_list|(
name|src_object
argument_list|)
expr_stmt|;
block|}
block|}
end_function

begin_comment
comment|/*  * This routine checks around the requested page for other pages that  * might be able to be faulted in.  This routine brackets the viable  * pages for the pages to be paged in.  *  * Inputs:  *	m, rbehind, rahead  *  * Outputs:  *  marray (array of vm_page_t), reqpage (index of requested page)  *  * Return value:  *  number of pages in marray  */
end_comment

begin_function
specifier|static
name|int
name|vm_fault_additional_pages
parameter_list|(
name|m
parameter_list|,
name|rbehind
parameter_list|,
name|rahead
parameter_list|,
name|marray
parameter_list|,
name|reqpage
parameter_list|)
name|vm_page_t
name|m
decl_stmt|;
name|int
name|rbehind
decl_stmt|;
name|int
name|rahead
decl_stmt|;
name|vm_page_t
modifier|*
name|marray
decl_stmt|;
name|int
modifier|*
name|reqpage
decl_stmt|;
block|{
name|int
name|i
decl_stmt|,
name|j
decl_stmt|;
name|vm_object_t
name|object
decl_stmt|;
name|vm_pindex_t
name|pindex
decl_stmt|,
name|startpindex
decl_stmt|,
name|endpindex
decl_stmt|,
name|tpindex
decl_stmt|;
name|vm_page_t
name|rtm
decl_stmt|;
name|int
name|cbehind
decl_stmt|,
name|cahead
decl_stmt|;
name|VM_OBJECT_ASSERT_WLOCKED
argument_list|(
name|m
operator|->
name|object
argument_list|)
expr_stmt|;
name|object
operator|=
name|m
operator|->
name|object
expr_stmt|;
name|pindex
operator|=
name|m
operator|->
name|pindex
expr_stmt|;
name|cbehind
operator|=
name|cahead
operator|=
literal|0
expr_stmt|;
comment|/* 	 * if the requested page is not available, then give up now 	 */
if|if
condition|(
operator|!
name|vm_pager_has_page
argument_list|(
name|object
argument_list|,
name|pindex
argument_list|,
operator|&
name|cbehind
argument_list|,
operator|&
name|cahead
argument_list|)
condition|)
block|{
return|return
literal|0
return|;
block|}
if|if
condition|(
operator|(
name|cbehind
operator|==
literal|0
operator|)
operator|&&
operator|(
name|cahead
operator|==
literal|0
operator|)
condition|)
block|{
operator|*
name|reqpage
operator|=
literal|0
expr_stmt|;
name|marray
index|[
literal|0
index|]
operator|=
name|m
expr_stmt|;
return|return
literal|1
return|;
block|}
if|if
condition|(
name|rahead
operator|>
name|cahead
condition|)
block|{
name|rahead
operator|=
name|cahead
expr_stmt|;
block|}
if|if
condition|(
name|rbehind
operator|>
name|cbehind
condition|)
block|{
name|rbehind
operator|=
name|cbehind
expr_stmt|;
block|}
comment|/* 	 * scan backward for the read behind pages -- in memory  	 */
if|if
condition|(
name|pindex
operator|>
literal|0
condition|)
block|{
if|if
condition|(
name|rbehind
operator|>
name|pindex
condition|)
block|{
name|rbehind
operator|=
name|pindex
expr_stmt|;
name|startpindex
operator|=
literal|0
expr_stmt|;
block|}
else|else
block|{
name|startpindex
operator|=
name|pindex
operator|-
name|rbehind
expr_stmt|;
block|}
if|if
condition|(
operator|(
name|rtm
operator|=
name|TAILQ_PREV
argument_list|(
name|m
argument_list|,
name|pglist
argument_list|,
name|listq
argument_list|)
operator|)
operator|!=
name|NULL
operator|&&
name|rtm
operator|->
name|pindex
operator|>=
name|startpindex
condition|)
name|startpindex
operator|=
name|rtm
operator|->
name|pindex
operator|+
literal|1
expr_stmt|;
comment|/* tpindex is unsigned; beware of numeric underflow. */
for|for
control|(
name|i
operator|=
literal|0
operator|,
name|tpindex
operator|=
name|pindex
operator|-
literal|1
init|;
name|tpindex
operator|>=
name|startpindex
operator|&&
name|tpindex
operator|<
name|pindex
condition|;
name|i
operator|++
operator|,
name|tpindex
operator|--
control|)
block|{
name|rtm
operator|=
name|vm_page_alloc
argument_list|(
name|object
argument_list|,
name|tpindex
argument_list|,
name|VM_ALLOC_NORMAL
operator||
name|VM_ALLOC_IFNOTCACHED
argument_list|)
expr_stmt|;
if|if
condition|(
name|rtm
operator|==
name|NULL
condition|)
block|{
comment|/* 				 * Shift the allocated pages to the 				 * beginning of the array. 				 */
for|for
control|(
name|j
operator|=
literal|0
init|;
name|j
operator|<
name|i
condition|;
name|j
operator|++
control|)
block|{
name|marray
index|[
name|j
index|]
operator|=
name|marray
index|[
name|j
operator|+
name|tpindex
operator|+
literal|1
operator|-
name|startpindex
index|]
expr_stmt|;
block|}
break|break;
block|}
name|marray
index|[
name|tpindex
operator|-
name|startpindex
index|]
operator|=
name|rtm
expr_stmt|;
block|}
block|}
else|else
block|{
name|startpindex
operator|=
literal|0
expr_stmt|;
name|i
operator|=
literal|0
expr_stmt|;
block|}
name|marray
index|[
name|i
index|]
operator|=
name|m
expr_stmt|;
comment|/* page offset of the required page */
operator|*
name|reqpage
operator|=
name|i
expr_stmt|;
name|tpindex
operator|=
name|pindex
operator|+
literal|1
expr_stmt|;
name|i
operator|++
expr_stmt|;
comment|/* 	 * scan forward for the read ahead pages 	 */
name|endpindex
operator|=
name|tpindex
operator|+
name|rahead
expr_stmt|;
if|if
condition|(
operator|(
name|rtm
operator|=
name|TAILQ_NEXT
argument_list|(
name|m
argument_list|,
name|listq
argument_list|)
operator|)
operator|!=
name|NULL
operator|&&
name|rtm
operator|->
name|pindex
operator|<
name|endpindex
condition|)
name|endpindex
operator|=
name|rtm
operator|->
name|pindex
expr_stmt|;
if|if
condition|(
name|endpindex
operator|>
name|object
operator|->
name|size
condition|)
name|endpindex
operator|=
name|object
operator|->
name|size
expr_stmt|;
for|for
control|(
init|;
name|tpindex
operator|<
name|endpindex
condition|;
name|i
operator|++
operator|,
name|tpindex
operator|++
control|)
block|{
name|rtm
operator|=
name|vm_page_alloc
argument_list|(
name|object
argument_list|,
name|tpindex
argument_list|,
name|VM_ALLOC_NORMAL
operator||
name|VM_ALLOC_IFNOTCACHED
argument_list|)
expr_stmt|;
if|if
condition|(
name|rtm
operator|==
name|NULL
condition|)
block|{
break|break;
block|}
name|marray
index|[
name|i
index|]
operator|=
name|rtm
expr_stmt|;
block|}
comment|/* return number of pages */
return|return
name|i
return|;
block|}
end_function

begin_comment
comment|/*  * Block entry into the machine-independent layer's page fault handler by  * the calling thread.  Subsequent calls to vm_fault() by that thread will  * return KERN_PROTECTION_FAILURE.  Enable machine-dependent handling of  * spurious page faults.   */
end_comment

begin_function
name|int
name|vm_fault_disable_pagefaults
parameter_list|(
name|void
parameter_list|)
block|{
return|return
operator|(
name|curthread_pflags_set
argument_list|(
name|TDP_NOFAULTING
operator||
name|TDP_RESETSPUR
argument_list|)
operator|)
return|;
block|}
end_function

begin_function
name|void
name|vm_fault_enable_pagefaults
parameter_list|(
name|int
name|save
parameter_list|)
block|{
name|curthread_pflags_restore
argument_list|(
name|save
argument_list|)
expr_stmt|;
block|}
end_function

end_unit

