begin_unit|revision:0.9.5;language:C;cregit-version:0.0.1
begin_comment
comment|/*  * Copyright (c) 1991, 1993  *	The Regents of the University of California.  All rights reserved.  * Copyright (c) 1994 John S. Dyson  * All rights reserved.  * Copyright (c) 1994 David Greenman  * All rights reserved.  *  *  * This code is derived from software contributed to Berkeley by  * The Mach Operating System project at Carnegie-Mellon University.  *  * Redistribution and use in source and binary forms, with or without  * modification, are permitted provided that the following conditions  * are met:  * 1. Redistributions of source code must retain the above copyright  *    notice, this list of conditions and the following disclaimer.  * 2. Redistributions in binary form must reproduce the above copyright  *    notice, this list of conditions and the following disclaimer in the  *    documentation and/or other materials provided with the distribution.  * 3. All advertising materials mentioning features or use of this software  *    must display the following acknowledgement:  *	This product includes software developed by the University of  *	California, Berkeley and its contributors.  * 4. Neither the name of the University nor the names of its contributors  *    may be used to endorse or promote products derived from this software  *    without specific prior written permission.  *  * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND  * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE  * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE  * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS  * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)  * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT  * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF  * SUCH DAMAGE.  *  *	from: @(#)vm_fault.c	8.4 (Berkeley) 1/12/94  *  *  * Copyright (c) 1987, 1990 Carnegie-Mellon University.  * All rights reserved.  *  * Authors: Avadis Tevanian, Jr., Michael Wayne Young  *  * Permission to use, copy, modify and distribute this software and  * its documentation is hereby granted, provided that both the copyright  * notice and this permission notice appear in all copies of the  * software, derivative works or modified versions, and any portions  * thereof, and that both notices appear in supporting documentation.  *  * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"  * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND  * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.  *  * Carnegie Mellon requests users of this software to return to  *  *  Software Distribution Coordinator  or  Software.Distribution@CS.CMU.EDU  *  School of Computer Science  *  Carnegie Mellon University  *  Pittsburgh PA 15213-3890  *  * any improvements or extensions that they make and grant Carnegie the  * rights to redistribute these changes.  *  * $Id: vm_fault.c,v 1.73 1998/01/06 05:25:54 dyson Exp $  */
end_comment

begin_comment
comment|/*  *	Page fault handling module.  */
end_comment

begin_include
include|#
directive|include
file|<sys/param.h>
end_include

begin_include
include|#
directive|include
file|<sys/systm.h>
end_include

begin_include
include|#
directive|include
file|<sys/proc.h>
end_include

begin_include
include|#
directive|include
file|<sys/vnode.h>
end_include

begin_include
include|#
directive|include
file|<sys/resourcevar.h>
end_include

begin_include
include|#
directive|include
file|<sys/vmmeter.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_param.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_prot.h>
end_include

begin_include
include|#
directive|include
file|<sys/lock.h>
end_include

begin_include
include|#
directive|include
file|<vm/pmap.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_map.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_object.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_page.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_pageout.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_kern.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_pager.h>
end_include

begin_include
include|#
directive|include
file|<vm/vnode_pager.h>
end_include

begin_include
include|#
directive|include
file|<vm/vm_extern.h>
end_include

begin_decl_stmt
name|int
name|vm_fault_additional_pages
name|__P
argument_list|(
operator|(
name|vm_page_t
operator|,
name|int
operator|,
name|int
operator|,
name|vm_page_t
operator|*
operator|,
name|int
operator|*
operator|)
argument_list|)
decl_stmt|;
end_decl_stmt

begin_define
define|#
directive|define
name|VM_FAULT_READ_AHEAD
value|4
end_define

begin_define
define|#
directive|define
name|VM_FAULT_READ_BEHIND
value|3
end_define

begin_define
define|#
directive|define
name|VM_FAULT_READ
value|(VM_FAULT_READ_AHEAD+VM_FAULT_READ_BEHIND+1)
end_define

begin_comment
comment|/*  *	vm_fault:  *  *	Handle a page fault occuring at the given address,  *	requiring the given permissions, in the map specified.  *	If successful, the page is inserted into the  *	associated physical map.  *  *	NOTE: the given address should be truncated to the  *	proper page address.  *  *	KERN_SUCCESS is returned if the page fault is handled; otherwise,  *	a standard error specifying why the fault is fatal is returned.  *  *  *	The map in question must be referenced, and remains so.  *	Caller may hold no locks.  */
end_comment

begin_function
name|int
name|vm_fault
parameter_list|(
name|vm_map_t
name|map
parameter_list|,
name|vm_offset_t
name|vaddr
parameter_list|,
name|vm_prot_t
name|fault_type
parameter_list|,
name|int
name|fault_flags
parameter_list|)
block|{
name|vm_object_t
name|first_object
decl_stmt|;
name|vm_pindex_t
name|first_pindex
decl_stmt|;
name|vm_map_entry_t
name|entry
decl_stmt|;
specifier|register
name|vm_object_t
name|object
decl_stmt|;
specifier|register
name|vm_pindex_t
name|pindex
decl_stmt|;
name|vm_page_t
name|m
decl_stmt|;
name|vm_page_t
name|first_m
decl_stmt|;
name|vm_prot_t
name|prot
decl_stmt|;
name|int
name|result
decl_stmt|;
name|boolean_t
name|wired
decl_stmt|;
name|boolean_t
name|su
decl_stmt|;
name|boolean_t
name|lookup_still_valid
decl_stmt|;
name|vm_page_t
name|old_m
decl_stmt|;
name|vm_object_t
name|next_object
decl_stmt|;
name|vm_page_t
name|marray
index|[
name|VM_FAULT_READ
index|]
decl_stmt|;
name|int
name|hardfault
init|=
literal|0
decl_stmt|;
name|struct
name|vnode
modifier|*
name|vp
init|=
name|NULL
decl_stmt|;
name|struct
name|proc
modifier|*
name|p
init|=
name|curproc
decl_stmt|;
comment|/* XXX */
name|cnt
operator|.
name|v_vm_faults
operator|++
expr_stmt|;
comment|/* needs lock XXX */
comment|/*  *	Recovery actions  */
define|#
directive|define
name|FREE_PAGE
parameter_list|(
name|m
parameter_list|)
value|{				\ 	PAGE_WAKEUP(m);					\ 	vm_page_free(m);				\ }
define|#
directive|define
name|RELEASE_PAGE
parameter_list|(
name|m
parameter_list|)
value|{				\ 	PAGE_WAKEUP(m);					\ 	if (m->queue != PQ_ACTIVE) vm_page_activate(m);		\ }
define|#
directive|define
name|UNLOCK_MAP
value|{				\ 	if (lookup_still_valid) {			\ 		vm_map_lookup_done(map, entry);		\ 		lookup_still_valid = FALSE;		\ 	}						\ }
define|#
directive|define
name|UNLOCK_THINGS
value|{				\ 	vm_object_pip_wakeup(object); \ 	if (object != first_object) {			\ 		FREE_PAGE(first_m);			\ 		vm_object_pip_wakeup(first_object); \ 	}						\ 	UNLOCK_MAP;					\ 	if (vp != NULL) VOP_UNLOCK(vp, 0, p);		\ }
define|#
directive|define
name|UNLOCK_AND_DEALLOCATE
value|{			\ 	UNLOCK_THINGS;					\ 	vm_object_deallocate(first_object);		\ }
name|RetryFault
label|:
empty_stmt|;
comment|/* 	 * Find the backing store object and offset into it to begin the 	 * search. 	 */
if|if
condition|(
operator|(
name|result
operator|=
name|vm_map_lookup
argument_list|(
operator|&
name|map
argument_list|,
name|vaddr
argument_list|,
name|fault_type
argument_list|,
operator|&
name|entry
argument_list|,
operator|&
name|first_object
argument_list|,
operator|&
name|first_pindex
argument_list|,
operator|&
name|prot
argument_list|,
operator|&
name|wired
argument_list|,
operator|&
name|su
argument_list|)
operator|)
operator|!=
name|KERN_SUCCESS
condition|)
block|{
return|return
operator|(
name|result
operator|)
return|;
block|}
if|if
condition|(
name|entry
operator|->
name|eflags
operator|&
name|MAP_ENTRY_NOFAULT
condition|)
block|{
name|panic
argument_list|(
literal|"vm_fault: fault on nofault entry, addr: %lx"
argument_list|,
name|vaddr
argument_list|)
expr_stmt|;
block|}
comment|/* 	 * If we are user-wiring a r/w segment, and it is COW, then 	 * we need to do the COW operation.  Note that we don't COW 	 * currently RO sections now, because it is NOT desirable 	 * to COW .text.  We simply keep .text from ever being COW'ed 	 * and take the heat that one cannot debug wired .text sections. 	 */
if|if
condition|(
operator|(
operator|(
name|fault_flags
operator|&
name|VM_FAULT_WIRE_MASK
operator|)
operator|==
name|VM_FAULT_USER_WIRE
operator|)
operator|&&
operator|(
name|entry
operator|->
name|eflags
operator|&
name|MAP_ENTRY_NEEDS_COPY
operator|)
condition|)
block|{
if|if
condition|(
name|entry
operator|->
name|protection
operator|&
name|VM_PROT_WRITE
condition|)
block|{
name|int
name|tresult
decl_stmt|;
name|vm_map_lookup_done
argument_list|(
name|map
argument_list|,
name|entry
argument_list|)
expr_stmt|;
name|tresult
operator|=
name|vm_map_lookup
argument_list|(
operator|&
name|map
argument_list|,
name|vaddr
argument_list|,
name|VM_PROT_READ
operator||
name|VM_PROT_WRITE
argument_list|,
operator|&
name|entry
argument_list|,
operator|&
name|first_object
argument_list|,
operator|&
name|first_pindex
argument_list|,
operator|&
name|prot
argument_list|,
operator|&
name|wired
argument_list|,
operator|&
name|su
argument_list|)
expr_stmt|;
if|if
condition|(
name|tresult
operator|!=
name|KERN_SUCCESS
condition|)
return|return
name|tresult
return|;
block|}
else|else
block|{
comment|/* 			 * If we don't COW now, on a user wire, the user will never 			 * be able to write to the mapping.  If we don't make this 			 * restriction, the bookkeeping would be nearly impossible. 			 */
name|entry
operator|->
name|max_protection
operator|&=
operator|~
name|VM_PROT_WRITE
expr_stmt|;
block|}
block|}
comment|/* 	 * Make a reference to this object to prevent its disposal while we 	 * are messing with it.  Once we have the reference, the map is free 	 * to be diddled.  Since objects reference their shadows (and copies), 	 * they will stay around as well. 	 */
name|vm_object_reference
argument_list|(
name|first_object
argument_list|)
expr_stmt|;
name|first_object
operator|->
name|paging_in_progress
operator|++
expr_stmt|;
name|vp
operator|=
name|vnode_pager_lock
argument_list|(
name|first_object
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|fault_type
operator|&
name|VM_PROT_WRITE
operator|)
operator|&&
operator|(
name|first_object
operator|->
name|type
operator|==
name|OBJT_VNODE
operator|)
condition|)
block|{
name|vm_freeze_copyopts
argument_list|(
name|first_object
argument_list|,
name|first_pindex
argument_list|,
name|first_pindex
operator|+
literal|1
argument_list|)
expr_stmt|;
block|}
name|lookup_still_valid
operator|=
name|TRUE
expr_stmt|;
if|if
condition|(
name|wired
condition|)
name|fault_type
operator|=
name|prot
expr_stmt|;
name|first_m
operator|=
name|NULL
expr_stmt|;
comment|/* 	 * INVARIANTS (through entire routine): 	 * 	 * 1)	At all times, we must either have the object lock or a busy 	 * page in some object to prevent some other process from trying to 	 * bring in the same page. 	 * 	 * Note that we cannot hold any locks during the pager access or when 	 * waiting for memory, so we use a busy page then. 	 * 	 * Note also that we aren't as concerned about more than one thead 	 * attempting to pager_data_unlock the same page at once, so we don't 	 * hold the page as busy then, but do record the highest unlock value 	 * so far.  [Unlock requests may also be delivered out of order.] 	 * 	 * 2)	Once we have a busy page, we must remove it from the pageout 	 * queues, so that the pageout daemon will not grab it away. 	 * 	 * 3)	To prevent another process from racing us down the shadow chain 	 * and entering a new page in the top object before we do, we must 	 * keep a busy page in the top object while following the shadow 	 * chain. 	 * 	 * 4)	We must increment paging_in_progress on any object for which 	 * we have a busy page, to prevent vm_object_collapse from removing 	 * the busy page without our noticing. 	 */
comment|/* 	 * Search for the page at object/offset. 	 */
name|object
operator|=
name|first_object
expr_stmt|;
name|pindex
operator|=
name|first_pindex
expr_stmt|;
comment|/* 	 * See whether this page is resident 	 */
while|while
condition|(
name|TRUE
condition|)
block|{
name|m
operator|=
name|vm_page_lookup
argument_list|(
name|object
argument_list|,
name|pindex
argument_list|)
expr_stmt|;
if|if
condition|(
name|m
operator|!=
name|NULL
condition|)
block|{
name|int
name|queue
decl_stmt|;
comment|/* 			 * If the page is being brought in, wait for it and 			 * then retry. 			 */
if|if
condition|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_BUSY
operator|)
operator|||
name|m
operator|->
name|busy
condition|)
block|{
name|int
name|s
decl_stmt|;
name|UNLOCK_THINGS
expr_stmt|;
name|s
operator|=
name|splvm
argument_list|()
expr_stmt|;
if|if
condition|(
operator|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_BUSY
operator|)
operator|||
name|m
operator|->
name|busy
operator|)
condition|)
block|{
name|m
operator|->
name|flags
operator||=
name|PG_WANTED
operator||
name|PG_REFERENCED
expr_stmt|;
name|cnt
operator|.
name|v_intrans
operator|++
expr_stmt|;
name|tsleep
argument_list|(
name|m
argument_list|,
name|PSWP
argument_list|,
literal|"vmpfw"
argument_list|,
literal|0
argument_list|)
expr_stmt|;
block|}
name|splx
argument_list|(
name|s
argument_list|)
expr_stmt|;
name|vm_object_deallocate
argument_list|(
name|first_object
argument_list|)
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
name|queue
operator|=
name|m
operator|->
name|queue
expr_stmt|;
name|vm_page_unqueue_nowakeup
argument_list|(
name|m
argument_list|)
expr_stmt|;
comment|/* 			 * Mark page busy for other processes, and the pagedaemon. 			 */
if|if
condition|(
operator|(
operator|(
name|queue
operator|-
name|m
operator|->
name|pc
operator|)
operator|==
name|PQ_CACHE
operator|)
operator|&&
operator|(
name|cnt
operator|.
name|v_free_count
operator|+
name|cnt
operator|.
name|v_cache_count
operator|)
operator|<
name|cnt
operator|.
name|v_free_min
condition|)
block|{
name|vm_page_activate
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|UNLOCK_AND_DEALLOCATE
expr_stmt|;
name|VM_WAIT
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
name|m
operator|->
name|flags
operator||=
name|PG_BUSY
expr_stmt|;
if|if
condition|(
comment|/*m->valid&& */
operator|(
operator|(
name|m
operator|->
name|valid
operator|&
name|VM_PAGE_BITS_ALL
operator|)
operator|!=
name|VM_PAGE_BITS_ALL
operator|)
operator|&&
name|m
operator|->
name|object
operator|!=
name|kernel_object
operator|&&
name|m
operator|->
name|object
operator|!=
name|kmem_object
condition|)
block|{
goto|goto
name|readrest
goto|;
block|}
break|break;
block|}
if|if
condition|(
operator|(
operator|(
name|object
operator|->
name|type
operator|!=
name|OBJT_DEFAULT
operator|)
operator|&&
operator|(
operator|(
operator|(
name|fault_flags
operator|&
name|VM_FAULT_WIRE_MASK
operator|)
operator|==
literal|0
operator|)
operator|||
name|wired
operator|)
operator|)
operator|||
operator|(
name|object
operator|==
name|first_object
operator|)
condition|)
block|{
if|if
condition|(
name|pindex
operator|>=
name|object
operator|->
name|size
condition|)
block|{
name|UNLOCK_AND_DEALLOCATE
expr_stmt|;
return|return
operator|(
name|KERN_PROTECTION_FAILURE
operator|)
return|;
block|}
comment|/* 			 * Allocate a new page for this object/offset pair. 			 */
name|m
operator|=
name|vm_page_alloc
argument_list|(
name|object
argument_list|,
name|pindex
argument_list|,
operator|(
name|vp
operator|||
name|object
operator|->
name|backing_object
operator|)
condition|?
name|VM_ALLOC_NORMAL
else|:
name|VM_ALLOC_ZERO
argument_list|)
expr_stmt|;
if|if
condition|(
name|m
operator|==
name|NULL
condition|)
block|{
name|UNLOCK_AND_DEALLOCATE
expr_stmt|;
name|VM_WAIT
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
block|}
name|readrest
label|:
if|if
condition|(
name|object
operator|->
name|type
operator|!=
name|OBJT_DEFAULT
operator|&&
operator|(
operator|(
operator|(
name|fault_flags
operator|&
name|VM_FAULT_WIRE_MASK
operator|)
operator|==
literal|0
operator|)
operator|||
name|wired
operator|)
condition|)
block|{
name|int
name|rv
decl_stmt|;
name|int
name|faultcount
decl_stmt|;
name|int
name|reqpage
decl_stmt|;
name|int
name|ahead
decl_stmt|,
name|behind
decl_stmt|;
name|ahead
operator|=
name|VM_FAULT_READ_AHEAD
expr_stmt|;
name|behind
operator|=
name|VM_FAULT_READ_BEHIND
expr_stmt|;
if|if
condition|(
name|first_object
operator|->
name|behavior
operator|==
name|OBJ_RANDOM
condition|)
block|{
name|ahead
operator|=
literal|0
expr_stmt|;
name|behind
operator|=
literal|0
expr_stmt|;
block|}
if|if
condition|(
operator|(
name|first_object
operator|->
name|type
operator|!=
name|OBJT_DEVICE
operator|)
operator|&&
operator|(
name|first_object
operator|->
name|behavior
operator|==
name|OBJ_SEQUENTIAL
operator|)
condition|)
block|{
name|vm_pindex_t
name|firstpindex
decl_stmt|,
name|tmppindex
decl_stmt|;
if|if
condition|(
name|first_pindex
operator|<
literal|2
operator|*
operator|(
name|VM_FAULT_READ_BEHIND
operator|+
name|VM_FAULT_READ_AHEAD
operator|+
literal|1
operator|)
condition|)
name|firstpindex
operator|=
literal|0
expr_stmt|;
else|else
name|firstpindex
operator|=
name|first_pindex
operator|-
literal|2
operator|*
operator|(
name|VM_FAULT_READ_BEHIND
operator|+
name|VM_FAULT_READ_AHEAD
operator|+
literal|1
operator|)
expr_stmt|;
for|for
control|(
name|tmppindex
operator|=
name|first_pindex
operator|-
literal|1
init|;
name|tmppindex
operator|>=
name|firstpindex
condition|;
operator|--
name|tmppindex
control|)
block|{
name|vm_page_t
name|mt
decl_stmt|;
name|mt
operator|=
name|vm_page_lookup
argument_list|(
name|first_object
argument_list|,
name|tmppindex
argument_list|)
expr_stmt|;
if|if
condition|(
name|mt
operator|==
name|NULL
operator|||
operator|(
name|mt
operator|->
name|valid
operator|!=
name|VM_PAGE_BITS_ALL
operator|)
condition|)
break|break;
if|if
condition|(
name|mt
operator|->
name|busy
operator|||
operator|(
name|mt
operator|->
name|flags
operator|&
operator|(
name|PG_BUSY
operator||
name|PG_FICTITIOUS
operator|)
operator|)
operator|||
name|mt
operator|->
name|hold_count
operator|||
name|mt
operator|->
name|wire_count
condition|)
continue|continue;
if|if
condition|(
name|mt
operator|->
name|dirty
operator|==
literal|0
condition|)
name|vm_page_test_dirty
argument_list|(
name|mt
argument_list|)
expr_stmt|;
if|if
condition|(
name|mt
operator|->
name|dirty
condition|)
block|{
name|vm_page_protect
argument_list|(
name|mt
argument_list|,
name|VM_PROT_NONE
argument_list|)
expr_stmt|;
name|vm_page_deactivate
argument_list|(
name|mt
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|vm_page_cache
argument_list|(
name|mt
argument_list|)
expr_stmt|;
block|}
block|}
name|ahead
operator|+=
name|behind
expr_stmt|;
name|behind
operator|=
literal|0
expr_stmt|;
block|}
comment|/* 			 * now we find out if any other pages should be paged 			 * in at this time this routine checks to see if the 			 * pages surrounding this fault reside in the same 			 * object as the page for this fault.  If they do, 			 * then they are faulted in also into the object.  The 			 * array "marray" returned contains an array of 			 * vm_page_t structs where one of them is the 			 * vm_page_t passed to the routine.  The reqpage 			 * return value is the index into the marray for the 			 * vm_page_t passed to the routine. 			 */
name|faultcount
operator|=
name|vm_fault_additional_pages
argument_list|(
name|m
argument_list|,
name|behind
argument_list|,
name|ahead
argument_list|,
name|marray
argument_list|,
operator|&
name|reqpage
argument_list|)
expr_stmt|;
comment|/* 			 * Call the pager to retrieve the data, if any, after 			 * releasing the lock on the map. 			 */
name|UNLOCK_MAP
expr_stmt|;
name|rv
operator|=
name|faultcount
condition|?
name|vm_pager_get_pages
argument_list|(
name|object
argument_list|,
name|marray
argument_list|,
name|faultcount
argument_list|,
name|reqpage
argument_list|)
else|:
name|VM_PAGER_FAIL
expr_stmt|;
if|if
condition|(
name|rv
operator|==
name|VM_PAGER_OK
condition|)
block|{
comment|/* 				 * Found the page. Leave it busy while we play 				 * with it. 				 */
comment|/* 				 * Relookup in case pager changed page. Pager 				 * is responsible for disposition of old page 				 * if moved. 				 */
name|m
operator|=
name|vm_page_lookup
argument_list|(
name|object
argument_list|,
name|pindex
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|m
condition|)
block|{
name|UNLOCK_AND_DEALLOCATE
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
name|hardfault
operator|++
expr_stmt|;
break|break;
block|}
comment|/* 			 * Remove the bogus page (which does not exist at this 			 * object/offset); before doing so, we must get back 			 * our object lock to preserve our invariant. 			 * 			 * Also wake up any other process that may want to bring 			 * in this page. 			 * 			 * If this is the top-level object, we must leave the 			 * busy page to prevent another process from rushing 			 * past us, and inserting the page in that object at 			 * the same time that we are. 			 */
if|if
condition|(
name|rv
operator|==
name|VM_PAGER_ERROR
condition|)
name|printf
argument_list|(
literal|"vm_fault: pager input (probably hardware) error, PID %d failure\n"
argument_list|,
name|curproc
operator|->
name|p_pid
argument_list|)
expr_stmt|;
comment|/* 			 * Data outside the range of the pager or an I/O error 			 */
comment|/* 			 * XXX - the check for kernel_map is a kludge to work 			 * around having the machine panic on a kernel space 			 * fault w/ I/O error. 			 */
if|if
condition|(
operator|(
operator|(
name|map
operator|!=
name|kernel_map
operator|)
operator|&&
operator|(
name|rv
operator|==
name|VM_PAGER_ERROR
operator|)
operator|)
operator|||
operator|(
name|rv
operator|==
name|VM_PAGER_BAD
operator|)
condition|)
block|{
name|FREE_PAGE
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|UNLOCK_AND_DEALLOCATE
expr_stmt|;
return|return
operator|(
operator|(
name|rv
operator|==
name|VM_PAGER_ERROR
operator|)
condition|?
name|KERN_FAILURE
else|:
name|KERN_PROTECTION_FAILURE
operator|)
return|;
block|}
if|if
condition|(
name|object
operator|!=
name|first_object
condition|)
block|{
name|FREE_PAGE
argument_list|(
name|m
argument_list|)
expr_stmt|;
comment|/* 				 * XXX - we cannot just fall out at this 				 * point, m has been freed and is invalid! 				 */
block|}
block|}
comment|/* 		 * We get here if the object has default pager (or unwiring) or the 		 * pager doesn't have the page. 		 */
if|if
condition|(
name|object
operator|==
name|first_object
condition|)
name|first_m
operator|=
name|m
expr_stmt|;
comment|/* 		 * Move on to the next object.  Lock the next object before 		 * unlocking the current one. 		 */
name|pindex
operator|+=
name|OFF_TO_IDX
argument_list|(
name|object
operator|->
name|backing_object_offset
argument_list|)
expr_stmt|;
name|next_object
operator|=
name|object
operator|->
name|backing_object
expr_stmt|;
if|if
condition|(
name|next_object
operator|==
name|NULL
condition|)
block|{
comment|/* 			 * If there's no object left, fill the page in the top 			 * object with zeros. 			 */
if|if
condition|(
name|object
operator|!=
name|first_object
condition|)
block|{
name|vm_object_pip_wakeup
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|object
operator|=
name|first_object
expr_stmt|;
name|pindex
operator|=
name|first_pindex
expr_stmt|;
name|m
operator|=
name|first_m
expr_stmt|;
block|}
name|first_m
operator|=
name|NULL
expr_stmt|;
if|if
condition|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_ZERO
operator|)
operator|==
literal|0
condition|)
name|vm_page_zero_fill
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|cnt
operator|.
name|v_zfod
operator|++
expr_stmt|;
break|break;
block|}
else|else
block|{
if|if
condition|(
name|object
operator|!=
name|first_object
condition|)
block|{
name|vm_object_pip_wakeup
argument_list|(
name|object
argument_list|)
expr_stmt|;
block|}
name|object
operator|=
name|next_object
expr_stmt|;
name|object
operator|->
name|paging_in_progress
operator|++
expr_stmt|;
block|}
block|}
if|#
directive|if
name|defined
argument_list|(
name|DIAGNOSTIC
argument_list|)
if|if
condition|(
operator|(
name|m
operator|->
name|flags
operator|&
name|PG_BUSY
operator|)
operator|==
literal|0
condition|)
name|panic
argument_list|(
literal|"vm_fault: not busy after main loop"
argument_list|)
expr_stmt|;
endif|#
directive|endif
comment|/* 	 * PAGE HAS BEEN FOUND. [Loop invariant still holds -- the object lock 	 * is held.] 	 */
name|old_m
operator|=
name|m
expr_stmt|;
comment|/* save page that would be copied */
comment|/* 	 * If the page is being written, but isn't already owned by the 	 * top-level object, we have to copy it into a new page owned by the 	 * top-level object. 	 */
if|if
condition|(
name|object
operator|!=
name|first_object
condition|)
block|{
comment|/* 		 * We only really need to copy if we want to write it. 		 */
if|if
condition|(
name|fault_type
operator|&
name|VM_PROT_WRITE
condition|)
block|{
comment|/* 			 * This allows pages to be virtually copied from a backing_object 			 * into the first_object, where the backing object has no other 			 * refs to it, and cannot gain any more refs.  Instead of a 			 * bcopy, we just move the page from the backing object to the 			 * first object.  Note that we must mark the page dirty in the 			 * first object so that it will go out to swap when needed. 			 */
if|if
condition|(
name|lookup_still_valid
operator|&&
comment|/* 				 * Only one shadow object 				 */
operator|(
name|object
operator|->
name|shadow_count
operator|==
literal|1
operator|)
operator|&&
comment|/* 				 * No COW refs, except us 				 */
operator|(
name|object
operator|->
name|ref_count
operator|==
literal|1
operator|)
operator|&&
comment|/* 				 * Noone else can look this object up 				 */
operator|(
name|object
operator|->
name|handle
operator|==
name|NULL
operator|)
operator|&&
comment|/* 				 * No other ways to look the object up 				 */
operator|(
operator|(
name|object
operator|->
name|type
operator|==
name|OBJT_DEFAULT
operator|)
operator|||
operator|(
name|object
operator|->
name|type
operator|==
name|OBJT_SWAP
operator|)
operator|)
operator|&&
comment|/* 				 * We don't chase down the shadow chain 				 */
operator|(
name|object
operator|==
name|first_object
operator|->
name|backing_object
operator|)
condition|)
block|{
comment|/* 				 * get rid of the unnecessary page 				 */
name|vm_page_protect
argument_list|(
name|first_m
argument_list|,
name|VM_PROT_NONE
argument_list|)
expr_stmt|;
name|PAGE_WAKEUP
argument_list|(
name|first_m
argument_list|)
expr_stmt|;
name|vm_page_free
argument_list|(
name|first_m
argument_list|)
expr_stmt|;
comment|/* 				 * grab the page and put it into the process'es object 				 */
name|vm_page_rename
argument_list|(
name|m
argument_list|,
name|first_object
argument_list|,
name|first_pindex
argument_list|)
expr_stmt|;
name|first_m
operator|=
name|m
expr_stmt|;
name|m
operator|->
name|dirty
operator|=
name|VM_PAGE_BITS_ALL
expr_stmt|;
name|m
operator|=
name|NULL
expr_stmt|;
block|}
else|else
block|{
comment|/* 				 * Oh, well, lets copy it. 				 */
name|vm_page_copy
argument_list|(
name|m
argument_list|,
name|first_m
argument_list|)
expr_stmt|;
block|}
comment|/* 			 * This code handles the case where there are two references to the 			 * backing object, and one reference is getting a copy of the 			 * page.  If the other reference is the only other object that 			 * points to the backing object, then perform a virtual copy 			 * from the backing object to the other object after the 			 * page is copied to the current first_object.  If the other 			 * object already has the page, we destroy it in the backing object 			 * performing an optimized collapse-type operation.  We don't 			 * bother removing the page from the backing object's swap space. 			 */
if|if
condition|(
name|lookup_still_valid
operator|&&
comment|/* 				 * make sure that we have two shadow objs 				 */
operator|(
name|object
operator|->
name|shadow_count
operator|==
literal|2
operator|)
operator|&&
comment|/* 				 * And no COW refs -- note that there are sometimes 				 * temp refs to objs, but ignore that case -- we just 				 * punt. 				 */
operator|(
name|object
operator|->
name|ref_count
operator|==
literal|2
operator|)
operator|&&
comment|/* 				 * Noone else can look us up 				 */
operator|(
name|object
operator|->
name|handle
operator|==
name|NULL
operator|)
operator|&&
comment|/* 				 * Not something that can be referenced elsewhere 				 */
operator|(
operator|(
name|object
operator|->
name|type
operator|==
name|OBJT_DEFAULT
operator|)
operator|||
operator|(
name|object
operator|->
name|type
operator|==
name|OBJT_SWAP
operator|)
operator|)
operator|&&
comment|/* 				 * We don't bother chasing down object chain 				 */
operator|(
name|object
operator|==
name|first_object
operator|->
name|backing_object
operator|)
condition|)
block|{
name|vm_object_t
name|other_object
decl_stmt|;
name|vm_pindex_t
name|other_pindex
decl_stmt|,
name|other_pindex_offset
decl_stmt|;
name|vm_page_t
name|tm
decl_stmt|;
name|other_object
operator|=
name|TAILQ_FIRST
argument_list|(
operator|&
name|object
operator|->
name|shadow_head
argument_list|)
expr_stmt|;
if|if
condition|(
name|other_object
operator|==
name|first_object
condition|)
name|other_object
operator|=
name|TAILQ_NEXT
argument_list|(
name|other_object
argument_list|,
name|shadow_list
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|other_object
condition|)
name|panic
argument_list|(
literal|"vm_fault: other object missing"
argument_list|)
expr_stmt|;
if|if
condition|(
name|other_object
operator|&&
operator|(
name|other_object
operator|->
name|type
operator|==
name|OBJT_DEFAULT
operator|)
operator|&&
operator|(
name|other_object
operator|->
name|paging_in_progress
operator|==
literal|0
operator|)
condition|)
block|{
name|other_pindex_offset
operator|=
name|OFF_TO_IDX
argument_list|(
name|other_object
operator|->
name|backing_object_offset
argument_list|)
expr_stmt|;
if|if
condition|(
name|pindex
operator|>=
name|other_pindex_offset
condition|)
block|{
name|other_pindex
operator|=
name|pindex
operator|-
name|other_pindex_offset
expr_stmt|;
comment|/* 						 * If the other object has the page, just free it. 						 */
if|if
condition|(
operator|(
name|tm
operator|=
name|vm_page_lookup
argument_list|(
name|other_object
argument_list|,
name|other_pindex
argument_list|)
operator|)
condition|)
block|{
if|if
condition|(
operator|(
name|tm
operator|->
name|flags
operator|&
name|PG_BUSY
operator|)
operator|==
literal|0
operator|&&
name|tm
operator|->
name|busy
operator|==
literal|0
operator|&&
name|tm
operator|->
name|valid
operator|==
name|VM_PAGE_BITS_ALL
condition|)
block|{
comment|/* 								 * get rid of the unnecessary page 								 */
name|vm_page_protect
argument_list|(
name|m
argument_list|,
name|VM_PROT_NONE
argument_list|)
expr_stmt|;
name|PAGE_WAKEUP
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_page_free
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|m
operator|=
name|NULL
expr_stmt|;
name|tm
operator|->
name|dirty
operator|=
name|VM_PAGE_BITS_ALL
expr_stmt|;
name|first_m
operator|->
name|dirty
operator|=
name|VM_PAGE_BITS_ALL
expr_stmt|;
block|}
block|}
else|else
block|{
comment|/* 							 * If the other object doesn't have the page, 							 * then we move it there. 							 */
name|vm_page_rename
argument_list|(
name|m
argument_list|,
name|other_object
argument_list|,
name|other_pindex
argument_list|)
expr_stmt|;
name|m
operator|->
name|dirty
operator|=
name|VM_PAGE_BITS_ALL
expr_stmt|;
name|m
operator|->
name|valid
operator|=
name|VM_PAGE_BITS_ALL
expr_stmt|;
block|}
block|}
block|}
block|}
if|if
condition|(
name|m
condition|)
block|{
if|if
condition|(
name|m
operator|->
name|queue
operator|!=
name|PQ_ACTIVE
condition|)
name|vm_page_activate
argument_list|(
name|m
argument_list|)
expr_stmt|;
comment|/* 			 * We no longer need the old page or object. 			 */
name|PAGE_WAKEUP
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
name|vm_object_pip_wakeup
argument_list|(
name|object
argument_list|)
expr_stmt|;
comment|/* 			 * Only use the new page below... 			 */
name|cnt
operator|.
name|v_cow_faults
operator|++
expr_stmt|;
name|m
operator|=
name|first_m
expr_stmt|;
name|object
operator|=
name|first_object
expr_stmt|;
name|pindex
operator|=
name|first_pindex
expr_stmt|;
comment|/* 			 * Now that we've gotten the copy out of the way, 			 * let's try to collapse the top object. 			 * 			 * But we have to play ugly games with 			 * paging_in_progress to do that... 			 */
name|vm_object_pip_wakeup
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|vm_object_collapse
argument_list|(
name|object
argument_list|)
expr_stmt|;
name|object
operator|->
name|paging_in_progress
operator|++
expr_stmt|;
block|}
else|else
block|{
name|prot
operator|&=
operator|~
name|VM_PROT_WRITE
expr_stmt|;
block|}
block|}
comment|/* 	 * We must verify that the maps have not changed since our last 	 * lookup. 	 */
if|if
condition|(
operator|!
name|lookup_still_valid
condition|)
block|{
name|vm_object_t
name|retry_object
decl_stmt|;
name|vm_pindex_t
name|retry_pindex
decl_stmt|;
name|vm_prot_t
name|retry_prot
decl_stmt|;
comment|/* 		 * Since map entries may be pageable, make sure we can take a 		 * page fault on them. 		 */
comment|/* 		 * To avoid trying to write_lock the map while another process 		 * has it read_locked (in vm_map_pageable), we do not try for 		 * write permission.  If the page is still writable, we will 		 * get write permission.  If it is not, or has been marked 		 * needs_copy, we enter the mapping without write permission, 		 * and will merely take another fault. 		 */
name|result
operator|=
name|vm_map_lookup
argument_list|(
operator|&
name|map
argument_list|,
name|vaddr
argument_list|,
name|fault_type
operator|&
operator|~
name|VM_PROT_WRITE
argument_list|,
operator|&
name|entry
argument_list|,
operator|&
name|retry_object
argument_list|,
operator|&
name|retry_pindex
argument_list|,
operator|&
name|retry_prot
argument_list|,
operator|&
name|wired
argument_list|,
operator|&
name|su
argument_list|)
expr_stmt|;
comment|/* 		 * If we don't need the page any longer, put it on the active 		 * list (the easiest thing to do here).  If no one needs it, 		 * pageout will grab it eventually. 		 */
if|if
condition|(
name|result
operator|!=
name|KERN_SUCCESS
condition|)
block|{
name|RELEASE_PAGE
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|UNLOCK_AND_DEALLOCATE
expr_stmt|;
return|return
operator|(
name|result
operator|)
return|;
block|}
name|lookup_still_valid
operator|=
name|TRUE
expr_stmt|;
if|if
condition|(
operator|(
name|retry_object
operator|!=
name|first_object
operator|)
operator|||
operator|(
name|retry_pindex
operator|!=
name|first_pindex
operator|)
condition|)
block|{
name|RELEASE_PAGE
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|UNLOCK_AND_DEALLOCATE
expr_stmt|;
goto|goto
name|RetryFault
goto|;
block|}
comment|/* 		 * Check whether the protection has changed or the object has 		 * been copied while we left the map unlocked. Changing from 		 * read to write permission is OK - we leave the page 		 * write-protected, and catch the write fault. Changing from 		 * write to read permission means that we can't mark the page 		 * write-enabled after all. 		 */
name|prot
operator|&=
name|retry_prot
expr_stmt|;
block|}
comment|/* 	 * Put this page into the physical map. We had to do the unlock above 	 * because pmap_enter may cause other faults.   We don't put the page 	 * back on the active queue until later so that the page-out daemon 	 * won't find us (yet). 	 */
if|if
condition|(
name|prot
operator|&
name|VM_PROT_WRITE
condition|)
block|{
name|m
operator|->
name|flags
operator||=
name|PG_WRITEABLE
expr_stmt|;
name|m
operator|->
name|object
operator|->
name|flags
operator||=
name|OBJ_WRITEABLE
operator||
name|OBJ_MIGHTBEDIRTY
expr_stmt|;
comment|/* 		 * If the fault is a write, we know that this page is being 		 * written NOW. This will save on the pmap_is_modified() calls 		 * later. 		 */
if|if
condition|(
name|fault_flags
operator|&
name|VM_FAULT_DIRTY
condition|)
block|{
name|m
operator|->
name|dirty
operator|=
name|VM_PAGE_BITS_ALL
expr_stmt|;
block|}
block|}
name|UNLOCK_THINGS
expr_stmt|;
name|m
operator|->
name|valid
operator|=
name|VM_PAGE_BITS_ALL
expr_stmt|;
name|m
operator|->
name|flags
operator|&=
operator|~
name|PG_ZERO
expr_stmt|;
name|pmap_enter
argument_list|(
name|map
operator|->
name|pmap
argument_list|,
name|vaddr
argument_list|,
name|VM_PAGE_TO_PHYS
argument_list|(
name|m
argument_list|)
argument_list|,
name|prot
argument_list|,
name|wired
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
operator|(
name|fault_flags
operator|&
name|VM_FAULT_WIRE_MASK
operator|)
operator|==
literal|0
operator|)
operator|&&
operator|(
name|wired
operator|==
literal|0
operator|)
condition|)
name|pmap_prefault
argument_list|(
name|map
operator|->
name|pmap
argument_list|,
name|vaddr
argument_list|,
name|entry
argument_list|,
name|first_object
argument_list|)
expr_stmt|;
name|m
operator|->
name|flags
operator||=
name|PG_MAPPED
operator||
name|PG_REFERENCED
expr_stmt|;
if|if
condition|(
name|fault_flags
operator|&
name|VM_FAULT_HOLD
condition|)
name|vm_page_hold
argument_list|(
name|m
argument_list|)
expr_stmt|;
comment|/* 	 * If the page is not wired down, then put it where the pageout daemon 	 * can find it. 	 */
if|if
condition|(
name|fault_flags
operator|&
name|VM_FAULT_WIRE_MASK
condition|)
block|{
if|if
condition|(
name|wired
condition|)
name|vm_page_wire
argument_list|(
name|m
argument_list|)
expr_stmt|;
else|else
name|vm_page_unwire
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|m
operator|->
name|queue
operator|!=
name|PQ_ACTIVE
condition|)
name|vm_page_activate
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|curproc
operator|&&
operator|(
name|curproc
operator|->
name|p_flag
operator|&
name|P_INMEM
operator|)
operator|&&
name|curproc
operator|->
name|p_stats
condition|)
block|{
if|if
condition|(
name|hardfault
condition|)
block|{
name|curproc
operator|->
name|p_stats
operator|->
name|p_ru
operator|.
name|ru_majflt
operator|++
expr_stmt|;
block|}
else|else
block|{
name|curproc
operator|->
name|p_stats
operator|->
name|p_ru
operator|.
name|ru_minflt
operator|++
expr_stmt|;
block|}
block|}
comment|/* 	 * Unlock everything, and return 	 */
name|PAGE_WAKEUP
argument_list|(
name|m
argument_list|)
expr_stmt|;
name|vm_object_deallocate
argument_list|(
name|first_object
argument_list|)
expr_stmt|;
return|return
operator|(
name|KERN_SUCCESS
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  *	vm_fault_wire:  *  *	Wire down a range of virtual addresses in a map.  */
end_comment

begin_function
name|int
name|vm_fault_wire
parameter_list|(
name|map
parameter_list|,
name|start
parameter_list|,
name|end
parameter_list|)
name|vm_map_t
name|map
decl_stmt|;
name|vm_offset_t
name|start
decl_stmt|,
name|end
decl_stmt|;
block|{
specifier|register
name|vm_offset_t
name|va
decl_stmt|;
specifier|register
name|pmap_t
name|pmap
decl_stmt|;
name|int
name|rv
decl_stmt|;
name|pmap
operator|=
name|vm_map_pmap
argument_list|(
name|map
argument_list|)
expr_stmt|;
comment|/* 	 * Inform the physical mapping system that the range of addresses may 	 * not fault, so that page tables and such can be locked down as well. 	 */
name|pmap_pageable
argument_list|(
name|pmap
argument_list|,
name|start
argument_list|,
name|end
argument_list|,
name|FALSE
argument_list|)
expr_stmt|;
comment|/* 	 * We simulate a fault to get the page and enter it in the physical 	 * map. 	 */
for|for
control|(
name|va
operator|=
name|start
init|;
name|va
operator|<
name|end
condition|;
name|va
operator|+=
name|PAGE_SIZE
control|)
block|{
name|rv
operator|=
name|vm_fault
argument_list|(
name|map
argument_list|,
name|va
argument_list|,
name|VM_PROT_READ
operator||
name|VM_PROT_WRITE
argument_list|,
name|VM_FAULT_CHANGE_WIRING
argument_list|)
expr_stmt|;
if|if
condition|(
name|rv
condition|)
block|{
if|if
condition|(
name|va
operator|!=
name|start
condition|)
name|vm_fault_unwire
argument_list|(
name|map
argument_list|,
name|start
argument_list|,
name|va
argument_list|)
expr_stmt|;
return|return
operator|(
name|rv
operator|)
return|;
block|}
block|}
return|return
operator|(
name|KERN_SUCCESS
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  *	vm_fault_user_wire:  *  *	Wire down a range of virtual addresses in a map.  This  *	is for user mode though, so we only ask for read access  *	on currently read only sections.  */
end_comment

begin_function
name|int
name|vm_fault_user_wire
parameter_list|(
name|map
parameter_list|,
name|start
parameter_list|,
name|end
parameter_list|)
name|vm_map_t
name|map
decl_stmt|;
name|vm_offset_t
name|start
decl_stmt|,
name|end
decl_stmt|;
block|{
specifier|register
name|vm_offset_t
name|va
decl_stmt|;
specifier|register
name|pmap_t
name|pmap
decl_stmt|;
name|int
name|rv
decl_stmt|;
name|pmap
operator|=
name|vm_map_pmap
argument_list|(
name|map
argument_list|)
expr_stmt|;
comment|/* 	 * Inform the physical mapping system that the range of addresses may 	 * not fault, so that page tables and such can be locked down as well. 	 */
name|pmap_pageable
argument_list|(
name|pmap
argument_list|,
name|start
argument_list|,
name|end
argument_list|,
name|FALSE
argument_list|)
expr_stmt|;
comment|/* 	 * We simulate a fault to get the page and enter it in the physical 	 * map. 	 */
for|for
control|(
name|va
operator|=
name|start
init|;
name|va
operator|<
name|end
condition|;
name|va
operator|+=
name|PAGE_SIZE
control|)
block|{
name|rv
operator|=
name|vm_fault
argument_list|(
name|map
argument_list|,
name|va
argument_list|,
name|VM_PROT_READ
argument_list|,
name|VM_FAULT_USER_WIRE
argument_list|)
expr_stmt|;
if|if
condition|(
name|rv
condition|)
block|{
if|if
condition|(
name|va
operator|!=
name|start
condition|)
name|vm_fault_unwire
argument_list|(
name|map
argument_list|,
name|start
argument_list|,
name|va
argument_list|)
expr_stmt|;
return|return
operator|(
name|rv
operator|)
return|;
block|}
block|}
return|return
operator|(
name|KERN_SUCCESS
operator|)
return|;
block|}
end_function

begin_comment
comment|/*  *	vm_fault_unwire:  *  *	Unwire a range of virtual addresses in a map.  */
end_comment

begin_function
name|void
name|vm_fault_unwire
parameter_list|(
name|map
parameter_list|,
name|start
parameter_list|,
name|end
parameter_list|)
name|vm_map_t
name|map
decl_stmt|;
name|vm_offset_t
name|start
decl_stmt|,
name|end
decl_stmt|;
block|{
specifier|register
name|vm_offset_t
name|va
decl_stmt|,
name|pa
decl_stmt|;
specifier|register
name|pmap_t
name|pmap
decl_stmt|;
name|pmap
operator|=
name|vm_map_pmap
argument_list|(
name|map
argument_list|)
expr_stmt|;
comment|/* 	 * Since the pages are wired down, we must be able to get their 	 * mappings from the physical map system. 	 */
for|for
control|(
name|va
operator|=
name|start
init|;
name|va
operator|<
name|end
condition|;
name|va
operator|+=
name|PAGE_SIZE
control|)
block|{
name|pa
operator|=
name|pmap_extract
argument_list|(
name|pmap
argument_list|,
name|va
argument_list|)
expr_stmt|;
if|if
condition|(
name|pa
operator|!=
operator|(
name|vm_offset_t
operator|)
literal|0
condition|)
block|{
name|pmap_change_wiring
argument_list|(
name|pmap
argument_list|,
name|va
argument_list|,
name|FALSE
argument_list|)
expr_stmt|;
name|vm_page_unwire
argument_list|(
name|PHYS_TO_VM_PAGE
argument_list|(
name|pa
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
comment|/* 	 * Inform the physical mapping system that the range of addresses may 	 * fault, so that page tables and such may be unwired themselves. 	 */
name|pmap_pageable
argument_list|(
name|pmap
argument_list|,
name|start
argument_list|,
name|end
argument_list|,
name|TRUE
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|/*  *	Routine:  *		vm_fault_copy_entry  *	Function:  *		Copy all of the pages from a wired-down map entry to another.  *  *	In/out conditions:  *		The source and destination maps must be locked for write.  *		The source map entry must be wired down (or be a sharing map  *		entry corresponding to a main map entry that is wired down).  */
end_comment

begin_function
name|void
name|vm_fault_copy_entry
parameter_list|(
name|dst_map
parameter_list|,
name|src_map
parameter_list|,
name|dst_entry
parameter_list|,
name|src_entry
parameter_list|)
name|vm_map_t
name|dst_map
decl_stmt|;
name|vm_map_t
name|src_map
decl_stmt|;
name|vm_map_entry_t
name|dst_entry
decl_stmt|;
name|vm_map_entry_t
name|src_entry
decl_stmt|;
block|{
name|vm_object_t
name|dst_object
decl_stmt|;
name|vm_object_t
name|src_object
decl_stmt|;
name|vm_ooffset_t
name|dst_offset
decl_stmt|;
name|vm_ooffset_t
name|src_offset
decl_stmt|;
name|vm_prot_t
name|prot
decl_stmt|;
name|vm_offset_t
name|vaddr
decl_stmt|;
name|vm_page_t
name|dst_m
decl_stmt|;
name|vm_page_t
name|src_m
decl_stmt|;
ifdef|#
directive|ifdef
name|lint
name|src_map
operator|++
expr_stmt|;
endif|#
directive|endif
comment|/* lint */
name|src_object
operator|=
name|src_entry
operator|->
name|object
operator|.
name|vm_object
expr_stmt|;
name|src_offset
operator|=
name|src_entry
operator|->
name|offset
expr_stmt|;
comment|/* 	 * Create the top-level object for the destination entry. (Doesn't 	 * actually shadow anything - we copy the pages directly.) 	 */
name|dst_object
operator|=
name|vm_object_allocate
argument_list|(
name|OBJT_DEFAULT
argument_list|,
operator|(
name|vm_size_t
operator|)
name|OFF_TO_IDX
argument_list|(
name|dst_entry
operator|->
name|end
operator|-
name|dst_entry
operator|->
name|start
argument_list|)
argument_list|)
expr_stmt|;
name|dst_entry
operator|->
name|object
operator|.
name|vm_object
operator|=
name|dst_object
expr_stmt|;
name|dst_entry
operator|->
name|offset
operator|=
literal|0
expr_stmt|;
name|prot
operator|=
name|dst_entry
operator|->
name|max_protection
expr_stmt|;
comment|/* 	 * Loop through all of the pages in the entry's range, copying each 	 * one from the source object (it should be there) to the destination 	 * object. 	 */
for|for
control|(
name|vaddr
operator|=
name|dst_entry
operator|->
name|start
operator|,
name|dst_offset
operator|=
literal|0
init|;
name|vaddr
operator|<
name|dst_entry
operator|->
name|end
condition|;
name|vaddr
operator|+=
name|PAGE_SIZE
operator|,
name|dst_offset
operator|+=
name|PAGE_SIZE
control|)
block|{
comment|/* 		 * Allocate a page in the destination object 		 */
do|do
block|{
name|dst_m
operator|=
name|vm_page_alloc
argument_list|(
name|dst_object
argument_list|,
name|OFF_TO_IDX
argument_list|(
name|dst_offset
argument_list|)
argument_list|,
name|VM_ALLOC_NORMAL
argument_list|)
expr_stmt|;
if|if
condition|(
name|dst_m
operator|==
name|NULL
condition|)
block|{
name|VM_WAIT
expr_stmt|;
block|}
block|}
do|while
condition|(
name|dst_m
operator|==
name|NULL
condition|)
do|;
comment|/* 		 * Find the page in the source object, and copy it in. 		 * (Because the source is wired down, the page will be in 		 * memory.) 		 */
name|src_m
operator|=
name|vm_page_lookup
argument_list|(
name|src_object
argument_list|,
name|OFF_TO_IDX
argument_list|(
name|dst_offset
operator|+
name|src_offset
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|src_m
operator|==
name|NULL
condition|)
name|panic
argument_list|(
literal|"vm_fault_copy_wired: page missing"
argument_list|)
expr_stmt|;
name|vm_page_copy
argument_list|(
name|src_m
argument_list|,
name|dst_m
argument_list|)
expr_stmt|;
comment|/* 		 * Enter it in the pmap... 		 */
name|dst_m
operator|->
name|flags
operator|&=
operator|~
name|PG_ZERO
expr_stmt|;
name|pmap_enter
argument_list|(
name|dst_map
operator|->
name|pmap
argument_list|,
name|vaddr
argument_list|,
name|VM_PAGE_TO_PHYS
argument_list|(
name|dst_m
argument_list|)
argument_list|,
name|prot
argument_list|,
name|FALSE
argument_list|)
expr_stmt|;
name|dst_m
operator|->
name|flags
operator||=
name|PG_WRITEABLE
operator||
name|PG_MAPPED
expr_stmt|;
comment|/* 		 * Mark it no longer busy, and put it on the active list. 		 */
name|vm_page_activate
argument_list|(
name|dst_m
argument_list|)
expr_stmt|;
name|PAGE_WAKEUP
argument_list|(
name|dst_m
argument_list|)
expr_stmt|;
block|}
block|}
end_function

begin_comment
comment|/*  * This routine checks around the requested page for other pages that  * might be able to be faulted in.  This routine brackets the viable  * pages for the pages to be paged in.  *  * Inputs:  *	m, rbehind, rahead  *  * Outputs:  *  marray (array of vm_page_t), reqpage (index of requested page)  *  * Return value:  *  number of pages in marray  */
end_comment

begin_function
name|int
name|vm_fault_additional_pages
parameter_list|(
name|m
parameter_list|,
name|rbehind
parameter_list|,
name|rahead
parameter_list|,
name|marray
parameter_list|,
name|reqpage
parameter_list|)
name|vm_page_t
name|m
decl_stmt|;
name|int
name|rbehind
decl_stmt|;
name|int
name|rahead
decl_stmt|;
name|vm_page_t
modifier|*
name|marray
decl_stmt|;
name|int
modifier|*
name|reqpage
decl_stmt|;
block|{
name|int
name|i
decl_stmt|;
name|vm_object_t
name|object
decl_stmt|;
name|vm_pindex_t
name|pindex
decl_stmt|,
name|startpindex
decl_stmt|,
name|endpindex
decl_stmt|,
name|tpindex
decl_stmt|;
name|vm_offset_t
name|size
decl_stmt|;
name|vm_page_t
name|rtm
decl_stmt|;
name|int
name|treqpage
decl_stmt|;
name|int
name|cbehind
decl_stmt|,
name|cahead
decl_stmt|;
name|object
operator|=
name|m
operator|->
name|object
expr_stmt|;
name|pindex
operator|=
name|m
operator|->
name|pindex
expr_stmt|;
comment|/* 	 * we don't fault-ahead for device pager 	 */
if|if
condition|(
name|object
operator|->
name|type
operator|==
name|OBJT_DEVICE
condition|)
block|{
operator|*
name|reqpage
operator|=
literal|0
expr_stmt|;
name|marray
index|[
literal|0
index|]
operator|=
name|m
expr_stmt|;
return|return
literal|1
return|;
block|}
comment|/* 	 * if the requested page is not available, then give up now 	 */
if|if
condition|(
operator|!
name|vm_pager_has_page
argument_list|(
name|object
argument_list|,
name|OFF_TO_IDX
argument_list|(
name|object
operator|->
name|paging_offset
argument_list|)
operator|+
name|pindex
argument_list|,
operator|&
name|cbehind
argument_list|,
operator|&
name|cahead
argument_list|)
condition|)
return|return
literal|0
return|;
if|if
condition|(
operator|(
name|cbehind
operator|==
literal|0
operator|)
operator|&&
operator|(
name|cahead
operator|==
literal|0
operator|)
condition|)
block|{
operator|*
name|reqpage
operator|=
literal|0
expr_stmt|;
name|marray
index|[
literal|0
index|]
operator|=
name|m
expr_stmt|;
return|return
literal|1
return|;
block|}
if|if
condition|(
name|rahead
operator|>
name|cahead
condition|)
block|{
name|rahead
operator|=
name|cahead
expr_stmt|;
block|}
if|if
condition|(
name|rbehind
operator|>
name|cbehind
condition|)
block|{
name|rbehind
operator|=
name|cbehind
expr_stmt|;
block|}
comment|/* 	 * try to do any readahead that we might have free pages for. 	 */
if|if
condition|(
operator|(
name|rahead
operator|+
name|rbehind
operator|)
operator|>
operator|(
operator|(
name|cnt
operator|.
name|v_free_count
operator|+
name|cnt
operator|.
name|v_cache_count
operator|)
operator|-
name|cnt
operator|.
name|v_free_reserved
operator|)
condition|)
block|{
name|pagedaemon_wakeup
argument_list|()
expr_stmt|;
operator|*
name|reqpage
operator|=
literal|0
expr_stmt|;
name|marray
index|[
literal|0
index|]
operator|=
name|m
expr_stmt|;
return|return
literal|1
return|;
block|}
comment|/* 	 * scan backward for the read behind pages -- in memory or on disk not 	 * in same object 	 */
name|tpindex
operator|=
name|pindex
operator|-
literal|1
expr_stmt|;
if|if
condition|(
name|tpindex
operator|<
name|pindex
condition|)
block|{
if|if
condition|(
name|rbehind
operator|>
name|pindex
condition|)
name|rbehind
operator|=
name|pindex
expr_stmt|;
name|startpindex
operator|=
name|pindex
operator|-
name|rbehind
expr_stmt|;
while|while
condition|(
name|tpindex
operator|>=
name|startpindex
condition|)
block|{
if|if
condition|(
name|vm_page_lookup
argument_list|(
name|object
argument_list|,
name|tpindex
argument_list|)
condition|)
block|{
name|startpindex
operator|=
name|tpindex
operator|+
literal|1
expr_stmt|;
break|break;
block|}
if|if
condition|(
name|tpindex
operator|==
literal|0
condition|)
break|break;
name|tpindex
operator|-=
literal|1
expr_stmt|;
block|}
block|}
else|else
block|{
name|startpindex
operator|=
name|pindex
expr_stmt|;
block|}
comment|/* 	 * scan forward for the read ahead pages -- in memory or on disk not 	 * in same object 	 */
name|tpindex
operator|=
name|pindex
operator|+
literal|1
expr_stmt|;
name|endpindex
operator|=
name|pindex
operator|+
operator|(
name|rahead
operator|+
literal|1
operator|)
expr_stmt|;
if|if
condition|(
name|endpindex
operator|>
name|object
operator|->
name|size
condition|)
name|endpindex
operator|=
name|object
operator|->
name|size
expr_stmt|;
while|while
condition|(
name|tpindex
operator|<
name|endpindex
condition|)
block|{
if|if
condition|(
name|vm_page_lookup
argument_list|(
name|object
argument_list|,
name|tpindex
argument_list|)
condition|)
block|{
break|break;
block|}
name|tpindex
operator|+=
literal|1
expr_stmt|;
block|}
name|endpindex
operator|=
name|tpindex
expr_stmt|;
comment|/* calculate number of bytes of pages */
name|size
operator|=
name|endpindex
operator|-
name|startpindex
expr_stmt|;
comment|/* calculate the page offset of the required page */
name|treqpage
operator|=
name|pindex
operator|-
name|startpindex
expr_stmt|;
comment|/* see if we have space (again) */
if|if
condition|(
operator|(
name|cnt
operator|.
name|v_free_count
operator|+
name|cnt
operator|.
name|v_cache_count
operator|)
operator|>
operator|(
name|cnt
operator|.
name|v_free_reserved
operator|+
name|size
operator|)
condition|)
block|{
comment|/* 		 * get our pages and don't block for them 		 */
for|for
control|(
name|i
operator|=
literal|0
init|;
name|i
operator|<
name|size
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|i
operator|!=
name|treqpage
condition|)
block|{
name|rtm
operator|=
name|vm_page_alloc
argument_list|(
name|object
argument_list|,
name|startpindex
operator|+
name|i
argument_list|,
name|VM_ALLOC_NORMAL
argument_list|)
expr_stmt|;
if|if
condition|(
name|rtm
operator|==
name|NULL
condition|)
block|{
if|if
condition|(
name|i
operator|<
name|treqpage
condition|)
block|{
name|int
name|j
decl_stmt|;
for|for
control|(
name|j
operator|=
literal|0
init|;
name|j
operator|<
name|i
condition|;
name|j
operator|++
control|)
block|{
name|FREE_PAGE
argument_list|(
name|marray
index|[
name|j
index|]
argument_list|)
expr_stmt|;
block|}
operator|*
name|reqpage
operator|=
literal|0
expr_stmt|;
name|marray
index|[
literal|0
index|]
operator|=
name|m
expr_stmt|;
return|return
literal|1
return|;
block|}
else|else
block|{
name|size
operator|=
name|i
expr_stmt|;
operator|*
name|reqpage
operator|=
name|treqpage
expr_stmt|;
return|return
name|size
return|;
block|}
block|}
name|marray
index|[
name|i
index|]
operator|=
name|rtm
expr_stmt|;
block|}
else|else
block|{
name|marray
index|[
name|i
index|]
operator|=
name|m
expr_stmt|;
block|}
block|}
operator|*
name|reqpage
operator|=
name|treqpage
expr_stmt|;
return|return
name|size
return|;
block|}
operator|*
name|reqpage
operator|=
literal|0
expr_stmt|;
name|marray
index|[
literal|0
index|]
operator|=
name|m
expr_stmt|;
return|return
literal|1
return|;
block|}
end_function

end_unit

