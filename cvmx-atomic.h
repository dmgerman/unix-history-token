begin_unit|revision:0.9.5;language:C;cregit-version:0.0.1
begin_comment
comment|/***********************license start***************  * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights  * reserved.  *  *  * Redistribution and use in source and binary forms, with or without  * modification, are permitted provided that the following conditions are  * met:  *  *   * Redistributions of source code must retain the above copyright  *     notice, this list of conditions and the following disclaimer.  *  *   * Redistributions in binary form must reproduce the above  *     copyright notice, this list of conditions and the following  *     disclaimer in the documentation and/or other materials provided  *     with the distribution.   *   * Neither the name of Cavium Inc. nor the names of  *     its contributors may be used to endorse or promote products  *     derived from this software without specific prior written  *     permission.   * This Software, including technical data, may be subject to U.S. export  control  * laws, including the U.S. Export Administration Act and its  associated  * regulations, and may be subject to export or import  regulations in other  * countries.   * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"  * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR  * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO  * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR  * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM  * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,  * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF  * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR  * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR  * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.  ***********************license end**************************************/
end_comment

begin_comment
comment|/**  * @file  *  * This file provides atomic operations  *  *<hr>$Revision: 70030 $<hr>  *  *  */
end_comment

begin_ifndef
ifndef|#
directive|ifndef
name|__CVMX_ATOMIC_H__
end_ifndef

begin_define
define|#
directive|define
name|__CVMX_ATOMIC_H__
end_define

begin_ifdef
ifdef|#
directive|ifdef
name|__cplusplus
end_ifdef

begin_extern
extern|extern
literal|"C"
block|{
endif|#
directive|endif
comment|/**  * Atomically adds a signed value to a 32 bit (aligned) memory location.  *  * This version does not perform 'sync' operations to enforce memory  * operations.  This should only be used when there are no memory operation  * ordering constraints.  (This should NOT be used for reference counting -  * use the standard version instead.)  *  * @param ptr    address in memory to add incr to  * @param incr   amount to increment memory location by (signed)  */
specifier|static
specifier|inline
name|void
name|cvmx_atomic_add32_nosync
parameter_list|(
name|int32_t
modifier|*
name|ptr
parameter_list|,
name|int32_t
name|incr
parameter_list|)
block|{
if|if
condition|(
name|OCTEON_IS_MODEL
argument_list|(
name|OCTEON_CN3XXX
argument_list|)
condition|)
block|{
name|uint32_t
name|tmp
decl_stmt|;
asm|__asm__
specifier|__volatile__
asm|(         ".set noreorder         \n"         "1: ll   %[tmp], %[val] \n"         "   addu %[tmp], %[inc] \n"         "   sc   %[tmp], %[val] \n"         "   beqz %[tmp], 1b     \n"         "   nop                 \n"         ".set reorder           \n"         : [val] "+m" (*ptr), [tmp] "=&r" (tmp)         : [inc] "r" (incr)         : "memory");
block|}
else|else
block|{
asm|__asm__
specifier|__volatile__
asm|(         "   saa %[inc], (%[base]) \n"         : "+m" (*ptr)         : [inc] "r" (incr), [base] "r" (ptr)         : "memory");
block|}
block|}
comment|/**  * Atomically adds a signed value to a 32 bit (aligned) memory location.  *  * Memory access ordering is enforced before/after the atomic operation,  * so no additional 'sync' instructions are required.  *  *  * @param ptr    address in memory to add incr to  * @param incr   amount to increment memory location by (signed)  */
specifier|static
specifier|inline
name|void
name|cvmx_atomic_add32
parameter_list|(
name|int32_t
modifier|*
name|ptr
parameter_list|,
name|int32_t
name|incr
parameter_list|)
block|{
name|CVMX_SYNCWS
expr_stmt|;
name|cvmx_atomic_add32_nosync
argument_list|(
name|ptr
argument_list|,
name|incr
argument_list|)
expr_stmt|;
name|CVMX_SYNCWS
expr_stmt|;
block|}
comment|/**  * Atomically sets a 32 bit (aligned) memory location to a value  *  * @param ptr    address of memory to set  * @param value  value to set memory location to.  */
specifier|static
specifier|inline
name|void
name|cvmx_atomic_set32
parameter_list|(
name|int32_t
modifier|*
name|ptr
parameter_list|,
name|int32_t
name|value
parameter_list|)
block|{
name|CVMX_SYNCWS
expr_stmt|;
operator|*
name|ptr
operator|=
name|value
expr_stmt|;
name|CVMX_SYNCWS
expr_stmt|;
block|}
comment|/**  * Returns the current value of a 32 bit (aligned) memory  * location.  *  * @param ptr    Address of memory to get  * @return Value of the memory  */
specifier|static
specifier|inline
name|int32_t
name|cvmx_atomic_get32
parameter_list|(
name|int32_t
modifier|*
name|ptr
parameter_list|)
block|{
return|return
operator|*
operator|(
specifier|volatile
name|int32_t
operator|*
operator|)
name|ptr
return|;
block|}
comment|/**  * Atomically adds a signed value to a 64 bit (aligned) memory location.  *  * This version does not perform 'sync' operations to enforce memory  * operations.  This should only be used when there are no memory operation  * ordering constraints.  (This should NOT be used for reference counting -  * use the standard version instead.)  *  * @param ptr    address in memory to add incr to  * @param incr   amount to increment memory location by (signed)  */
specifier|static
specifier|inline
name|void
name|cvmx_atomic_add64_nosync
parameter_list|(
name|int64_t
modifier|*
name|ptr
parameter_list|,
name|int64_t
name|incr
parameter_list|)
block|{
if|if
condition|(
name|OCTEON_IS_MODEL
argument_list|(
name|OCTEON_CN3XXX
argument_list|)
condition|)
block|{
name|uint64_t
name|tmp
decl_stmt|;
asm|__asm__
specifier|__volatile__
asm|(         ".set noreorder         \n"         "1: lld  %[tmp], %[val] \n"         "   daddu %[tmp], %[inc] \n"         "   scd  %[tmp], %[val] \n"         "   beqz %[tmp], 1b     \n"         "   nop                 \n"         ".set reorder           \n"         : [val] "+m" (*ptr), [tmp] "=&r" (tmp)         : [inc] "r" (incr)         : "memory");
block|}
else|else
block|{
asm|__asm__
specifier|__volatile__
asm|(         "   saad %[inc], (%[base])  \n"         : "+m" (*ptr)         : [inc] "r" (incr), [base] "r" (ptr)         : "memory");
block|}
block|}
comment|/**  * Atomically adds a signed value to a 64 bit (aligned) memory location.  *  * Memory access ordering is enforced before/after the atomic operation,  * so no additional 'sync' instructions are required.  *  *  * @param ptr    address in memory to add incr to  * @param incr   amount to increment memory location by (signed)  */
specifier|static
specifier|inline
name|void
name|cvmx_atomic_add64
parameter_list|(
name|int64_t
modifier|*
name|ptr
parameter_list|,
name|int64_t
name|incr
parameter_list|)
block|{
name|CVMX_SYNCWS
expr_stmt|;
name|cvmx_atomic_add64_nosync
argument_list|(
name|ptr
argument_list|,
name|incr
argument_list|)
expr_stmt|;
name|CVMX_SYNCWS
expr_stmt|;
block|}
comment|/**  * Atomically sets a 64 bit (aligned) memory location to a value  *  * @param ptr    address of memory to set  * @param value  value to set memory location to.  */
specifier|static
specifier|inline
name|void
name|cvmx_atomic_set64
parameter_list|(
name|int64_t
modifier|*
name|ptr
parameter_list|,
name|int64_t
name|value
parameter_list|)
block|{
name|CVMX_SYNCWS
expr_stmt|;
operator|*
name|ptr
operator|=
name|value
expr_stmt|;
name|CVMX_SYNCWS
expr_stmt|;
block|}
comment|/**  * Returns the current value of a 64 bit (aligned) memory  * location.  *  * @param ptr    Address of memory to get  * @return Value of the memory  */
specifier|static
specifier|inline
name|int64_t
name|cvmx_atomic_get64
parameter_list|(
name|int64_t
modifier|*
name|ptr
parameter_list|)
block|{
return|return
operator|*
operator|(
specifier|volatile
name|int64_t
operator|*
operator|)
name|ptr
return|;
block|}
comment|/**  * Atomically compares the old value with the value at ptr, and if they match,  * stores new_val to ptr.  * If *ptr and old don't match, function returns failure immediately.  * If *ptr and old match, function spins until *ptr updated to new atomically, or  *  until *ptr and old no longer match  *  * Does no memory synchronization.  *  * @return 1 on success (match and store)  *         0 on no match  */
specifier|static
specifier|inline
name|uint32_t
name|cvmx_atomic_compare_and_store32_nosync
parameter_list|(
name|uint32_t
modifier|*
name|ptr
parameter_list|,
name|uint32_t
name|old_val
parameter_list|,
name|uint32_t
name|new_val
parameter_list|)
block|{
name|uint32_t
name|tmp
decl_stmt|,
name|ret
decl_stmt|;
asm|__asm__
specifier|__volatile__
asm|(     ".set noreorder         \n"     "1: ll   %[tmp], %[val] \n"     "   li   %[ret], 0     \n"     "   bne  %[tmp], %[old], 2f \n"     "   move %[tmp], %[new_val] \n"     "   sc   %[tmp], %[val] \n"     "   beqz %[tmp], 1b     \n"     "   li   %[ret], 1      \n"     "2: nop               \n"     ".set reorder           \n"     : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)     : [old] "r" (old_val), [new_val] "r" (new_val)     : "memory");
return|return
operator|(
name|ret
operator|)
return|;
block|}
comment|/**  * Atomically compares the old value with the value at ptr, and if they match,  * stores new_val to ptr.  * If *ptr and old don't match, function returns failure immediately.  * If *ptr and old match, function spins until *ptr updated to new atomically, or  *  until *ptr and old no longer match  *  * Does memory synchronization that is required to use this as a locking primitive.  *  * @return 1 on success (match and store)  *         0 on no match  */
specifier|static
specifier|inline
name|uint32_t
name|cvmx_atomic_compare_and_store32
parameter_list|(
name|uint32_t
modifier|*
name|ptr
parameter_list|,
name|uint32_t
name|old_val
parameter_list|,
name|uint32_t
name|new_val
parameter_list|)
block|{
name|uint32_t
name|ret
decl_stmt|;
name|CVMX_SYNCWS
expr_stmt|;
name|ret
operator|=
name|cvmx_atomic_compare_and_store32_nosync
argument_list|(
name|ptr
argument_list|,
name|old_val
argument_list|,
name|new_val
argument_list|)
expr_stmt|;
name|CVMX_SYNCWS
expr_stmt|;
return|return
name|ret
return|;
block|}
comment|/**  * Atomically compares the old value with the value at ptr, and if they match,  * stores new_val to ptr.  * If *ptr and old don't match, function returns failure immediately.  * If *ptr and old match, function spins until *ptr updated to new atomically, or  *  until *ptr and old no longer match  *  * Does no memory synchronization.  *  * @return 1 on success (match and store)  *         0 on no match  */
specifier|static
specifier|inline
name|uint64_t
name|cvmx_atomic_compare_and_store64_nosync
parameter_list|(
name|uint64_t
modifier|*
name|ptr
parameter_list|,
name|uint64_t
name|old_val
parameter_list|,
name|uint64_t
name|new_val
parameter_list|)
block|{
name|uint64_t
name|tmp
decl_stmt|,
name|ret
decl_stmt|;
asm|__asm__
specifier|__volatile__
asm|(     ".set noreorder         \n"     "1: lld  %[tmp], %[val] \n"     "   li   %[ret], 0     \n"     "   bne  %[tmp], %[old], 2f \n"     "   move %[tmp], %[new_val] \n"     "   scd  %[tmp], %[val] \n"     "   beqz %[tmp], 1b     \n"     "   li   %[ret], 1      \n"     "2: nop               \n"     ".set reorder           \n"     : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)     : [old] "r" (old_val), [new_val] "r" (new_val)     : "memory");
return|return
operator|(
name|ret
operator|)
return|;
block|}
comment|/**  * Atomically compares the old value with the value at ptr, and if they match,  * stores new_val to ptr.  * If *ptr and old don't match, function returns failure immediately.  * If *ptr and old match, function spins until *ptr updated to new atomically, or  *  until *ptr and old no longer match  *  * Does memory synchronization that is required to use this as a locking primitive.  *  * @return 1 on success (match and store)  *         0 on no match  */
specifier|static
specifier|inline
name|uint64_t
name|cvmx_atomic_compare_and_store64
parameter_list|(
name|uint64_t
modifier|*
name|ptr
parameter_list|,
name|uint64_t
name|old_val
parameter_list|,
name|uint64_t
name|new_val
parameter_list|)
block|{
name|uint64_t
name|ret
decl_stmt|;
name|CVMX_SYNCWS
expr_stmt|;
name|ret
operator|=
name|cvmx_atomic_compare_and_store64_nosync
argument_list|(
name|ptr
argument_list|,
name|old_val
argument_list|,
name|new_val
argument_list|)
expr_stmt|;
name|CVMX_SYNCWS
expr_stmt|;
return|return
name|ret
return|;
block|}
comment|/**  * Atomically adds a signed value to a 64 bit (aligned) memory location,  * and returns previous value.  *  * This version does not perform 'sync' operations to enforce memory  * operations.  This should only be used when there are no memory operation  * ordering constraints.  (This should NOT be used for reference counting -  * use the standard version instead.)  *  * @param ptr    address in memory to add incr to  * @param incr   amount to increment memory location by (signed)  *  * @return Value of memory location before increment  */
specifier|static
specifier|inline
name|int64_t
name|cvmx_atomic_fetch_and_add64_nosync
parameter_list|(
name|int64_t
modifier|*
name|ptr
parameter_list|,
name|int64_t
name|incr
parameter_list|)
block|{
name|uint64_t
name|tmp
decl_stmt|,
name|ret
decl_stmt|;
if|if
condition|(
name|OCTEON_IS_MODEL
argument_list|(
name|OCTEON_CN6XXX
argument_list|)
operator|||
name|OCTEON_IS_MODEL
argument_list|(
name|OCTEON_CNF7XXX
argument_list|)
condition|)
block|{
name|CVMX_PUSH_OCTEON2
expr_stmt|;
if|if
condition|(
name|__builtin_constant_p
argument_list|(
name|incr
argument_list|)
operator|&&
name|incr
operator|==
literal|1
condition|)
block|{
asm|__asm__
specifier|__volatile__
asm|( 		"laid  %0,(%2)" 		: "=r" (ret), "+m" (ptr) : "r" (ptr) : "memory");
block|}
elseif|else
if|if
condition|(
name|__builtin_constant_p
argument_list|(
name|incr
argument_list|)
operator|&&
name|incr
operator|==
operator|-
literal|1
condition|)
block|{
asm|__asm__
specifier|__volatile__
asm|( 		"ladd  %0,(%2)" 		: "=r" (ret), "+m" (ptr) : "r" (ptr) : "memory");
block|}
else|else
block|{
asm|__asm__
specifier|__volatile__
asm|( 		"laad  %0,(%2),%3" 		: "=r" (ret), "+m" (ptr) : "r" (ptr), "r" (incr) : "memory");
block|}
name|CVMX_POP_OCTEON2
expr_stmt|;
block|}
else|else
block|{
asm|__asm__
specifier|__volatile__
asm|(             ".set noreorder          \n"             "1: lld   %[tmp], %[val] \n"             "   move  %[ret], %[tmp] \n"             "   daddu %[tmp], %[inc] \n"             "   scd   %[tmp], %[val] \n"             "   beqz  %[tmp], 1b     \n"             "   nop                  \n"             ".set reorder            \n"             : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)             : [inc] "r" (incr)             : "memory");
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
comment|/**  * Atomically adds a signed value to a 64 bit (aligned) memory location,  * and returns previous value.  *  * Memory access ordering is enforced before/after the atomic operation,  * so no additional 'sync' instructions are required.  *  * @param ptr    address in memory to add incr to  * @param incr   amount to increment memory location by (signed)  *  * @return Value of memory location before increment  */
specifier|static
specifier|inline
name|int64_t
name|cvmx_atomic_fetch_and_add64
parameter_list|(
name|int64_t
modifier|*
name|ptr
parameter_list|,
name|int64_t
name|incr
parameter_list|)
block|{
name|uint64_t
name|ret
decl_stmt|;
name|CVMX_SYNCWS
expr_stmt|;
name|ret
operator|=
name|cvmx_atomic_fetch_and_add64_nosync
argument_list|(
name|ptr
argument_list|,
name|incr
argument_list|)
expr_stmt|;
name|CVMX_SYNCWS
expr_stmt|;
return|return
name|ret
return|;
block|}
comment|/**  * Atomically adds a signed value to a 32 bit (aligned) memory location,  * and returns previous value.  *  * This version does not perform 'sync' operations to enforce memory  * operations.  This should only be used when there are no memory operation  * ordering constraints.  (This should NOT be used for reference counting -  * use the standard version instead.)  *  * @param ptr    address in memory to add incr to  * @param incr   amount to increment memory location by (signed)  *  * @return Value of memory location before increment  */
specifier|static
specifier|inline
name|int32_t
name|cvmx_atomic_fetch_and_add32_nosync
parameter_list|(
name|int32_t
modifier|*
name|ptr
parameter_list|,
name|int32_t
name|incr
parameter_list|)
block|{
name|uint32_t
name|tmp
decl_stmt|,
name|ret
decl_stmt|;
if|if
condition|(
name|OCTEON_IS_MODEL
argument_list|(
name|OCTEON_CN6XXX
argument_list|)
operator|||
name|OCTEON_IS_MODEL
argument_list|(
name|OCTEON_CNF7XXX
argument_list|)
condition|)
block|{
name|CVMX_PUSH_OCTEON2
expr_stmt|;
if|if
condition|(
name|__builtin_constant_p
argument_list|(
name|incr
argument_list|)
operator|&&
name|incr
operator|==
literal|1
condition|)
block|{
asm|__asm__
specifier|__volatile__
asm|( 		"lai  %0,(%2)" 		: "=r" (ret), "+m" (ptr) : "r" (ptr) : "memory");
block|}
elseif|else
if|if
condition|(
name|__builtin_constant_p
argument_list|(
name|incr
argument_list|)
operator|&&
name|incr
operator|==
operator|-
literal|1
condition|)
block|{
asm|__asm__
specifier|__volatile__
asm|( 		"lad  %0,(%2)" 		: "=r" (ret), "+m" (ptr) : "r" (ptr) : "memory");
block|}
else|else
block|{
asm|__asm__
specifier|__volatile__
asm|( 		"laa  %0,(%2),%3" 		: "=r" (ret), "+m" (ptr) : "r" (ptr), "r" (incr) : "memory");
block|}
name|CVMX_POP_OCTEON2
expr_stmt|;
block|}
else|else
block|{
asm|__asm__
specifier|__volatile__
asm|(             ".set noreorder         \n"             "1: ll   %[tmp], %[val] \n"             "   move %[ret], %[tmp] \n"             "   addu %[tmp], %[inc] \n"             "   sc   %[tmp], %[val] \n"             "   beqz %[tmp], 1b     \n"             "   nop                 \n"             ".set reorder           \n"             : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)             : [inc] "r" (incr)             : "memory");
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
comment|/**  * Atomically adds a signed value to a 32 bit (aligned) memory location,  * and returns previous value.  *  * Memory access ordering is enforced before/after the atomic operation,  * so no additional 'sync' instructions are required.  *  * @param ptr    address in memory to add incr to  * @param incr   amount to increment memory location by (signed)  *  * @return Value of memory location before increment  */
specifier|static
specifier|inline
name|int32_t
name|cvmx_atomic_fetch_and_add32
parameter_list|(
name|int32_t
modifier|*
name|ptr
parameter_list|,
name|int32_t
name|incr
parameter_list|)
block|{
name|uint32_t
name|ret
decl_stmt|;
name|CVMX_SYNCWS
expr_stmt|;
name|ret
operator|=
name|cvmx_atomic_fetch_and_add32_nosync
argument_list|(
name|ptr
argument_list|,
name|incr
argument_list|)
expr_stmt|;
name|CVMX_SYNCWS
expr_stmt|;
return|return
name|ret
return|;
block|}
comment|/**  * Atomically set bits in a 64 bit (aligned) memory location,  * and returns previous value.  *  * This version does not perform 'sync' operations to enforce memory  * operations.  This should only be used when there are no memory operation  * ordering constraints.  *  * @param ptr    address in memory  * @param mask   mask of bits to set  *  * @return Value of memory location before setting bits  */
specifier|static
specifier|inline
name|uint64_t
name|cvmx_atomic_fetch_and_bset64_nosync
parameter_list|(
name|uint64_t
modifier|*
name|ptr
parameter_list|,
name|uint64_t
name|mask
parameter_list|)
block|{
name|uint64_t
name|tmp
decl_stmt|,
name|ret
decl_stmt|;
asm|__asm__
specifier|__volatile__
asm|(     ".set noreorder         \n"     "1: lld  %[tmp], %[val] \n"     "   move %[ret], %[tmp] \n"     "   or   %[tmp], %[msk] \n"     "   scd  %[tmp], %[val] \n"     "   beqz %[tmp], 1b     \n"     "   nop                 \n"     ".set reorder           \n"     : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)     : [msk] "r" (mask)     : "memory");
return|return
operator|(
name|ret
operator|)
return|;
block|}
comment|/**  * Atomically set bits in a 32 bit (aligned) memory location,  * and returns previous value.  *  * This version does not perform 'sync' operations to enforce memory  * operations.  This should only be used when there are no memory operation  * ordering constraints.  *  * @param ptr    address in memory  * @param mask   mask of bits to set  *  * @return Value of memory location before setting bits  */
specifier|static
specifier|inline
name|uint32_t
name|cvmx_atomic_fetch_and_bset32_nosync
parameter_list|(
name|uint32_t
modifier|*
name|ptr
parameter_list|,
name|uint32_t
name|mask
parameter_list|)
block|{
name|uint32_t
name|tmp
decl_stmt|,
name|ret
decl_stmt|;
asm|__asm__
specifier|__volatile__
asm|(     ".set noreorder         \n"     "1: ll   %[tmp], %[val] \n"     "   move %[ret], %[tmp] \n"     "   or   %[tmp], %[msk] \n"     "   sc   %[tmp], %[val] \n"     "   beqz %[tmp], 1b     \n"     "   nop                 \n"     ".set reorder           \n"     : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)     : [msk] "r" (mask)     : "memory");
return|return
operator|(
name|ret
operator|)
return|;
block|}
comment|/**  * Atomically clear bits in a 64 bit (aligned) memory location,  * and returns previous value.  *  * This version does not perform 'sync' operations to enforce memory  * operations.  This should only be used when there are no memory operation  * ordering constraints.  *  * @param ptr    address in memory  * @param mask   mask of bits to clear  *  * @return Value of memory location before clearing bits  */
specifier|static
specifier|inline
name|uint64_t
name|cvmx_atomic_fetch_and_bclr64_nosync
parameter_list|(
name|uint64_t
modifier|*
name|ptr
parameter_list|,
name|uint64_t
name|mask
parameter_list|)
block|{
name|uint64_t
name|tmp
decl_stmt|,
name|ret
decl_stmt|;
asm|__asm__
specifier|__volatile__
asm|(     ".set noreorder         \n"     "   nor  %[msk], 0      \n"     "1: lld  %[tmp], %[val] \n"     "   move %[ret], %[tmp] \n"     "   and  %[tmp], %[msk] \n"     "   scd  %[tmp], %[val] \n"     "   beqz %[tmp], 1b     \n"     "   nop                 \n"     ".set reorder           \n"     : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret), [msk] "+r" (mask)     : : "memory");
return|return
operator|(
name|ret
operator|)
return|;
block|}
comment|/**  * Atomically clear bits in a 32 bit (aligned) memory location,  * and returns previous value.  *  * This version does not perform 'sync' operations to enforce memory  * operations.  This should only be used when there are no memory operation  * ordering constraints.  *  * @param ptr    address in memory  * @param mask   mask of bits to clear  *  * @return Value of memory location before clearing bits  */
specifier|static
specifier|inline
name|uint32_t
name|cvmx_atomic_fetch_and_bclr32_nosync
parameter_list|(
name|uint32_t
modifier|*
name|ptr
parameter_list|,
name|uint32_t
name|mask
parameter_list|)
block|{
name|uint32_t
name|tmp
decl_stmt|,
name|ret
decl_stmt|;
asm|__asm__
specifier|__volatile__
asm|(     ".set noreorder         \n"     "   nor  %[msk], 0      \n"     "1: ll   %[tmp], %[val] \n"     "   move %[ret], %[tmp] \n"     "   and  %[tmp], %[msk] \n"     "   sc   %[tmp], %[val] \n"     "   beqz %[tmp], 1b     \n"     "   nop                 \n"     ".set reorder           \n"     : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret), [msk] "+r" (mask)     : : "memory");
return|return
operator|(
name|ret
operator|)
return|;
block|}
comment|/**  * Atomically swaps value in 64 bit (aligned) memory location,  * and returns previous value.  *  * This version does not perform 'sync' operations to enforce memory  * operations.  This should only be used when there are no memory operation  * ordering constraints.  *  * @param ptr       address in memory  * @param new_val   new value to write  *  * @return Value of memory location before swap operation  */
specifier|static
specifier|inline
name|uint64_t
name|cvmx_atomic_swap64_nosync
parameter_list|(
name|uint64_t
modifier|*
name|ptr
parameter_list|,
name|uint64_t
name|new_val
parameter_list|)
block|{
name|uint64_t
name|tmp
decl_stmt|,
name|ret
decl_stmt|;
if|if
condition|(
name|OCTEON_IS_MODEL
argument_list|(
name|OCTEON_CN6XXX
argument_list|)
operator|||
name|OCTEON_IS_MODEL
argument_list|(
name|OCTEON_CNF7XXX
argument_list|)
condition|)
block|{
name|CVMX_PUSH_OCTEON2
expr_stmt|;
if|if
condition|(
name|__builtin_constant_p
argument_list|(
name|new_val
argument_list|)
operator|&&
name|new_val
operator|==
literal|0
condition|)
block|{
asm|__asm__
specifier|__volatile__
asm|( 		"lacd  %0,(%1)" 		: "=r" (ret) : "r" (ptr) : "memory");
block|}
elseif|else
if|if
condition|(
name|__builtin_constant_p
argument_list|(
name|new_val
argument_list|)
operator|&&
name|new_val
operator|==
operator|~
literal|0ull
condition|)
block|{
asm|__asm__
specifier|__volatile__
asm|( 		"lasd  %0,(%1)" 		: "=r" (ret) : "r" (ptr) : "memory");
block|}
else|else
block|{
asm|__asm__
specifier|__volatile__
asm|( 		"lawd  %0,(%1),%2" 		: "=r" (ret) : "r" (ptr), "r" (new_val) : "memory");
block|}
name|CVMX_POP_OCTEON2
expr_stmt|;
block|}
else|else
block|{
asm|__asm__
specifier|__volatile__
asm|(             ".set noreorder         \n"             "1: lld  %[ret], %[val] \n"             "   move %[tmp], %[new_val] \n"             "   scd  %[tmp], %[val] \n"             "   beqz %[tmp],  1b    \n"             "   nop                 \n"             ".set reorder           \n"             : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)             : [new_val] "r"  (new_val)             : "memory");
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
comment|/**  * Atomically swaps value in 32 bit (aligned) memory location,  * and returns previous value.  *  * This version does not perform 'sync' operations to enforce memory  * operations.  This should only be used when there are no memory operation  * ordering constraints.  *  * @param ptr       address in memory  * @param new_val   new value to write  *  * @return Value of memory location before swap operation  */
specifier|static
specifier|inline
name|uint32_t
name|cvmx_atomic_swap32_nosync
parameter_list|(
name|uint32_t
modifier|*
name|ptr
parameter_list|,
name|uint32_t
name|new_val
parameter_list|)
block|{
name|uint32_t
name|tmp
decl_stmt|,
name|ret
decl_stmt|;
if|if
condition|(
name|OCTEON_IS_MODEL
argument_list|(
name|OCTEON_CN6XXX
argument_list|)
operator|||
name|OCTEON_IS_MODEL
argument_list|(
name|OCTEON_CNF7XXX
argument_list|)
condition|)
block|{
name|CVMX_PUSH_OCTEON2
expr_stmt|;
if|if
condition|(
name|__builtin_constant_p
argument_list|(
name|new_val
argument_list|)
operator|&&
name|new_val
operator|==
literal|0
condition|)
block|{
asm|__asm__
specifier|__volatile__
asm|( 		"lac  %0,(%1)" 		: "=r" (ret) : "r" (ptr) : "memory");
block|}
elseif|else
if|if
condition|(
name|__builtin_constant_p
argument_list|(
name|new_val
argument_list|)
operator|&&
name|new_val
operator|==
operator|~
literal|0u
condition|)
block|{
asm|__asm__
specifier|__volatile__
asm|( 		"las  %0,(%1)" 		: "=r" (ret) : "r" (ptr) : "memory");
block|}
else|else
block|{
asm|__asm__
specifier|__volatile__
asm|( 		"law  %0,(%1),%2" 		: "=r" (ret) : "r" (ptr), "r" (new_val) : "memory");
block|}
name|CVMX_POP_OCTEON2
expr_stmt|;
block|}
else|else
block|{
asm|__asm__
specifier|__volatile__
asm|(         ".set noreorder         \n"         "1: ll   %[ret], %[val] \n"         "   move %[tmp], %[new_val] \n"         "   sc   %[tmp], %[val] \n"         "   beqz %[tmp],  1b    \n"         "   nop                 \n"         ".set reorder           \n"         : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)         : [new_val] "r"  (new_val)         : "memory");
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
ifdef|#
directive|ifdef
name|__cplusplus
block|}
end_extern

begin_endif
endif|#
directive|endif
end_endif

begin_endif
endif|#
directive|endif
end_endif

begin_comment
comment|/* __CVMX_ATOMIC_H__ */
end_comment

end_unit

